{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Walk up the directory tree until we find 'src'\n",
        "path = current_dir\n",
        "src_path = None\n",
        "\n",
        "while True:\n",
        "    if os.path.basename(path) == \"src\":\n",
        "        src_path = path\n",
        "        break\n",
        "    parent = os.path.dirname(path)\n",
        "    if parent == path:  # reached filesystem root\n",
        "        break\n",
        "    path = parent\n",
        "\n",
        "# Add src to sys.path if found\n",
        "if src_path and src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Add the src directory to Python path to enable imports from helpers\n",
        "# The notebook is in src/ncf/, so we go up one level to get to src/\n",
        "current_dir = os.getcwd()\n",
        "# Navigate to src directory (parent of ncf directory)\n",
        "if os.path.basename(current_dir) == 'ncf':\n",
        "    src_path = os.path.dirname(current_dir)\n",
        "else:\n",
        "    # Try to find src in the current path\n",
        "    parts = current_dir.split(os.sep)\n",
        "    if 'src' in parts:\n",
        "        src_idx = parts.index('src')\n",
        "        src_path = os.sep.join(parts[:src_idx + 1])\n",
        "    else:\n",
        "        # Fallback: assume src is a sibling or child directory\n",
        "        src_path = os.path.join(os.path.dirname(current_dir), 'src')\n",
        "\n",
        "if os.path.exists(src_path) and src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "# Local imports\n",
        "from helpers import download_ml1m_dataset\n",
        "from utils.ml_to_ncf import preprocess_ml1m_to_ncf_format\n",
        "from utils.ncfdata import NCFData\n",
        "from helpers.ncf_model import NCF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model architecture: NeuMF-end\n",
            "✓ Directories configured\n",
            "  - Data directory: /Users/abbas/Documents/Codes/thesis/recommender/src/../data (will be created/used for downloaded data)\n",
            "  - Model save path: /Users/abbas/Documents/Codes/thesis/recommender/src/../models\n"
          ]
        }
      ],
      "source": [
        "dataset = 'ml-1m'\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Choose which model architecture to use\n",
        "# Options:\n",
        "#   - 'MLP': Multi-Layer Perceptron only (non-linear interactions)\n",
        "#   - 'GMF': Generalized Matrix Factorization only (linear interactions)\n",
        "#   - 'NeuMF-end': Neural Matrix Factorization trained from scratch (end-to-end)\n",
        "#   - 'NeuMF-pre': Neural Matrix Factorization with pre-trained GMF and MLP models\n",
        "model_name = 'NeuMF-end'\n",
        "assert model_name in ['MLP', 'GMF', 'NeuMF-end', 'NeuMF-pre'], \\\n",
        "    f\"Model must be 'MLP', 'GMF', 'NeuMF-end', or 'NeuMF-pre', got '{model_name}'\"\n",
        "\n",
        "print(f\"✓ Model architecture: {model_name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA AND MODEL PATHS CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Data will be downloaded automatically during training\n",
        "# We'll create a local data directory to store downloaded files\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Model saving directory\n",
        "model_path = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "GMF_model_path = os.path.join(model_path, 'GMF.pth')\n",
        "MLP_model_path = os.path.join(model_path, 'MLP.pth')\n",
        "NeuMF_model_path = os.path.join(model_path, 'NeuMF.pth')\n",
        "\n",
        "print(f\"✓ Directories configured\")\n",
        "print(f\"  - Data directory: {data_dir} (will be created/used for downloaded data)\")\n",
        "print(f\"  - Model save path: {model_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.4 TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Learning rate: Controls how big steps the optimizer takes during training\n",
        "# Too high: training might be unstable or diverge\n",
        "# Too low: training will be very slow\n",
        "# Typical range: 0.0001 to 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dropout rate: Regularization technique to prevent overfitting\n",
        "# Randomly sets some neurons to zero during training\n",
        "# Range: 0.0 (no dropout) to 0.9 (very aggressive dropout)\n",
        "# 0.0 means no dropout (all neurons active)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Batch size: Number of training examples processed together in one iteration\n",
        "# Larger batch size: more stable gradients, but requires more memory\n",
        "# Smaller batch size: less memory, but noisier gradients\n",
        "# Typical values: 64, 128, 256, 512\n",
        "batch_size = 256\n",
        "\n",
        "# Number of training epochs: How many times we'll iterate through the entire dataset\n",
        "# More epochs: better learning, but risk of overfitting\n",
        "# Too few epochs: model might not learn enough\n",
        "epochs = 20\n",
        "\n",
        "# Top-K for evaluation: When evaluating, we recommend top K items to each user\n",
        "# We measure if the true item is in the top K recommendations\n",
        "# Common values: 5, 10, 20\n",
        "top_k = 10\n",
        "\n",
        "# Factor number: Dimension of the embedding vectors for users and items\n",
        "# Larger: more capacity to learn complex patterns, but more parameters\n",
        "# Smaller: fewer parameters, faster training, but less capacity\n",
        "# Common values: 8, 16, 32, 64\n",
        "factor_num = 32\n",
        "\n",
        "# Number of MLP layers: Depth of the Multi-Layer Perceptron component\n",
        "# More layers: can learn more complex non-linear patterns\n",
        "# Fewer layers: simpler model, faster training\n",
        "# Typical range: 1 to 5 layers\n",
        "num_layers = 3\n",
        "\n",
        "# Number of negative samples for training: For each positive (user, item) pair,\n",
        "# we sample this many negative items (items the user hasn't interacted with)\n",
        "# More negatives: better learning signal, but slower training\n",
        "# Fewer negatives: faster training, but potentially weaker learning\n",
        "# Common values: 1, 4, 8\n",
        "num_ng = 4\n",
        "\n",
        "# Number of negative samples for testing: During evaluation, for each test item,\n",
        "# we also provide this many negative items. The model should rank the true item higher.\n",
        "# Typically 99 negatives + 1 positive = 100 items total per test case\n",
        "test_num_ng = 99\n",
        "\n",
        "# Whether to save the trained model\n",
        "save_model = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 3.1: Downloading MovieLens 1M Dataset\n",
            "======================================================================\n",
            "✓ Dataset already exists at /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"STEP 3.1: Downloading MovieLens 1M Dataset\")\n",
        "print(\"=\" * 70)\n",
        "ratings_file = download_ml1m_dataset(data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 1000209 ratings\n",
            "  - Unique users: 6040\n",
            "  - Unique movies: 3706\n",
            "\n",
            "Filtering positive interactions (ratings >= 4)...\n",
            "✓ 575281 positive interactions (out of 1000209 total)\n",
            "\n",
            "Remapping user and item IDs to be contiguous...\n",
            "✓ Remapped to 6038 users and 3533 items\n",
            "\n",
            "Splitting data (train: 80%, test: 20%)...\n",
            "✓ Training pairs: 460225\n",
            "✓ Test pairs: 115056\n",
            "\n",
            "Saving training data to /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Saved 460225 training pairs\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Generating test negative samples (99 negatives per test case)...\n",
            "✓ Generated test negative samples: 115056 test cases\n"
          ]
        }
      ],
      "source": [
        "train_rating_path, test_rating_path, test_negative_path, user_num, item_num, train_mat = \\\n",
        "    preprocess_ml1m_to_ncf_format(ratings_file, data_dir, test_ratio=0.2, test_negatives=99)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 4.1: Loading Data Files\n",
            "======================================================================\n",
            "Loading training data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Loaded 460225 training pairs\n",
            "  - Users: 6038\n",
            "  - Items: 3533\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Loading test data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.test.negative...\n",
            "✓ Loaded 11505600 test pairs (including negatives)\n",
            "\n",
            "======================================================================\n",
            "✓ Data loading complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: PYTORCH DATASET CLASS AND DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step creates the PyTorch Dataset class and data loading functions.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 4.1 LOAD ALL DATA FILES\n",
        "# ============================================================================\n",
        "\n",
        "def load_all_data(train_rating_path, test_negative_path):\n",
        "    \"\"\"\n",
        "    Loads all data files into memory for efficient access during training.\n",
        "    \n",
        "    This function loads:\n",
        "    1. Training data: user-item pairs from train.rating file\n",
        "    2. Test data: user-item pairs with negative samples from test.negative file\n",
        "    3. Training matrix: sparse matrix for efficient negative sampling\n",
        "    \n",
        "    Parameters:\n",
        "    - train_rating_path: Path to the training rating file\n",
        "    - test_negative_path: Path to the test negative file\n",
        "    \n",
        "    Returns:\n",
        "    - train_data: List of [user, item] pairs for training\n",
        "    - test_data: List of [user, item] pairs for testing (includes negatives)\n",
        "    - user_num: Total number of users\n",
        "    - item_num: Total number of items\n",
        "    - train_mat: Sparse matrix of training interactions\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 4.1: Loading Data Files\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Load training data\n",
        "    print(f\"Loading training data from {train_rating_path}...\")\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating_path,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['user', 'item'],\n",
        "        usecols=[0, 1],\n",
        "        dtype={0: np.int32, 1: np.int32}\n",
        "    )\n",
        "    \n",
        "    # Calculate number of users and items\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "    \n",
        "    print(f\"✓ Loaded {len(train_data)} training pairs\")\n",
        "    print(f\"  - Users: {user_num}\")\n",
        "    print(f\"  - Items: {item_num}\")\n",
        "    \n",
        "    # Convert to list of lists for easier processing\n",
        "    train_data = train_data.values.tolist()\n",
        "    \n",
        "    # Create sparse training matrix (Dictionary of Keys format)\n",
        "    # This is used to quickly check if a user-item pair exists in training data\n",
        "    print(\"\\nCreating training interaction matrix...\")\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for u, i in train_data:\n",
        "        train_mat[u, i] = 1.0\n",
        "    print(f\"✓ Training matrix created: {train_mat.nnz} interactions\")\n",
        "    \n",
        "    # Load test data with negative samples\n",
        "    print(f\"\\nLoading test data from {test_negative_path}...\")\n",
        "    test_data = []\n",
        "    with open(test_negative_path, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line is not None and line != '':\n",
        "            # Format: (user, item)\\tneg1\\tneg2\\t...\\tneg99\n",
        "            arr = line.strip().split('\\t')\n",
        "            \n",
        "            # Parse the positive pair: (user, item)\n",
        "            # eval() converts string \"(123, 456)\" to tuple (123, 456)\n",
        "            positive_pair = eval(arr[0])\n",
        "            u = positive_pair[0]\n",
        "            i = positive_pair[1]\n",
        "            \n",
        "            # Add the positive pair\n",
        "            test_data.append([u, i])\n",
        "            \n",
        "            # Add all negative items for this user\n",
        "            for neg_item in arr[1:]:\n",
        "                if neg_item:  # Skip empty strings\n",
        "                    test_data.append([u, int(neg_item)])\n",
        "            \n",
        "            line = fd.readline()\n",
        "    \n",
        "    print(f\"✓ Loaded {len(test_data)} test pairs (including negatives)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ Data loading complete!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return train_data, test_data, user_num, item_num, train_mat\n",
        "\n",
        "# Load all data\n",
        "train_data, test_data, user_num, item_num, train_mat = load_all_data(\n",
        "    train_rating_path, test_negative_path\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = NCFData(\n",
        "    train_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=num_ng,  # From Step 2 configuration\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "test_dataset = NCFData(\n",
        "    test_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=0,  # No negative sampling for testing\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,  # From Step 2 configuration\n",
        "    shuffle=True,  # Shuffle training data each epoch\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks (avoids pickling errors)\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_num_ng + 1,  # 1 positive + test_num_ng negatives\n",
        "    shuffle=False,  # Don't shuffle test data\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Verify num_workers is 0 (safety check)\n",
        "assert train_loader.num_workers == 0, f\"ERROR: train_loader.num_workers is {train_loader.num_workers}, must be 0!\"\n",
        "assert test_loader.num_workers == 0, f\"ERROR: test_loader.num_workers is {test_loader.num_workers}, must be 0!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5.2: Creating and Initializing Model\n",
            "======================================================================\n",
            "✓ Model on CPU\n",
            "\n",
            "✓ Model created successfully!\n",
            "  - Total parameters: 1,574,657\n",
            "  - Trainable parameters: 1,574,657\n",
            "\n",
            "Model Architecture:\n",
            "  - Users: 6,038\n",
            "  - Items: 3,533\n",
            "  - GMF embeddings: 32 dimensions\n",
            "  - MLP embeddings: 128 dimensions\n",
            "  - MLP layers: 3 (with dropout=0.1)\n",
            "  - Prediction layer: 64 → 1\n",
            "\n",
            "======================================================================\n",
            "COMPLETE MODEL STRUCTURE:\n",
            "======================================================================\n",
            "NCF(\n",
            "  (embed_user_GMF): Embedding(6038, 32)\n",
            "  (embed_item_GMF): Embedding(3533, 32)\n",
            "  (embed_user_MLP): Embedding(6038, 128)\n",
            "  (embed_item_MLP): Embedding(3533, 128)\n",
            "  (MLP_layers): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (8): ReLU()\n",
            "  )\n",
            "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "LAYER-BY-LAYER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "[GMF Path - Generalized Matrix Factorization]\n",
            "  embed_user_GMF: Embedding(6038, 32)\n",
            "    → Converts user IDs to 32-dimensional vectors\n",
            "  embed_item_GMF: Embedding(3533, 32)\n",
            "    → Converts item IDs to 32-dimensional vectors\n",
            "  Element-wise product: user_emb * item_emb\n",
            "    → Output shape: [batch_size, 32]\n",
            "\n",
            "[MLP Path - Multi-Layer Perceptron]\n",
            "  embed_user_MLP: Embedding(6038, 128)\n",
            "    → Converts user IDs to 128-dimensional vectors\n",
            "  embed_item_MLP: Embedding(3533, 128)\n",
            "    → Converts item IDs to 128-dimensional vectors\n",
            "  Concatenation: [user_emb, item_emb]\n",
            "    → Output shape: [batch_size, 256]\n",
            "\n",
            "  MLP Layers (3 layers):\n",
            "    Layer 1:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(256, 128)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 128]\n",
            "    Layer 2:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(128, 64)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 64]\n",
            "    Layer 3:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(64, 32)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 32]\n",
            "\n",
            "[Prediction Layer]\n",
            "  Input: Concatenated GMF + MLP [64 dimensions]\n",
            "    → GMF: [32 dims] + MLP: [32 dims]\n",
            "  Linear(64, 1)\n",
            "    → Output: [batch_size, 1] (interaction score)\n",
            "    → Higher score = more likely user will like item\n",
            "\n",
            "======================================================================\n",
            "PARAMETER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "GMF Embeddings:\n",
            "  User embeddings: 6,038 × 32 = 193,216 parameters\n",
            "  Item embeddings: 3,533 × 32 = 113,056 parameters\n",
            "  GMF Total: 306,272 parameters\n",
            "\n",
            "MLP Embeddings:\n",
            "  User embeddings: 6,038 × 128 = 772,864 parameters\n",
            "  Item embeddings: 3,533 × 128 = 452,224 parameters\n",
            "  MLP Embeddings Total: 1,225,088 parameters\n",
            "\n",
            "MLP Layers:\n",
            "  Layer 1 (Linear(256, 128)): 32,896 parameters\n",
            "  Layer 2 (Linear(128, 64)): 8,256 parameters\n",
            "  Layer 3 (Linear(64, 32)): 2,080 parameters\n",
            "  MLP Layers Total: 43,232 parameters\n",
            "\n",
            "Prediction Layer:\n",
            "  Linear(64, 1): 65 parameters\n",
            "\n",
            "======================================================================\n",
            "TOTAL MODEL PARAMETERS: 1,574,657\n",
            "Model Size (float32): ~6.01 MB\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 CREATE AND INITIALIZE THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5.2: Creating and Initializing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we need pre-trained models (for NeuMF-pre)\n",
        "if model_name == 'NeuMF-pre':\n",
        "    # For NeuMF-pre, we would load pre-trained GMF and MLP models\n",
        "    # For now, we'll use NeuMF-end (training from scratch)\n",
        "    print(\"⚠ NeuMF-pre requires pre-trained models.\")\n",
        "    print(\"  Switching to NeuMF-end (training from scratch)...\")\n",
        "    model_name = 'NeuMF-end'\n",
        "\n",
        "# Create the model\n",
        "ncf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name=model_name,\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    ncf_model = ncf_model.cuda()\n",
        "    print(\"✓ Model moved to GPU\")\n",
        "else:\n",
        "    print(\"✓ Model on CPU\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model created successfully!\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - Users: {user_num:,}\")\n",
        "print(f\"  - Items: {item_num:,}\")\n",
        "if model_name != 'MLP':\n",
        "    print(f\"  - GMF embeddings: {factor_num} dimensions\")\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(f\"  - MLP embeddings: {mlp_embed_dim} dimensions\")\n",
        "    print(f\"  - MLP layers: {num_layers} (with dropout={dropout_rate})\")\n",
        "print(f\"  - Prediction layer: {factor_num if model_name in ['MLP', 'GMF'] else factor_num * 2} → 1\")\n",
        "\n",
        "\n",
        "\n",
        "# Print the full model structure\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPLETE MODEL STRUCTURE:\")\n",
        "print(\"=\" * 70)\n",
        "print(ncf_model)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print detailed layer information\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LAYER-BY-LAYER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_name != 'MLP':\n",
        "    print(\"\\n[GMF Path - Generalized Matrix Factorization]\")\n",
        "    print(f\"  embed_user_GMF: Embedding({user_num}, {factor_num})\")\n",
        "    print(f\"    → Converts user IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  embed_item_GMF: Embedding({item_num}, {factor_num})\")\n",
        "    print(f\"    → Converts item IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  Element-wise product: user_emb * item_emb\")\n",
        "    print(f\"    → Output shape: [batch_size, {factor_num}]\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(\"\\n[MLP Path - Multi-Layer Perceptron]\")\n",
        "    print(f\"  embed_user_MLP: Embedding({user_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts user IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  embed_item_MLP: Embedding({item_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts item IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  Concatenation: [user_emb, item_emb]\")\n",
        "    print(f\"    → Output shape: [batch_size, {mlp_embed_dim * 2}]\")\n",
        "    \n",
        "    print(f\"\\n  MLP Layers ({num_layers} layers):\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        print(f\"    Layer {i+1}:\")\n",
        "        print(f\"      Dropout(p={dropout_rate})\")\n",
        "        print(f\"      Linear({input_size}, {output_size})\")\n",
        "        print(f\"      ReLU()\")\n",
        "        print(f\"      → Output shape: [batch_size, {output_size}]\")\n",
        "\n",
        "print(\"\\n[Prediction Layer]\")\n",
        "if model_name == 'GMF':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: GMF output [{factor_num} dimensions]\")\n",
        "elif model_name == 'MLP':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: MLP output [{factor_num} dimensions]\")\n",
        "else:  # NeuMF\n",
        "    predict_input = factor_num * 2\n",
        "    print(f\"  Input: Concatenated GMF + MLP [{factor_num * 2} dimensions]\")\n",
        "    print(f\"    → GMF: [{factor_num} dims] + MLP: [{factor_num} dims]\")\n",
        "\n",
        "print(f\"  Linear({predict_input}, 1)\")\n",
        "print(f\"    → Output: [batch_size, 1] (interaction score)\")\n",
        "print(f\"    → Higher score = more likely user will like item\")\n",
        "\n",
        "# Calculate and print parameter breakdown\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARAMETER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_params = 0\n",
        "if model_name != 'MLP':\n",
        "    gmf_user_params = user_num * factor_num\n",
        "    gmf_item_params = item_num * factor_num\n",
        "    gmf_total = gmf_user_params + gmf_item_params\n",
        "    total_params += gmf_total\n",
        "    print(f\"\\nGMF Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {factor_num} = {gmf_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {factor_num} = {gmf_item_params:,} parameters\")\n",
        "    print(f\"  GMF Total: {gmf_total:,} parameters\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    mlp_user_params = user_num * mlp_embed_dim\n",
        "    mlp_item_params = item_num * mlp_embed_dim\n",
        "    mlp_embed_total = mlp_user_params + mlp_item_params\n",
        "    total_params += mlp_embed_total\n",
        "    print(f\"\\nMLP Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {mlp_embed_dim} = {mlp_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {mlp_embed_dim} = {mlp_item_params:,} parameters\")\n",
        "    print(f\"  MLP Embeddings Total: {mlp_embed_total:,} parameters\")\n",
        "    \n",
        "    # MLP layers parameters\n",
        "    mlp_layer_params = 0\n",
        "    print(f\"\\nMLP Layers:\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        layer_params = (input_size * output_size) + output_size  # weights + bias\n",
        "        mlp_layer_params += layer_params\n",
        "        print(f\"  Layer {i+1} (Linear({input_size}, {output_size})): {layer_params:,} parameters\")\n",
        "    total_params += mlp_layer_params\n",
        "    print(f\"  MLP Layers Total: {mlp_layer_params:,} parameters\")\n",
        "\n",
        "# Prediction layer\n",
        "if model_name in ['MLP', 'GMF']:\n",
        "    predict_input = factor_num\n",
        "else:\n",
        "    predict_input = factor_num * 2\n",
        "predict_params = (predict_input * 1) + 1  # weights + bias\n",
        "total_params += predict_params\n",
        "print(f\"\\nPrediction Layer:\")\n",
        "print(f\"  Linear({predict_input}, 1): {predict_params:,} parameters\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"TOTAL MODEL PARAMETERS: {total_params:,}\")\n",
        "print(f\"Model Size (float32): ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.1: Hit Rate Metric\n",
            "======================================================================\n",
            "✓ Hit Rate function defined\n",
            "  - Returns 1 if true item is in top-K recommendations\n",
            "  - Returns 0 otherwise\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements evaluation metrics for recommendation systems:\n",
        "- Hit Rate (HR@K): Binary metric - is the true item in top K?\n",
        "- NDCG (Normalized Discounted Cumulative Gain@K): Ranking quality metric\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 6.1 HIT RATE METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def hit(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Hit Rate for a single test case.\n",
        "    \n",
        "    Hit Rate is 1 if the ground truth item is in the predicted top-K items,\n",
        "    otherwise 0.\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - 1 if gt_item is in pred_items, 0 otherwise\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.1: Hit Rate Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Hit Rate function defined\")\n",
        "print(\"  - Returns 1 if true item is in top-K recommendations\")\n",
        "print(\"  - Returns 0 otherwise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.2: NDCG Metric\n",
            "======================================================================\n",
            "✓ NDCG function defined\n",
            "  - Measures ranking quality\n",
            "  - Higher score for items ranked higher\n",
            "  - Returns 0 if true item not in recommendations\n",
            "\n",
            "NDCG Examples:\n",
            "  Top-5 recommendations: [10, 20, 30, 40, 50]\n",
            "  If true item is at position 0: NDCG = 1.000\n",
            "  If true item is at position 2: NDCG = 0.500\n",
            "  If true item is at position 4: NDCG = 0.387\n",
            "  If true item not in list: NDCG = 0.000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.2 NDCG METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single test case.\n",
        "    \n",
        "    NDCG measures ranking quality by:\n",
        "    1. Giving more weight to items ranked higher (position matters)\n",
        "    2. Using logarithmic discounting (relevance decreases with position)\n",
        "    \n",
        "    Formula: NDCG = 1 / log2(position + 2)\n",
        "    - Position 0 (top): 1 / log2(2) = 1.0\n",
        "    - Position 1: 1 / log2(3) ≈ 0.63\n",
        "    - Position 2: 1 / log2(4) = 0.5\n",
        "    - Position 9: 1 / log2(11) ≈ 0.29\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - NDCG score (0.0 to 1.0) if gt_item is in pred_items\n",
        "    - 0.0 if gt_item is not in pred_items\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        # Find the position (index) of the ground truth item\n",
        "        index = pred_items.index(gt_item)\n",
        "        # Calculate NDCG: 1 / log2(position + 2)\n",
        "        # +2 because: position 0 should give 1/log2(2) = 1.0\n",
        "        return np.reciprocal(np.log2(index + 2))\n",
        "    return 0.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.2: NDCG Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NDCG function defined\")\n",
        "print(\"  - Measures ranking quality\")\n",
        "print(\"  - Higher score for items ranked higher\")\n",
        "print(\"  - Returns 0 if true item not in recommendations\")\n",
        "\n",
        "# Example to demonstrate NDCG\n",
        "print(\"\\nNDCG Examples:\")\n",
        "example_items = [10, 20, 30, 40, 50]\n",
        "print(f\"  Top-5 recommendations: {example_items}\")\n",
        "print(f\"  If true item is at position 0: NDCG = {ndcg(10, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 2: NDCG = {ndcg(30, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 4: NDCG = {ndcg(50, example_items):.3f}\")\n",
        "print(f\"  If true item not in list: NDCG = {ndcg(99, example_items):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.3: Evaluation Function\n",
            "======================================================================\n",
            "✓ Evaluation function defined\n",
            "  - Evaluates model on test data\n",
            "  - Calculates average Hit Rate and NDCG\n",
            "  - Works with GPU or CPU\n",
            "\n",
            "✓ Evaluation ready (device: cpu)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.3 EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_metrics(model, test_loader, top_k, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "    \n",
        "    This function:\n",
        "    1. For each test case (1 positive + 99 negatives):\n",
        "       - Gets model predictions for all 100 items\n",
        "       - Selects top-K items with highest scores\n",
        "       - Checks if the true item is in top-K (Hit Rate)\n",
        "       - Calculates NDCG based on true item's position\n",
        "    2. Averages metrics across all test cases\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - test_loader: DataLoader with test data\n",
        "    - top_k: Number of top items to consider (e.g., 10)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - mean_HR: Average Hit Rate across all test cases\n",
        "    - mean_NDCG: Average NDCG across all test cases\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout)\n",
        "    \n",
        "    HR_list = []  # List to store Hit Rate for each test case\n",
        "    NDCG_list = []  # List to store NDCG for each test case\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation (faster, saves memory)\n",
        "        for user, item, label in test_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                user = user.cuda()\n",
        "                item = item.cuda()\n",
        "            else:\n",
        "                device = 'cpu'\n",
        "            \n",
        "            # Get model predictions for all items in this batch\n",
        "            # Batch size = test_num_ng + 1 = 100 (1 positive + 99 negatives)\n",
        "            predictions = model(user, item)  # [100] tensor of scores\n",
        "            \n",
        "            # Get top-K items with highest prediction scores\n",
        "            # torch.topk returns (values, indices)\n",
        "            _, indices = torch.topk(predictions, top_k)\n",
        "            \n",
        "            # Get the actual item IDs for top-K recommendations\n",
        "            # torch.take extracts items at given indices\n",
        "            recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "            \n",
        "            # The first item in the batch is always the positive (true) item\n",
        "            gt_item = item[0].item()  # Ground truth item ID\n",
        "            \n",
        "            # Calculate metrics for this test case\n",
        "            HR_list.append(hit(gt_item, recommends))\n",
        "            NDCG_list.append(ndcg(gt_item, recommends))\n",
        "    \n",
        "    # Calculate average metrics across all test cases\n",
        "    mean_HR = np.mean(HR_list)\n",
        "    mean_NDCG = np.mean(NDCG_list)\n",
        "    \n",
        "    return mean_HR, mean_NDCG\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.3: Evaluation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Evaluation function defined\")\n",
        "print(\"  - Evaluates model on test data\")\n",
        "print(\"  - Calculates average Hit Rate and NDCG\")\n",
        "print(\"  - Works with GPU or CPU\")\n",
        "\n",
        "# Determine device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n✓ Evaluation ready (device: {device})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 6:\n",
        "\n",
        "#### 6.1 Hit Rate (HR@K) - Binary Metric\n",
        "\n",
        "**What is Hit Rate?**\n",
        "- Measures whether the true item appears in the top-K recommendations\n",
        "- Binary metric: 1 if found, 0 if not found\n",
        "- Simple and intuitive: \"Did we recommend the right item?\"\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "Hit Rate = 1 (item is in top-10)\n",
        "```\n",
        "\n",
        "**Why Hit Rate?**\n",
        "- Users typically only see top-K recommendations\n",
        "- If true item is in top-K, recommendation is successful\n",
        "- Easy to interpret: \"X% of test cases had correct item in top-K\"\n",
        "\n",
        "**Limitations:**\n",
        "- Doesn't consider position (item at position 1 vs position 10 both get 1)\n",
        "- Binary: doesn't measure how good the ranking is\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.2 NDCG (Normalized Discounted Cumulative Gain) - Ranking Metric\n",
        "\n",
        "**What is NDCG?**\n",
        "- Measures ranking quality, not just presence\n",
        "- Gives more weight to items ranked higher\n",
        "- Uses logarithmic discounting (relevance decreases with position)\n",
        "\n",
        "**NDCG Formula:**\n",
        "```\n",
        "NDCG = 1 / log2(position + 2)\n",
        "```\n",
        "\n",
        "**Position vs NDCG Score:**\n",
        "| Position | NDCG Score | Meaning |\n",
        "|----------|-----------|---------|\n",
        "| 0 (top)  | 1.000     | Perfect! Item ranked #1 |\n",
        "| 1        | 0.631     | Item ranked #2 |\n",
        "| 2        | 0.500     | Item ranked #3 |\n",
        "| 3        | 0.431     | Item ranked #4 |\n",
        "| 4        | 0.387     | Item ranked #5 |\n",
        "| 9        | 0.289     | Item ranked #10 |\n",
        "\n",
        "**Why Logarithmic Discounting?**\n",
        "- Position 0 → 1: Big difference (user sees it first)\n",
        "- Position 9 → 10: Small difference (both far down)\n",
        "- Logarithmic function captures this diminishing importance\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "NDCG = 1 / log2(2 + 2) = 1 / log2(4) = 1 / 2 = 0.5\n",
        "```\n",
        "\n",
        "**Why NDCG?**\n",
        "- Considers position: Better ranking = Higher score\n",
        "- More informative than Hit Rate\n",
        "- Standard metric in information retrieval and recommendation systems\n",
        "\n",
        "**Limitations:**\n",
        "- More complex than Hit Rate\n",
        "- Requires understanding of logarithmic discounting\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.3 Evaluation Process\n",
        "\n",
        "**How Evaluation Works:**\n",
        "\n",
        "1. **For each test case** (1 positive + 99 negatives):\n",
        "   ```\n",
        "   User: 123\n",
        "   Items: [42 (positive), 1, 5, 7, 9, ... (99 negatives)]\n",
        "   ```\n",
        "\n",
        "2. **Get predictions**:\n",
        "   ```\n",
        "   Model scores: [0.8, 0.3, 0.2, 0.1, 0.05, ...]\n",
        "   Item 42 gets score 0.8 (highest!)\n",
        "   ```\n",
        "\n",
        "3. **Select top-K** (e.g., K=10):\n",
        "   ```\n",
        "   Top-10 items: [42, 1, 5, 7, 9, 12, 15, 18, 20, 23]\n",
        "   ```\n",
        "\n",
        "4. **Calculate metrics**:\n",
        "   - Hit Rate: Is 42 in top-10? Yes → HR = 1\n",
        "   - NDCG: Position of 42? Position 0 → NDCG = 1.0\n",
        "\n",
        "5. **Average across all test cases**:\n",
        "   ```\n",
        "   Mean HR = (1 + 0 + 1 + 1 + ...) / N\n",
        "   Mean NDCG = (1.0 + 0.0 + 0.5 + 0.63 + ...) / N\n",
        "   ```\n",
        "\n",
        "**Test Data Structure:**\n",
        "- Each batch: 100 items (1 positive + 99 negatives)\n",
        "- Model should rank the positive item higher than negatives\n",
        "- We measure if positive is in top-K\n",
        "\n",
        "**Why 99 Negatives?**\n",
        "- Simulates real-world scenario: recommend 1 from 100 candidates\n",
        "- Standard evaluation protocol (used in research papers)\n",
        "- Makes evaluation realistic and challenging\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.4 Understanding the Results\n",
        "\n",
        "**Good Performance:**\n",
        "- HR@10 > 0.6: 60% of test cases have true item in top-10\n",
        "- NDCG@10 > 0.4: Good ranking quality on average\n",
        "\n",
        "**Excellent Performance:**\n",
        "- HR@10 > 0.7: 70% success rate\n",
        "- NDCG@10 > 0.5: Very good ranking quality\n",
        "\n",
        "**What to Expect:**\n",
        "- Random baseline: HR@10 ≈ 0.1 (10% chance)\n",
        "- Good model: HR@10 ≈ 0.6-0.7\n",
        "- State-of-the-art: HR@10 > 0.7\n",
        "\n",
        "**Interpreting Results:**\n",
        "- **HR higher than NDCG**: Model finds items but doesn't rank them well\n",
        "- **NDCG close to HR**: Model ranks items well (good positions)\n",
        "- **Both low**: Model needs more training or better architecture\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 6 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Hit Rate metric for binary evaluation\n",
        "- NDCG metric for ranking quality\n",
        "- Complete evaluation function ready to use\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: Training Loop\n",
        "\n",
        "In this step, we'll implement the complete training process:\n",
        "1. **Loss Function**: Binary Cross-Entropy with Logits (for binary classification)\n",
        "2. **Optimizer**: Adam optimizer (adaptive learning rate)\n",
        "3. **Training Loop**: Iterate through epochs, train on batches, evaluate periodically\n",
        "4. **Model Saving**: Save the best model based on validation performance\n",
        "\n",
        "This is where the model learns to make good recommendations!\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.1: Setting Up Loss Function and Optimizer\n",
            "======================================================================\n",
            "✓ Loss function: BCEWithLogitsLoss\n",
            "  - For binary classification (like/dislike)\n",
            "  - Combines sigmoid + cross-entropy for stability\n",
            "\n",
            "✓ Optimizer: Adam\n",
            "  - Learning rate: 0.001\n",
            "  - Adaptive: adjusts learning rate automatically\n",
            "\n",
            "✓ Model ready for training\n",
            "  - Trainable parameters: 1,574,657\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements the complete training process for the NCF model.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 7.1 SETUP LOSS FUNCTION AND OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.1: Setting Up Loss Function and Optimizer\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy with Logits\n",
        "# This combines sigmoid activation + binary cross-entropy loss\n",
        "# More numerically stable than applying sigmoid separately\n",
        "# \n",
        "# Why BCEWithLogitsLoss?\n",
        "# - Our task: Predict if user will like item (binary: 1 or 0)\n",
        "# - Model outputs raw scores (logits), not probabilities\n",
        "# - BCEWithLogitsLoss applies sigmoid internally and computes loss\n",
        "# - More stable than: sigmoid(output) then BCE(sigmoid_output, label)\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "print(\"✓ Loss function: BCEWithLogitsLoss\")\n",
        "print(\"  - For binary classification (like/dislike)\")\n",
        "print(\"  - Combines sigmoid + cross-entropy for stability\")\n",
        "\n",
        "# Optimizer: Adam (Adaptive Moment Estimation)\n",
        "# Adam is an adaptive learning rate optimizer that:\n",
        "# - Adjusts learning rate per parameter\n",
        "# - Uses momentum (moving average of gradients)\n",
        "# - Works well for most deep learning tasks\n",
        "# - Better than SGD for this problem\n",
        "\n",
        "optimizer = optim.Adam(ncf_model.parameters(), lr=learning_rate)\n",
        "print(f\"\\n✓ Optimizer: Adam\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Adaptive: adjusts learning rate automatically\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "print(f\"\\n✓ Model ready for training\")\n",
        "print(f\"  - Trainable parameters: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.2: Starting Training\n",
            "======================================================================\n",
            "Training for 20 epochs...\n",
            "Model: NeuMF-end\n",
            "Device: cpu\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.4856\n",
            "  Batch 200/8989 - Loss: 0.4368\n",
            "  Batch 300/8989 - Loss: 0.4145\n",
            "  Batch 400/8989 - Loss: 0.4014\n",
            "  Batch 500/8989 - Loss: 0.3922\n",
            "  Batch 600/8989 - Loss: 0.3861\n",
            "  Batch 700/8989 - Loss: 0.3830\n",
            "  Batch 800/8989 - Loss: 0.3795\n",
            "  Batch 900/8989 - Loss: 0.3767\n",
            "  Batch 1000/8989 - Loss: 0.3740\n",
            "  Batch 1100/8989 - Loss: 0.3719\n",
            "  Batch 1200/8989 - Loss: 0.3707\n",
            "  Batch 1300/8989 - Loss: 0.3699\n",
            "  Batch 1400/8989 - Loss: 0.3687\n",
            "  Batch 1500/8989 - Loss: 0.3673\n",
            "  Batch 1600/8989 - Loss: 0.3664\n",
            "  Batch 1700/8989 - Loss: 0.3652\n",
            "  Batch 1800/8989 - Loss: 0.3641\n",
            "  Batch 1900/8989 - Loss: 0.3631\n",
            "  Batch 2000/8989 - Loss: 0.3626\n",
            "  Batch 2100/8989 - Loss: 0.3622\n",
            "  Batch 2200/8989 - Loss: 0.3616\n",
            "  Batch 2300/8989 - Loss: 0.3611\n",
            "  Batch 2400/8989 - Loss: 0.3603\n",
            "  Batch 2500/8989 - Loss: 0.3597\n",
            "  Batch 2600/8989 - Loss: 0.3591\n",
            "  Batch 2700/8989 - Loss: 0.3585\n",
            "  Batch 2800/8989 - Loss: 0.3577\n",
            "  Batch 2900/8989 - Loss: 0.3571\n",
            "  Batch 3000/8989 - Loss: 0.3563\n",
            "  Batch 3100/8989 - Loss: 0.3557\n",
            "  Batch 3200/8989 - Loss: 0.3551\n",
            "  Batch 3300/8989 - Loss: 0.3543\n",
            "  Batch 3400/8989 - Loss: 0.3537\n",
            "  Batch 3500/8989 - Loss: 0.3532\n",
            "  Batch 3600/8989 - Loss: 0.3524\n",
            "  Batch 3700/8989 - Loss: 0.3517\n",
            "  Batch 3800/8989 - Loss: 0.3510\n",
            "  Batch 3900/8989 - Loss: 0.3504\n",
            "  Batch 4000/8989 - Loss: 0.3497\n",
            "  Batch 4100/8989 - Loss: 0.3492\n",
            "  Batch 4200/8989 - Loss: 0.3486\n",
            "  Batch 4300/8989 - Loss: 0.3479\n",
            "  Batch 4400/8989 - Loss: 0.3473\n",
            "  Batch 4500/8989 - Loss: 0.3467\n",
            "  Batch 4600/8989 - Loss: 0.3462\n",
            "  Batch 4700/8989 - Loss: 0.3455\n",
            "  Batch 4800/8989 - Loss: 0.3450\n",
            "  Batch 4900/8989 - Loss: 0.3444\n",
            "  Batch 5000/8989 - Loss: 0.3438\n",
            "  Batch 5100/8989 - Loss: 0.3433\n",
            "  Batch 5200/8989 - Loss: 0.3426\n",
            "  Batch 5300/8989 - Loss: 0.3420\n",
            "  Batch 5400/8989 - Loss: 0.3414\n",
            "  Batch 5500/8989 - Loss: 0.3409\n",
            "  Batch 5600/8989 - Loss: 0.3405\n",
            "  Batch 5700/8989 - Loss: 0.3399\n",
            "  Batch 5800/8989 - Loss: 0.3395\n",
            "  Batch 5900/8989 - Loss: 0.3389\n",
            "  Batch 6000/8989 - Loss: 0.3385\n",
            "  Batch 6100/8989 - Loss: 0.3380\n",
            "  Batch 6200/8989 - Loss: 0.3375\n",
            "  Batch 6300/8989 - Loss: 0.3371\n",
            "  Batch 6400/8989 - Loss: 0.3366\n",
            "  Batch 6500/8989 - Loss: 0.3361\n",
            "  Batch 6600/8989 - Loss: 0.3356\n",
            "  Batch 6700/8989 - Loss: 0.3350\n",
            "  Batch 6800/8989 - Loss: 0.3346\n",
            "  Batch 6900/8989 - Loss: 0.3342\n",
            "  Batch 7000/8989 - Loss: 0.3338\n",
            "  Batch 7100/8989 - Loss: 0.3332\n",
            "  Batch 7200/8989 - Loss: 0.3328\n",
            "  Batch 7300/8989 - Loss: 0.3324\n",
            "  Batch 7400/8989 - Loss: 0.3318\n",
            "  Batch 7500/8989 - Loss: 0.3313\n",
            "  Batch 7600/8989 - Loss: 0.3309\n",
            "  Batch 7700/8989 - Loss: 0.3304\n",
            "  Batch 7800/8989 - Loss: 0.3301\n",
            "  Batch 7900/8989 - Loss: 0.3297\n",
            "  Batch 8000/8989 - Loss: 0.3293\n",
            "  Batch 8100/8989 - Loss: 0.3289\n",
            "  Batch 8200/8989 - Loss: 0.3285\n",
            "  Batch 8300/8989 - Loss: 0.3281\n",
            "  Batch 8400/8989 - Loss: 0.3277\n",
            "  Batch 8500/8989 - Loss: 0.3273\n",
            "  Batch 8600/8989 - Loss: 0.3269\n",
            "  Batch 8700/8989 - Loss: 0.3265\n",
            "  Batch 8800/8989 - Loss: 0.3261\n",
            "  Batch 8900/8989 - Loss: 0.3257\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.3254\n",
            "  HR@10: 0.7023\n",
            "  NDCG@10: 0.4255\n",
            "  ✓ New best model! (HR@10: 0.7023)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 2/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2771\n",
            "  Batch 200/8989 - Loss: 0.2769\n",
            "  Batch 300/8989 - Loss: 0.2743\n",
            "  Batch 400/8989 - Loss: 0.2734\n",
            "  Batch 500/8989 - Loss: 0.2743\n",
            "  Batch 600/8989 - Loss: 0.2739\n",
            "  Batch 700/8989 - Loss: 0.2731\n",
            "  Batch 800/8989 - Loss: 0.2735\n",
            "  Batch 900/8989 - Loss: 0.2737\n",
            "  Batch 1000/8989 - Loss: 0.2741\n",
            "  Batch 1100/8989 - Loss: 0.2745\n",
            "  Batch 1200/8989 - Loss: 0.2747\n",
            "  Batch 1300/8989 - Loss: 0.2743\n",
            "  Batch 1400/8989 - Loss: 0.2742\n",
            "  Batch 1500/8989 - Loss: 0.2743\n",
            "  Batch 1600/8989 - Loss: 0.2746\n",
            "  Batch 1700/8989 - Loss: 0.2745\n",
            "  Batch 1800/8989 - Loss: 0.2745\n",
            "  Batch 1900/8989 - Loss: 0.2750\n",
            "  Batch 2000/8989 - Loss: 0.2751\n",
            "  Batch 2100/8989 - Loss: 0.2748\n",
            "  Batch 2200/8989 - Loss: 0.2749\n",
            "  Batch 2300/8989 - Loss: 0.2746\n",
            "  Batch 2400/8989 - Loss: 0.2747\n",
            "  Batch 2500/8989 - Loss: 0.2746\n",
            "  Batch 2600/8989 - Loss: 0.2742\n",
            "  Batch 2700/8989 - Loss: 0.2743\n",
            "  Batch 2800/8989 - Loss: 0.2743\n",
            "  Batch 2900/8989 - Loss: 0.2744\n",
            "  Batch 3000/8989 - Loss: 0.2743\n",
            "  Batch 3100/8989 - Loss: 0.2742\n",
            "  Batch 3200/8989 - Loss: 0.2740\n",
            "  Batch 3300/8989 - Loss: 0.2739\n",
            "  Batch 3400/8989 - Loss: 0.2737\n",
            "  Batch 3500/8989 - Loss: 0.2738\n",
            "  Batch 3600/8989 - Loss: 0.2737\n",
            "  Batch 3700/8989 - Loss: 0.2737\n",
            "  Batch 3800/8989 - Loss: 0.2737\n",
            "  Batch 3900/8989 - Loss: 0.2736\n",
            "  Batch 4000/8989 - Loss: 0.2735\n",
            "  Batch 4100/8989 - Loss: 0.2734\n",
            "  Batch 4200/8989 - Loss: 0.2733\n",
            "  Batch 4300/8989 - Loss: 0.2732\n",
            "  Batch 4400/8989 - Loss: 0.2732\n",
            "  Batch 4500/8989 - Loss: 0.2732\n",
            "  Batch 4600/8989 - Loss: 0.2732\n",
            "  Batch 4700/8989 - Loss: 0.2732\n",
            "  Batch 4800/8989 - Loss: 0.2731\n",
            "  Batch 4900/8989 - Loss: 0.2731\n",
            "  Batch 5000/8989 - Loss: 0.2730\n",
            "  Batch 5100/8989 - Loss: 0.2730\n",
            "  Batch 5200/8989 - Loss: 0.2730\n",
            "  Batch 5300/8989 - Loss: 0.2730\n",
            "  Batch 5400/8989 - Loss: 0.2729\n",
            "  Batch 5500/8989 - Loss: 0.2728\n",
            "  Batch 5600/8989 - Loss: 0.2727\n",
            "  Batch 5700/8989 - Loss: 0.2726\n",
            "  Batch 5800/8989 - Loss: 0.2725\n",
            "  Batch 5900/8989 - Loss: 0.2723\n",
            "  Batch 6000/8989 - Loss: 0.2723\n",
            "  Batch 6100/8989 - Loss: 0.2723\n",
            "  Batch 6200/8989 - Loss: 0.2722\n",
            "  Batch 6300/8989 - Loss: 0.2721\n",
            "  Batch 6400/8989 - Loss: 0.2721\n",
            "  Batch 6500/8989 - Loss: 0.2719\n",
            "  Batch 6600/8989 - Loss: 0.2719\n",
            "  Batch 6700/8989 - Loss: 0.2718\n",
            "  Batch 6800/8989 - Loss: 0.2717\n",
            "  Batch 6900/8989 - Loss: 0.2716\n",
            "  Batch 7000/8989 - Loss: 0.2716\n",
            "  Batch 7100/8989 - Loss: 0.2715\n",
            "  Batch 7200/8989 - Loss: 0.2714\n",
            "  Batch 7300/8989 - Loss: 0.2713\n",
            "  Batch 7400/8989 - Loss: 0.2712\n",
            "  Batch 7500/8989 - Loss: 0.2712\n",
            "  Batch 7600/8989 - Loss: 0.2711\n",
            "  Batch 7700/8989 - Loss: 0.2710\n",
            "  Batch 7800/8989 - Loss: 0.2710\n",
            "  Batch 7900/8989 - Loss: 0.2709\n",
            "  Batch 8000/8989 - Loss: 0.2709\n",
            "  Batch 8100/8989 - Loss: 0.2708\n",
            "  Batch 8200/8989 - Loss: 0.2706\n",
            "  Batch 8300/8989 - Loss: 0.2706\n",
            "  Batch 8400/8989 - Loss: 0.2704\n",
            "  Batch 8500/8989 - Loss: 0.2704\n",
            "  Batch 8600/8989 - Loss: 0.2703\n",
            "  Batch 8700/8989 - Loss: 0.2703\n",
            "  Batch 8800/8989 - Loss: 0.2702\n",
            "  Batch 8900/8989 - Loss: 0.2702\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2701\n",
            "  HR@10: 0.7352\n",
            "  NDCG@10: 0.4519\n",
            "  ✓ New best model! (HR@10: 0.7352)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 3/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2444\n",
            "  Batch 200/8989 - Loss: 0.2429\n",
            "  Batch 300/8989 - Loss: 0.2442\n",
            "  Batch 400/8989 - Loss: 0.2455\n",
            "  Batch 500/8989 - Loss: 0.2456\n",
            "  Batch 600/8989 - Loss: 0.2453\n",
            "  Batch 700/8989 - Loss: 0.2455\n",
            "  Batch 800/8989 - Loss: 0.2456\n",
            "  Batch 900/8989 - Loss: 0.2454\n",
            "  Batch 1000/8989 - Loss: 0.2456\n",
            "  Batch 1100/8989 - Loss: 0.2458\n",
            "  Batch 1200/8989 - Loss: 0.2463\n",
            "  Batch 1300/8989 - Loss: 0.2461\n",
            "  Batch 1400/8989 - Loss: 0.2462\n",
            "  Batch 1500/8989 - Loss: 0.2462\n",
            "  Batch 1600/8989 - Loss: 0.2461\n",
            "  Batch 1700/8989 - Loss: 0.2458\n",
            "  Batch 1800/8989 - Loss: 0.2459\n",
            "  Batch 1900/8989 - Loss: 0.2461\n",
            "  Batch 2000/8989 - Loss: 0.2462\n",
            "  Batch 2100/8989 - Loss: 0.2465\n",
            "  Batch 2200/8989 - Loss: 0.2467\n",
            "  Batch 2300/8989 - Loss: 0.2468\n",
            "  Batch 2400/8989 - Loss: 0.2469\n",
            "  Batch 2500/8989 - Loss: 0.2469\n",
            "  Batch 2600/8989 - Loss: 0.2469\n",
            "  Batch 2700/8989 - Loss: 0.2470\n",
            "  Batch 2800/8989 - Loss: 0.2471\n",
            "  Batch 2900/8989 - Loss: 0.2472\n",
            "  Batch 3000/8989 - Loss: 0.2472\n",
            "  Batch 3100/8989 - Loss: 0.2474\n",
            "  Batch 3200/8989 - Loss: 0.2475\n",
            "  Batch 3300/8989 - Loss: 0.2476\n",
            "  Batch 3400/8989 - Loss: 0.2477\n",
            "  Batch 3500/8989 - Loss: 0.2478\n",
            "  Batch 3600/8989 - Loss: 0.2479\n",
            "  Batch 3700/8989 - Loss: 0.2479\n",
            "  Batch 3800/8989 - Loss: 0.2478\n",
            "  Batch 3900/8989 - Loss: 0.2477\n",
            "  Batch 4000/8989 - Loss: 0.2478\n",
            "  Batch 4100/8989 - Loss: 0.2478\n",
            "  Batch 4200/8989 - Loss: 0.2478\n",
            "  Batch 4300/8989 - Loss: 0.2478\n",
            "  Batch 4400/8989 - Loss: 0.2479\n",
            "  Batch 4500/8989 - Loss: 0.2480\n",
            "  Batch 4600/8989 - Loss: 0.2480\n",
            "  Batch 4700/8989 - Loss: 0.2479\n",
            "  Batch 4800/8989 - Loss: 0.2479\n",
            "  Batch 4900/8989 - Loss: 0.2479\n",
            "  Batch 5000/8989 - Loss: 0.2480\n",
            "  Batch 5100/8989 - Loss: 0.2480\n",
            "  Batch 5200/8989 - Loss: 0.2481\n",
            "  Batch 5300/8989 - Loss: 0.2482\n",
            "  Batch 5400/8989 - Loss: 0.2483\n",
            "  Batch 5500/8989 - Loss: 0.2482\n",
            "  Batch 5600/8989 - Loss: 0.2482\n",
            "  Batch 5700/8989 - Loss: 0.2482\n",
            "  Batch 5800/8989 - Loss: 0.2483\n",
            "  Batch 5900/8989 - Loss: 0.2484\n",
            "  Batch 6000/8989 - Loss: 0.2483\n",
            "  Batch 6100/8989 - Loss: 0.2484\n",
            "  Batch 6200/8989 - Loss: 0.2484\n",
            "  Batch 6300/8989 - Loss: 0.2484\n",
            "  Batch 6400/8989 - Loss: 0.2483\n",
            "  Batch 6500/8989 - Loss: 0.2484\n",
            "  Batch 6600/8989 - Loss: 0.2484\n",
            "  Batch 6700/8989 - Loss: 0.2483\n",
            "  Batch 6800/8989 - Loss: 0.2483\n",
            "  Batch 6900/8989 - Loss: 0.2483\n",
            "  Batch 7000/8989 - Loss: 0.2484\n",
            "  Batch 7100/8989 - Loss: 0.2483\n",
            "  Batch 7200/8989 - Loss: 0.2483\n",
            "  Batch 7300/8989 - Loss: 0.2484\n",
            "  Batch 7400/8989 - Loss: 0.2484\n",
            "  Batch 7500/8989 - Loss: 0.2484\n",
            "  Batch 7600/8989 - Loss: 0.2484\n",
            "  Batch 7700/8989 - Loss: 0.2484\n",
            "  Batch 7800/8989 - Loss: 0.2485\n",
            "  Batch 7900/8989 - Loss: 0.2484\n",
            "  Batch 8000/8989 - Loss: 0.2485\n",
            "  Batch 8100/8989 - Loss: 0.2485\n",
            "  Batch 8200/8989 - Loss: 0.2484\n",
            "  Batch 8300/8989 - Loss: 0.2484\n",
            "  Batch 8400/8989 - Loss: 0.2485\n",
            "  Batch 8500/8989 - Loss: 0.2485\n",
            "  Batch 8600/8989 - Loss: 0.2486\n",
            "  Batch 8700/8989 - Loss: 0.2486\n",
            "  Batch 8800/8989 - Loss: 0.2486\n",
            "  Batch 8900/8989 - Loss: 0.2486\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2486\n",
            "  HR@10: 0.7404\n",
            "  NDCG@10: 0.4565\n",
            "  ✓ New best model! (HR@10: 0.7404)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 4/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2311\n",
            "  Batch 200/8989 - Loss: 0.2298\n",
            "  Batch 300/8989 - Loss: 0.2286\n",
            "  Batch 400/8989 - Loss: 0.2289\n",
            "  Batch 500/8989 - Loss: 0.2285\n",
            "  Batch 600/8989 - Loss: 0.2278\n",
            "  Batch 700/8989 - Loss: 0.2289\n",
            "  Batch 800/8989 - Loss: 0.2291\n",
            "  Batch 900/8989 - Loss: 0.2291\n",
            "  Batch 1000/8989 - Loss: 0.2292\n",
            "  Batch 1100/8989 - Loss: 0.2296\n",
            "  Batch 1200/8989 - Loss: 0.2296\n",
            "  Batch 1300/8989 - Loss: 0.2303\n",
            "  Batch 1400/8989 - Loss: 0.2303\n",
            "  Batch 1500/8989 - Loss: 0.2307\n",
            "  Batch 1600/8989 - Loss: 0.2307\n",
            "  Batch 1700/8989 - Loss: 0.2307\n",
            "  Batch 1800/8989 - Loss: 0.2307\n",
            "  Batch 1900/8989 - Loss: 0.2308\n",
            "  Batch 2000/8989 - Loss: 0.2310\n",
            "  Batch 2100/8989 - Loss: 0.2311\n",
            "  Batch 2200/8989 - Loss: 0.2312\n",
            "  Batch 2300/8989 - Loss: 0.2313\n",
            "  Batch 2400/8989 - Loss: 0.2315\n",
            "  Batch 2500/8989 - Loss: 0.2317\n",
            "  Batch 2600/8989 - Loss: 0.2320\n",
            "  Batch 2700/8989 - Loss: 0.2320\n",
            "  Batch 2800/8989 - Loss: 0.2321\n",
            "  Batch 2900/8989 - Loss: 0.2322\n",
            "  Batch 3000/8989 - Loss: 0.2324\n",
            "  Batch 3100/8989 - Loss: 0.2325\n",
            "  Batch 3200/8989 - Loss: 0.2323\n",
            "  Batch 3300/8989 - Loss: 0.2323\n",
            "  Batch 3400/8989 - Loss: 0.2325\n",
            "  Batch 3500/8989 - Loss: 0.2325\n",
            "  Batch 3600/8989 - Loss: 0.2325\n",
            "  Batch 3700/8989 - Loss: 0.2326\n",
            "  Batch 3800/8989 - Loss: 0.2327\n",
            "  Batch 3900/8989 - Loss: 0.2327\n",
            "  Batch 4000/8989 - Loss: 0.2328\n",
            "  Batch 4100/8989 - Loss: 0.2329\n",
            "  Batch 4200/8989 - Loss: 0.2331\n",
            "  Batch 4300/8989 - Loss: 0.2332\n",
            "  Batch 4400/8989 - Loss: 0.2334\n",
            "  Batch 4500/8989 - Loss: 0.2335\n",
            "  Batch 4600/8989 - Loss: 0.2336\n",
            "  Batch 4700/8989 - Loss: 0.2338\n",
            "  Batch 4800/8989 - Loss: 0.2337\n",
            "  Batch 4900/8989 - Loss: 0.2338\n",
            "  Batch 5000/8989 - Loss: 0.2339\n",
            "  Batch 5100/8989 - Loss: 0.2339\n",
            "  Batch 5200/8989 - Loss: 0.2340\n",
            "  Batch 5300/8989 - Loss: 0.2340\n",
            "  Batch 5400/8989 - Loss: 0.2341\n",
            "  Batch 5500/8989 - Loss: 0.2341\n",
            "  Batch 5600/8989 - Loss: 0.2341\n",
            "  Batch 5700/8989 - Loss: 0.2342\n",
            "  Batch 5800/8989 - Loss: 0.2341\n",
            "  Batch 5900/8989 - Loss: 0.2342\n",
            "  Batch 6000/8989 - Loss: 0.2343\n",
            "  Batch 6100/8989 - Loss: 0.2345\n",
            "  Batch 6200/8989 - Loss: 0.2345\n",
            "  Batch 6300/8989 - Loss: 0.2346\n",
            "  Batch 6400/8989 - Loss: 0.2347\n",
            "  Batch 6500/8989 - Loss: 0.2347\n",
            "  Batch 6600/8989 - Loss: 0.2348\n",
            "  Batch 6700/8989 - Loss: 0.2348\n",
            "  Batch 6800/8989 - Loss: 0.2348\n",
            "  Batch 6900/8989 - Loss: 0.2349\n",
            "  Batch 7000/8989 - Loss: 0.2350\n",
            "  Batch 7100/8989 - Loss: 0.2350\n",
            "  Batch 7200/8989 - Loss: 0.2351\n",
            "  Batch 7300/8989 - Loss: 0.2351\n",
            "  Batch 7400/8989 - Loss: 0.2352\n",
            "  Batch 7500/8989 - Loss: 0.2352\n",
            "  Batch 7600/8989 - Loss: 0.2353\n",
            "  Batch 7700/8989 - Loss: 0.2354\n",
            "  Batch 7800/8989 - Loss: 0.2354\n",
            "  Batch 7900/8989 - Loss: 0.2355\n",
            "  Batch 8000/8989 - Loss: 0.2355\n",
            "  Batch 8100/8989 - Loss: 0.2356\n",
            "  Batch 8200/8989 - Loss: 0.2356\n",
            "  Batch 8300/8989 - Loss: 0.2356\n",
            "  Batch 8400/8989 - Loss: 0.2357\n",
            "  Batch 8500/8989 - Loss: 0.2357\n",
            "  Batch 8600/8989 - Loss: 0.2357\n",
            "  Batch 8700/8989 - Loss: 0.2358\n",
            "  Batch 8800/8989 - Loss: 0.2359\n",
            "  Batch 8900/8989 - Loss: 0.2360\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2361\n",
            "  HR@10: 0.7360\n",
            "  NDCG@10: 0.4530\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 5/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2147\n",
            "  Batch 200/8989 - Loss: 0.2130\n",
            "  Batch 300/8989 - Loss: 0.2138\n",
            "  Batch 400/8989 - Loss: 0.2132\n",
            "  Batch 500/8989 - Loss: 0.2147\n",
            "  Batch 600/8989 - Loss: 0.2154\n",
            "  Batch 700/8989 - Loss: 0.2154\n",
            "  Batch 800/8989 - Loss: 0.2164\n",
            "  Batch 900/8989 - Loss: 0.2166\n",
            "  Batch 1000/8989 - Loss: 0.2165\n",
            "  Batch 1100/8989 - Loss: 0.2169\n",
            "  Batch 1200/8989 - Loss: 0.2172\n",
            "  Batch 1300/8989 - Loss: 0.2173\n",
            "  Batch 1400/8989 - Loss: 0.2176\n",
            "  Batch 1500/8989 - Loss: 0.2179\n",
            "  Batch 1600/8989 - Loss: 0.2184\n",
            "  Batch 1700/8989 - Loss: 0.2187\n",
            "  Batch 1800/8989 - Loss: 0.2185\n",
            "  Batch 1900/8989 - Loss: 0.2190\n",
            "  Batch 2000/8989 - Loss: 0.2190\n",
            "  Batch 2100/8989 - Loss: 0.2193\n",
            "  Batch 2200/8989 - Loss: 0.2193\n",
            "  Batch 2300/8989 - Loss: 0.2197\n",
            "  Batch 2400/8989 - Loss: 0.2201\n",
            "  Batch 2500/8989 - Loss: 0.2203\n",
            "  Batch 2600/8989 - Loss: 0.2206\n",
            "  Batch 2700/8989 - Loss: 0.2208\n",
            "  Batch 2800/8989 - Loss: 0.2210\n",
            "  Batch 2900/8989 - Loss: 0.2213\n",
            "  Batch 3000/8989 - Loss: 0.2216\n",
            "  Batch 3100/8989 - Loss: 0.2216\n",
            "  Batch 3200/8989 - Loss: 0.2217\n",
            "  Batch 3300/8989 - Loss: 0.2218\n",
            "  Batch 3400/8989 - Loss: 0.2219\n",
            "  Batch 3500/8989 - Loss: 0.2219\n",
            "  Batch 3600/8989 - Loss: 0.2222\n",
            "  Batch 3700/8989 - Loss: 0.2224\n",
            "  Batch 3800/8989 - Loss: 0.2225\n",
            "  Batch 3900/8989 - Loss: 0.2225\n",
            "  Batch 4000/8989 - Loss: 0.2226\n",
            "  Batch 4100/8989 - Loss: 0.2227\n",
            "  Batch 4200/8989 - Loss: 0.2227\n",
            "  Batch 4300/8989 - Loss: 0.2229\n",
            "  Batch 4400/8989 - Loss: 0.2230\n",
            "  Batch 4500/8989 - Loss: 0.2231\n",
            "  Batch 4600/8989 - Loss: 0.2232\n",
            "  Batch 4700/8989 - Loss: 0.2232\n",
            "  Batch 4800/8989 - Loss: 0.2232\n",
            "  Batch 4900/8989 - Loss: 0.2234\n",
            "  Batch 5000/8989 - Loss: 0.2235\n",
            "  Batch 5100/8989 - Loss: 0.2235\n",
            "  Batch 5200/8989 - Loss: 0.2236\n",
            "  Batch 5300/8989 - Loss: 0.2237\n",
            "  Batch 5400/8989 - Loss: 0.2237\n",
            "  Batch 5500/8989 - Loss: 0.2239\n",
            "  Batch 5600/8989 - Loss: 0.2240\n",
            "  Batch 5700/8989 - Loss: 0.2240\n",
            "  Batch 5800/8989 - Loss: 0.2241\n",
            "  Batch 5900/8989 - Loss: 0.2242\n",
            "  Batch 6000/8989 - Loss: 0.2244\n",
            "  Batch 6100/8989 - Loss: 0.2244\n",
            "  Batch 6200/8989 - Loss: 0.2245\n",
            "  Batch 6300/8989 - Loss: 0.2246\n",
            "  Batch 6400/8989 - Loss: 0.2247\n",
            "  Batch 6500/8989 - Loss: 0.2247\n",
            "  Batch 6600/8989 - Loss: 0.2249\n",
            "  Batch 6700/8989 - Loss: 0.2249\n",
            "  Batch 6800/8989 - Loss: 0.2251\n",
            "  Batch 6900/8989 - Loss: 0.2252\n",
            "  Batch 7000/8989 - Loss: 0.2253\n",
            "  Batch 7100/8989 - Loss: 0.2254\n",
            "  Batch 7200/8989 - Loss: 0.2254\n",
            "  Batch 7300/8989 - Loss: 0.2255\n",
            "  Batch 7400/8989 - Loss: 0.2256\n",
            "  Batch 7500/8989 - Loss: 0.2257\n",
            "  Batch 7600/8989 - Loss: 0.2258\n",
            "  Batch 7700/8989 - Loss: 0.2258\n",
            "  Batch 7800/8989 - Loss: 0.2258\n",
            "  Batch 7900/8989 - Loss: 0.2259\n",
            "  Batch 8000/8989 - Loss: 0.2260\n",
            "  Batch 8100/8989 - Loss: 0.2261\n",
            "  Batch 8200/8989 - Loss: 0.2263\n",
            "  Batch 8300/8989 - Loss: 0.2264\n",
            "  Batch 8400/8989 - Loss: 0.2264\n",
            "  Batch 8500/8989 - Loss: 0.2264\n",
            "  Batch 8600/8989 - Loss: 0.2265\n",
            "  Batch 8700/8989 - Loss: 0.2265\n",
            "  Batch 8800/8989 - Loss: 0.2266\n",
            "  Batch 8900/8989 - Loss: 0.2267\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:41\n",
            "  Loss: 0.2267\n",
            "  HR@10: 0.7368\n",
            "  NDCG@10: 0.4552\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 6/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2047\n",
            "  Batch 200/8989 - Loss: 0.2034\n",
            "  Batch 300/8989 - Loss: 0.2063\n",
            "  Batch 400/8989 - Loss: 0.2079\n",
            "  Batch 500/8989 - Loss: 0.2085\n",
            "  Batch 600/8989 - Loss: 0.2084\n",
            "  Batch 700/8989 - Loss: 0.2090\n",
            "  Batch 800/8989 - Loss: 0.2090\n",
            "  Batch 900/8989 - Loss: 0.2089\n",
            "  Batch 1000/8989 - Loss: 0.2087\n",
            "  Batch 1100/8989 - Loss: 0.2085\n",
            "  Batch 1200/8989 - Loss: 0.2089\n",
            "  Batch 1300/8989 - Loss: 0.2090\n",
            "  Batch 1400/8989 - Loss: 0.2095\n",
            "  Batch 1500/8989 - Loss: 0.2099\n",
            "  Batch 1600/8989 - Loss: 0.2101\n",
            "  Batch 1700/8989 - Loss: 0.2104\n",
            "  Batch 1800/8989 - Loss: 0.2105\n",
            "  Batch 1900/8989 - Loss: 0.2107\n",
            "  Batch 2000/8989 - Loss: 0.2108\n",
            "  Batch 2100/8989 - Loss: 0.2110\n",
            "  Batch 2200/8989 - Loss: 0.2110\n",
            "  Batch 2300/8989 - Loss: 0.2113\n",
            "  Batch 2400/8989 - Loss: 0.2115\n",
            "  Batch 2500/8989 - Loss: 0.2116\n",
            "  Batch 2600/8989 - Loss: 0.2119\n",
            "  Batch 2700/8989 - Loss: 0.2119\n",
            "  Batch 2800/8989 - Loss: 0.2121\n",
            "  Batch 2900/8989 - Loss: 0.2122\n",
            "  Batch 3000/8989 - Loss: 0.2124\n",
            "  Batch 3100/8989 - Loss: 0.2126\n",
            "  Batch 3200/8989 - Loss: 0.2128\n",
            "  Batch 3300/8989 - Loss: 0.2131\n",
            "  Batch 3400/8989 - Loss: 0.2132\n",
            "  Batch 3500/8989 - Loss: 0.2132\n",
            "  Batch 3600/8989 - Loss: 0.2135\n",
            "  Batch 3700/8989 - Loss: 0.2138\n",
            "  Batch 3800/8989 - Loss: 0.2140\n",
            "  Batch 3900/8989 - Loss: 0.2140\n",
            "  Batch 4000/8989 - Loss: 0.2140\n",
            "  Batch 4100/8989 - Loss: 0.2142\n",
            "  Batch 4200/8989 - Loss: 0.2144\n",
            "  Batch 4300/8989 - Loss: 0.2145\n",
            "  Batch 4400/8989 - Loss: 0.2147\n",
            "  Batch 4500/8989 - Loss: 0.2149\n",
            "  Batch 4600/8989 - Loss: 0.2150\n",
            "  Batch 4700/8989 - Loss: 0.2151\n",
            "  Batch 4800/8989 - Loss: 0.2152\n",
            "  Batch 4900/8989 - Loss: 0.2153\n",
            "  Batch 5000/8989 - Loss: 0.2154\n",
            "  Batch 5100/8989 - Loss: 0.2155\n",
            "  Batch 5200/8989 - Loss: 0.2157\n",
            "  Batch 5300/8989 - Loss: 0.2157\n",
            "  Batch 5400/8989 - Loss: 0.2158\n",
            "  Batch 5500/8989 - Loss: 0.2159\n",
            "  Batch 5600/8989 - Loss: 0.2160\n",
            "  Batch 5700/8989 - Loss: 0.2162\n",
            "  Batch 5800/8989 - Loss: 0.2163\n",
            "  Batch 5900/8989 - Loss: 0.2165\n",
            "  Batch 6000/8989 - Loss: 0.2165\n",
            "  Batch 6100/8989 - Loss: 0.2167\n",
            "  Batch 6200/8989 - Loss: 0.2168\n",
            "  Batch 6300/8989 - Loss: 0.2170\n",
            "  Batch 6400/8989 - Loss: 0.2171\n",
            "  Batch 6500/8989 - Loss: 0.2173\n",
            "  Batch 6600/8989 - Loss: 0.2173\n",
            "  Batch 6700/8989 - Loss: 0.2175\n",
            "  Batch 6800/8989 - Loss: 0.2177\n",
            "  Batch 6900/8989 - Loss: 0.2178\n",
            "  Batch 7000/8989 - Loss: 0.2178\n",
            "  Batch 7100/8989 - Loss: 0.2179\n",
            "  Batch 7200/8989 - Loss: 0.2180\n",
            "  Batch 7300/8989 - Loss: 0.2181\n",
            "  Batch 7400/8989 - Loss: 0.2182\n",
            "  Batch 7500/8989 - Loss: 0.2183\n",
            "  Batch 7600/8989 - Loss: 0.2184\n",
            "  Batch 7700/8989 - Loss: 0.2185\n",
            "  Batch 7800/8989 - Loss: 0.2186\n",
            "  Batch 7900/8989 - Loss: 0.2187\n",
            "  Batch 8000/8989 - Loss: 0.2188\n",
            "  Batch 8100/8989 - Loss: 0.2189\n",
            "  Batch 8200/8989 - Loss: 0.2190\n",
            "  Batch 8300/8989 - Loss: 0.2191\n",
            "  Batch 8400/8989 - Loss: 0.2192\n",
            "  Batch 8500/8989 - Loss: 0.2193\n",
            "  Batch 8600/8989 - Loss: 0.2194\n",
            "  Batch 8700/8989 - Loss: 0.2195\n",
            "  Batch 8800/8989 - Loss: 0.2195\n",
            "  Batch 8900/8989 - Loss: 0.2196\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:40\n",
            "  Loss: 0.2197\n",
            "  HR@10: 0.7336\n",
            "  NDCG@10: 0.4491\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 7/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2048\n",
            "  Batch 200/8989 - Loss: 0.2033\n",
            "  Batch 300/8989 - Loss: 0.2040\n",
            "  Batch 400/8989 - Loss: 0.2027\n",
            "  Batch 500/8989 - Loss: 0.2026\n",
            "  Batch 600/8989 - Loss: 0.2024\n",
            "  Batch 700/8989 - Loss: 0.2020\n",
            "  Batch 800/8989 - Loss: 0.2024\n",
            "  Batch 900/8989 - Loss: 0.2028\n",
            "  Batch 1000/8989 - Loss: 0.2032\n",
            "  Batch 1100/8989 - Loss: 0.2036\n",
            "  Batch 1200/8989 - Loss: 0.2037\n",
            "  Batch 1300/8989 - Loss: 0.2037\n",
            "  Batch 1400/8989 - Loss: 0.2040\n",
            "  Batch 1500/8989 - Loss: 0.2039\n",
            "  Batch 1600/8989 - Loss: 0.2041\n",
            "  Batch 1700/8989 - Loss: 0.2043\n",
            "  Batch 1800/8989 - Loss: 0.2043\n",
            "  Batch 1900/8989 - Loss: 0.2044\n",
            "  Batch 2000/8989 - Loss: 0.2048\n",
            "  Batch 2100/8989 - Loss: 0.2049\n",
            "  Batch 2200/8989 - Loss: 0.2051\n",
            "  Batch 2300/8989 - Loss: 0.2053\n",
            "  Batch 2400/8989 - Loss: 0.2056\n",
            "  Batch 2500/8989 - Loss: 0.2058\n",
            "  Batch 2600/8989 - Loss: 0.2058\n",
            "  Batch 2700/8989 - Loss: 0.2060\n",
            "  Batch 2800/8989 - Loss: 0.2061\n",
            "  Batch 2900/8989 - Loss: 0.2064\n",
            "  Batch 3000/8989 - Loss: 0.2067\n",
            "  Batch 3100/8989 - Loss: 0.2070\n",
            "  Batch 3200/8989 - Loss: 0.2071\n",
            "  Batch 3300/8989 - Loss: 0.2073\n",
            "  Batch 3400/8989 - Loss: 0.2075\n",
            "  Batch 3500/8989 - Loss: 0.2076\n",
            "  Batch 3600/8989 - Loss: 0.2077\n",
            "  Batch 3700/8989 - Loss: 0.2078\n",
            "  Batch 3800/8989 - Loss: 0.2079\n",
            "  Batch 3900/8989 - Loss: 0.2081\n",
            "  Batch 4000/8989 - Loss: 0.2083\n",
            "  Batch 4100/8989 - Loss: 0.2084\n",
            "  Batch 4200/8989 - Loss: 0.2085\n",
            "  Batch 4300/8989 - Loss: 0.2086\n",
            "  Batch 4400/8989 - Loss: 0.2088\n",
            "  Batch 4500/8989 - Loss: 0.2089\n",
            "  Batch 4600/8989 - Loss: 0.2092\n",
            "  Batch 4700/8989 - Loss: 0.2093\n",
            "  Batch 4800/8989 - Loss: 0.2094\n",
            "  Batch 4900/8989 - Loss: 0.2096\n",
            "  Batch 5000/8989 - Loss: 0.2098\n",
            "  Batch 5100/8989 - Loss: 0.2098\n",
            "  Batch 5200/8989 - Loss: 0.2099\n",
            "  Batch 5300/8989 - Loss: 0.2100\n",
            "  Batch 5400/8989 - Loss: 0.2101\n",
            "  Batch 5500/8989 - Loss: 0.2103\n",
            "  Batch 5600/8989 - Loss: 0.2105\n",
            "  Batch 5700/8989 - Loss: 0.2107\n",
            "  Batch 5800/8989 - Loss: 0.2108\n",
            "  Batch 5900/8989 - Loss: 0.2109\n",
            "  Batch 6000/8989 - Loss: 0.2110\n",
            "  Batch 6100/8989 - Loss: 0.2111\n",
            "  Batch 6200/8989 - Loss: 0.2112\n",
            "  Batch 6300/8989 - Loss: 0.2113\n",
            "  Batch 6400/8989 - Loss: 0.2115\n",
            "  Batch 6500/8989 - Loss: 0.2116\n",
            "  Batch 6600/8989 - Loss: 0.2117\n",
            "  Batch 6700/8989 - Loss: 0.2118\n",
            "  Batch 6800/8989 - Loss: 0.2119\n",
            "  Batch 6900/8989 - Loss: 0.2120\n",
            "  Batch 7000/8989 - Loss: 0.2121\n",
            "  Batch 7100/8989 - Loss: 0.2122\n",
            "  Batch 7200/8989 - Loss: 0.2123\n",
            "  Batch 7300/8989 - Loss: 0.2123\n",
            "  Batch 7400/8989 - Loss: 0.2125\n",
            "  Batch 7500/8989 - Loss: 0.2125\n",
            "  Batch 7600/8989 - Loss: 0.2126\n",
            "  Batch 7700/8989 - Loss: 0.2127\n",
            "  Batch 7800/8989 - Loss: 0.2127\n",
            "  Batch 7900/8989 - Loss: 0.2128\n",
            "  Batch 8000/8989 - Loss: 0.2129\n",
            "  Batch 8100/8989 - Loss: 0.2130\n",
            "  Batch 8200/8989 - Loss: 0.2130\n",
            "  Batch 8300/8989 - Loss: 0.2131\n",
            "  Batch 8400/8989 - Loss: 0.2132\n",
            "  Batch 8500/8989 - Loss: 0.2133\n",
            "  Batch 8600/8989 - Loss: 0.2133\n",
            "  Batch 8700/8989 - Loss: 0.2134\n",
            "  Batch 8800/8989 - Loss: 0.2135\n",
            "  Batch 8900/8989 - Loss: 0.2135\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.2135\n",
            "  HR@10: 0.7371\n",
            "  NDCG@10: 0.4524\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 8/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2013\n",
            "  Batch 200/8989 - Loss: 0.1996\n",
            "  Batch 300/8989 - Loss: 0.1989\n",
            "  Batch 400/8989 - Loss: 0.1981\n",
            "  Batch 500/8989 - Loss: 0.1975\n",
            "  Batch 600/8989 - Loss: 0.1976\n",
            "  Batch 700/8989 - Loss: 0.1983\n",
            "  Batch 800/8989 - Loss: 0.1986\n",
            "  Batch 900/8989 - Loss: 0.1992\n",
            "  Batch 1000/8989 - Loss: 0.1990\n",
            "  Batch 1100/8989 - Loss: 0.1993\n",
            "  Batch 1200/8989 - Loss: 0.1989\n",
            "  Batch 1300/8989 - Loss: 0.1991\n",
            "  Batch 1400/8989 - Loss: 0.1989\n",
            "  Batch 1500/8989 - Loss: 0.1987\n",
            "  Batch 1600/8989 - Loss: 0.1986\n",
            "  Batch 1700/8989 - Loss: 0.1988\n",
            "  Batch 1800/8989 - Loss: 0.1989\n",
            "  Batch 1900/8989 - Loss: 0.1991\n",
            "  Batch 2000/8989 - Loss: 0.1994\n",
            "  Batch 2100/8989 - Loss: 0.1996\n",
            "  Batch 2200/8989 - Loss: 0.1999\n",
            "  Batch 2300/8989 - Loss: 0.1998\n",
            "  Batch 2400/8989 - Loss: 0.2002\n",
            "  Batch 2500/8989 - Loss: 0.2002\n",
            "  Batch 2600/8989 - Loss: 0.2003\n",
            "  Batch 2700/8989 - Loss: 0.2005\n",
            "  Batch 2800/8989 - Loss: 0.2008\n",
            "  Batch 2900/8989 - Loss: 0.2008\n",
            "  Batch 3000/8989 - Loss: 0.2010\n",
            "  Batch 3100/8989 - Loss: 0.2012\n",
            "  Batch 3200/8989 - Loss: 0.2013\n",
            "  Batch 3300/8989 - Loss: 0.2013\n",
            "  Batch 3400/8989 - Loss: 0.2015\n",
            "  Batch 3500/8989 - Loss: 0.2016\n",
            "  Batch 3600/8989 - Loss: 0.2018\n",
            "  Batch 3700/8989 - Loss: 0.2019\n",
            "  Batch 3800/8989 - Loss: 0.2022\n",
            "  Batch 3900/8989 - Loss: 0.2024\n",
            "  Batch 4000/8989 - Loss: 0.2026\n",
            "  Batch 4100/8989 - Loss: 0.2028\n",
            "  Batch 4200/8989 - Loss: 0.2030\n",
            "  Batch 4300/8989 - Loss: 0.2031\n",
            "  Batch 4400/8989 - Loss: 0.2032\n",
            "  Batch 4500/8989 - Loss: 0.2033\n",
            "  Batch 4600/8989 - Loss: 0.2035\n",
            "  Batch 4700/8989 - Loss: 0.2038\n",
            "  Batch 4800/8989 - Loss: 0.2039\n",
            "  Batch 4900/8989 - Loss: 0.2040\n",
            "  Batch 5000/8989 - Loss: 0.2042\n",
            "  Batch 5100/8989 - Loss: 0.2043\n",
            "  Batch 5200/8989 - Loss: 0.2045\n",
            "  Batch 5300/8989 - Loss: 0.2046\n",
            "  Batch 5400/8989 - Loss: 0.2047\n",
            "  Batch 5500/8989 - Loss: 0.2048\n",
            "  Batch 5600/8989 - Loss: 0.2048\n",
            "  Batch 5700/8989 - Loss: 0.2050\n",
            "  Batch 5800/8989 - Loss: 0.2052\n",
            "  Batch 5900/8989 - Loss: 0.2053\n",
            "  Batch 6000/8989 - Loss: 0.2054\n",
            "  Batch 6100/8989 - Loss: 0.2055\n",
            "  Batch 6200/8989 - Loss: 0.2055\n",
            "  Batch 6300/8989 - Loss: 0.2057\n",
            "  Batch 6400/8989 - Loss: 0.2058\n",
            "  Batch 6500/8989 - Loss: 0.2060\n",
            "  Batch 6600/8989 - Loss: 0.2061\n",
            "  Batch 6700/8989 - Loss: 0.2062\n",
            "  Batch 6800/8989 - Loss: 0.2064\n",
            "  Batch 6900/8989 - Loss: 0.2065\n",
            "  Batch 7000/8989 - Loss: 0.2066\n",
            "  Batch 7100/8989 - Loss: 0.2068\n",
            "  Batch 7200/8989 - Loss: 0.2069\n",
            "  Batch 7300/8989 - Loss: 0.2071\n",
            "  Batch 7400/8989 - Loss: 0.2072\n",
            "  Batch 7500/8989 - Loss: 0.2072\n",
            "  Batch 7600/8989 - Loss: 0.2074\n",
            "  Batch 7700/8989 - Loss: 0.2075\n",
            "  Batch 7800/8989 - Loss: 0.2076\n",
            "  Batch 7900/8989 - Loss: 0.2077\n",
            "  Batch 8000/8989 - Loss: 0.2077\n",
            "  Batch 8100/8989 - Loss: 0.2078\n",
            "  Batch 8200/8989 - Loss: 0.2079\n",
            "  Batch 8300/8989 - Loss: 0.2079\n",
            "  Batch 8400/8989 - Loss: 0.2081\n",
            "  Batch 8500/8989 - Loss: 0.2081\n",
            "  Batch 8600/8989 - Loss: 0.2082\n",
            "  Batch 8700/8989 - Loss: 0.2083\n",
            "  Batch 8800/8989 - Loss: 0.2084\n",
            "  Batch 8900/8989 - Loss: 0.2085\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.2087\n",
            "  HR@10: 0.7341\n",
            "  NDCG@10: 0.4508\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 9/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1860\n",
            "  Batch 200/8989 - Loss: 0.1872\n",
            "  Batch 300/8989 - Loss: 0.1912\n",
            "  Batch 400/8989 - Loss: 0.1919\n",
            "  Batch 500/8989 - Loss: 0.1912\n",
            "  Batch 600/8989 - Loss: 0.1914\n",
            "  Batch 700/8989 - Loss: 0.1916\n",
            "  Batch 800/8989 - Loss: 0.1920\n",
            "  Batch 900/8989 - Loss: 0.1922\n",
            "  Batch 1000/8989 - Loss: 0.1928\n",
            "  Batch 1100/8989 - Loss: 0.1929\n",
            "  Batch 1200/8989 - Loss: 0.1932\n",
            "  Batch 1300/8989 - Loss: 0.1935\n",
            "  Batch 1400/8989 - Loss: 0.1941\n",
            "  Batch 1500/8989 - Loss: 0.1942\n",
            "  Batch 1600/8989 - Loss: 0.1944\n",
            "  Batch 1700/8989 - Loss: 0.1945\n",
            "  Batch 1800/8989 - Loss: 0.1949\n",
            "  Batch 1900/8989 - Loss: 0.1951\n",
            "  Batch 2000/8989 - Loss: 0.1954\n",
            "  Batch 2100/8989 - Loss: 0.1956\n",
            "  Batch 2200/8989 - Loss: 0.1956\n",
            "  Batch 2300/8989 - Loss: 0.1957\n",
            "  Batch 2400/8989 - Loss: 0.1958\n",
            "  Batch 2500/8989 - Loss: 0.1960\n",
            "  Batch 2600/8989 - Loss: 0.1961\n",
            "  Batch 2700/8989 - Loss: 0.1961\n",
            "  Batch 2800/8989 - Loss: 0.1963\n",
            "  Batch 2900/8989 - Loss: 0.1964\n",
            "  Batch 3000/8989 - Loss: 0.1965\n",
            "  Batch 3100/8989 - Loss: 0.1966\n",
            "  Batch 3200/8989 - Loss: 0.1967\n",
            "  Batch 3300/8989 - Loss: 0.1969\n",
            "  Batch 3400/8989 - Loss: 0.1970\n",
            "  Batch 3500/8989 - Loss: 0.1970\n",
            "  Batch 3600/8989 - Loss: 0.1971\n",
            "  Batch 3700/8989 - Loss: 0.1971\n",
            "  Batch 3800/8989 - Loss: 0.1973\n",
            "  Batch 3900/8989 - Loss: 0.1973\n",
            "  Batch 4000/8989 - Loss: 0.1974\n",
            "  Batch 4100/8989 - Loss: 0.1976\n",
            "  Batch 4200/8989 - Loss: 0.1977\n",
            "  Batch 4300/8989 - Loss: 0.1978\n",
            "  Batch 4400/8989 - Loss: 0.1981\n",
            "  Batch 4500/8989 - Loss: 0.1982\n",
            "  Batch 4600/8989 - Loss: 0.1985\n",
            "  Batch 4700/8989 - Loss: 0.1986\n",
            "  Batch 4800/8989 - Loss: 0.1987\n",
            "  Batch 4900/8989 - Loss: 0.1989\n",
            "  Batch 5000/8989 - Loss: 0.1991\n",
            "  Batch 5100/8989 - Loss: 0.1992\n",
            "  Batch 5200/8989 - Loss: 0.1993\n",
            "  Batch 5300/8989 - Loss: 0.1995\n",
            "  Batch 5400/8989 - Loss: 0.1997\n",
            "  Batch 5500/8989 - Loss: 0.1997\n",
            "  Batch 5600/8989 - Loss: 0.1999\n",
            "  Batch 5700/8989 - Loss: 0.2000\n",
            "  Batch 5800/8989 - Loss: 0.2002\n",
            "  Batch 5900/8989 - Loss: 0.2002\n",
            "  Batch 6000/8989 - Loss: 0.2004\n",
            "  Batch 6100/8989 - Loss: 0.2005\n",
            "  Batch 6200/8989 - Loss: 0.2006\n",
            "  Batch 6300/8989 - Loss: 0.2007\n",
            "  Batch 6400/8989 - Loss: 0.2009\n",
            "  Batch 6500/8989 - Loss: 0.2010\n",
            "  Batch 6600/8989 - Loss: 0.2010\n",
            "  Batch 6700/8989 - Loss: 0.2012\n",
            "  Batch 6800/8989 - Loss: 0.2013\n",
            "  Batch 6900/8989 - Loss: 0.2014\n",
            "  Batch 7000/8989 - Loss: 0.2015\n",
            "  Batch 7100/8989 - Loss: 0.2017\n",
            "  Batch 7200/8989 - Loss: 0.2018\n",
            "  Batch 7300/8989 - Loss: 0.2019\n",
            "  Batch 7400/8989 - Loss: 0.2020\n",
            "  Batch 7500/8989 - Loss: 0.2021\n",
            "  Batch 7600/8989 - Loss: 0.2022\n",
            "  Batch 7700/8989 - Loss: 0.2024\n",
            "  Batch 7800/8989 - Loss: 0.2025\n",
            "  Batch 7900/8989 - Loss: 0.2026\n",
            "  Batch 8000/8989 - Loss: 0.2027\n",
            "  Batch 8100/8989 - Loss: 0.2028\n",
            "  Batch 8200/8989 - Loss: 0.2029\n",
            "  Batch 8300/8989 - Loss: 0.2029\n",
            "  Batch 8400/8989 - Loss: 0.2030\n",
            "  Batch 8500/8989 - Loss: 0.2031\n",
            "  Batch 8600/8989 - Loss: 0.2032\n",
            "  Batch 8700/8989 - Loss: 0.2033\n",
            "  Batch 8800/8989 - Loss: 0.2034\n",
            "  Batch 8900/8989 - Loss: 0.2034\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.2036\n",
            "  HR@10: 0.7296\n",
            "  NDCG@10: 0.4493\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 10/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1828\n",
            "  Batch 200/8989 - Loss: 0.1842\n",
            "  Batch 300/8989 - Loss: 0.1859\n",
            "  Batch 400/8989 - Loss: 0.1876\n",
            "  Batch 500/8989 - Loss: 0.1881\n",
            "  Batch 600/8989 - Loss: 0.1883\n",
            "  Batch 700/8989 - Loss: 0.1885\n",
            "  Batch 800/8989 - Loss: 0.1879\n",
            "  Batch 900/8989 - Loss: 0.1883\n",
            "  Batch 1000/8989 - Loss: 0.1880\n",
            "  Batch 1100/8989 - Loss: 0.1884\n",
            "  Batch 1200/8989 - Loss: 0.1886\n",
            "  Batch 1300/8989 - Loss: 0.1888\n",
            "  Batch 1400/8989 - Loss: 0.1891\n",
            "  Batch 1500/8989 - Loss: 0.1893\n",
            "  Batch 1600/8989 - Loss: 0.1894\n",
            "  Batch 1700/8989 - Loss: 0.1896\n",
            "  Batch 1800/8989 - Loss: 0.1897\n",
            "  Batch 1900/8989 - Loss: 0.1898\n",
            "  Batch 2000/8989 - Loss: 0.1903\n",
            "  Batch 2100/8989 - Loss: 0.1903\n",
            "  Batch 2200/8989 - Loss: 0.1905\n",
            "  Batch 2300/8989 - Loss: 0.1908\n",
            "  Batch 2400/8989 - Loss: 0.1911\n",
            "  Batch 2500/8989 - Loss: 0.1912\n",
            "  Batch 2600/8989 - Loss: 0.1913\n",
            "  Batch 2700/8989 - Loss: 0.1913\n",
            "  Batch 2800/8989 - Loss: 0.1916\n",
            "  Batch 2900/8989 - Loss: 0.1917\n",
            "  Batch 3000/8989 - Loss: 0.1920\n",
            "  Batch 3100/8989 - Loss: 0.1922\n",
            "  Batch 3200/8989 - Loss: 0.1923\n",
            "  Batch 3300/8989 - Loss: 0.1924\n",
            "  Batch 3400/8989 - Loss: 0.1926\n",
            "  Batch 3500/8989 - Loss: 0.1927\n",
            "  Batch 3600/8989 - Loss: 0.1928\n",
            "  Batch 3700/8989 - Loss: 0.1930\n",
            "  Batch 3800/8989 - Loss: 0.1930\n",
            "  Batch 3900/8989 - Loss: 0.1930\n",
            "  Batch 4000/8989 - Loss: 0.1932\n",
            "  Batch 4100/8989 - Loss: 0.1933\n",
            "  Batch 4200/8989 - Loss: 0.1934\n",
            "  Batch 4300/8989 - Loss: 0.1935\n",
            "  Batch 4400/8989 - Loss: 0.1937\n",
            "  Batch 4500/8989 - Loss: 0.1938\n",
            "  Batch 4600/8989 - Loss: 0.1938\n",
            "  Batch 4700/8989 - Loss: 0.1940\n",
            "  Batch 4800/8989 - Loss: 0.1941\n",
            "  Batch 4900/8989 - Loss: 0.1944\n",
            "  Batch 5000/8989 - Loss: 0.1945\n",
            "  Batch 5100/8989 - Loss: 0.1946\n",
            "  Batch 5200/8989 - Loss: 0.1947\n",
            "  Batch 5300/8989 - Loss: 0.1949\n",
            "  Batch 5400/8989 - Loss: 0.1950\n",
            "  Batch 5500/8989 - Loss: 0.1950\n",
            "  Batch 5600/8989 - Loss: 0.1952\n",
            "  Batch 5700/8989 - Loss: 0.1953\n",
            "  Batch 5800/8989 - Loss: 0.1954\n",
            "  Batch 5900/8989 - Loss: 0.1955\n",
            "  Batch 6000/8989 - Loss: 0.1957\n",
            "  Batch 6100/8989 - Loss: 0.1958\n",
            "  Batch 6200/8989 - Loss: 0.1960\n",
            "  Batch 6300/8989 - Loss: 0.1961\n",
            "  Batch 6400/8989 - Loss: 0.1962\n",
            "  Batch 6500/8989 - Loss: 0.1964\n",
            "  Batch 6600/8989 - Loss: 0.1964\n",
            "  Batch 6700/8989 - Loss: 0.1965\n",
            "  Batch 6800/8989 - Loss: 0.1967\n",
            "  Batch 6900/8989 - Loss: 0.1968\n",
            "  Batch 7000/8989 - Loss: 0.1969\n",
            "  Batch 7100/8989 - Loss: 0.1969\n",
            "  Batch 7200/8989 - Loss: 0.1971\n",
            "  Batch 7300/8989 - Loss: 0.1972\n",
            "  Batch 7400/8989 - Loss: 0.1974\n",
            "  Batch 7500/8989 - Loss: 0.1975\n",
            "  Batch 7600/8989 - Loss: 0.1976\n",
            "  Batch 7700/8989 - Loss: 0.1977\n",
            "  Batch 7800/8989 - Loss: 0.1978\n",
            "  Batch 7900/8989 - Loss: 0.1980\n",
            "  Batch 8000/8989 - Loss: 0.1981\n",
            "  Batch 8100/8989 - Loss: 0.1982\n",
            "  Batch 8200/8989 - Loss: 0.1984\n",
            "  Batch 8300/8989 - Loss: 0.1985\n",
            "  Batch 8400/8989 - Loss: 0.1986\n",
            "  Batch 8500/8989 - Loss: 0.1988\n",
            "  Batch 8600/8989 - Loss: 0.1989\n",
            "  Batch 8700/8989 - Loss: 0.1989\n",
            "  Batch 8800/8989 - Loss: 0.1991\n",
            "  Batch 8900/8989 - Loss: 0.1992\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1993\n",
            "  HR@10: 0.7305\n",
            "  NDCG@10: 0.4487\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 11/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1805\n",
            "  Batch 200/8989 - Loss: 0.1817\n",
            "  Batch 300/8989 - Loss: 0.1816\n",
            "  Batch 400/8989 - Loss: 0.1821\n",
            "  Batch 500/8989 - Loss: 0.1823\n",
            "  Batch 600/8989 - Loss: 0.1833\n",
            "  Batch 700/8989 - Loss: 0.1838\n",
            "  Batch 800/8989 - Loss: 0.1840\n",
            "  Batch 900/8989 - Loss: 0.1835\n",
            "  Batch 1000/8989 - Loss: 0.1837\n",
            "  Batch 1100/8989 - Loss: 0.1837\n",
            "  Batch 1200/8989 - Loss: 0.1837\n",
            "  Batch 1300/8989 - Loss: 0.1836\n",
            "  Batch 1400/8989 - Loss: 0.1837\n",
            "  Batch 1500/8989 - Loss: 0.1837\n",
            "  Batch 1600/8989 - Loss: 0.1841\n",
            "  Batch 1700/8989 - Loss: 0.1842\n",
            "  Batch 1800/8989 - Loss: 0.1842\n",
            "  Batch 1900/8989 - Loss: 0.1845\n",
            "  Batch 2000/8989 - Loss: 0.1846\n",
            "  Batch 2100/8989 - Loss: 0.1850\n",
            "  Batch 2200/8989 - Loss: 0.1854\n",
            "  Batch 2300/8989 - Loss: 0.1856\n",
            "  Batch 2400/8989 - Loss: 0.1857\n",
            "  Batch 2500/8989 - Loss: 0.1857\n",
            "  Batch 2600/8989 - Loss: 0.1858\n",
            "  Batch 2700/8989 - Loss: 0.1858\n",
            "  Batch 2800/8989 - Loss: 0.1859\n",
            "  Batch 2900/8989 - Loss: 0.1862\n",
            "  Batch 3000/8989 - Loss: 0.1863\n",
            "  Batch 3100/8989 - Loss: 0.1864\n",
            "  Batch 3200/8989 - Loss: 0.1866\n",
            "  Batch 3300/8989 - Loss: 0.1869\n",
            "  Batch 3400/8989 - Loss: 0.1872\n",
            "  Batch 3500/8989 - Loss: 0.1875\n",
            "  Batch 3600/8989 - Loss: 0.1877\n",
            "  Batch 3700/8989 - Loss: 0.1879\n",
            "  Batch 3800/8989 - Loss: 0.1881\n",
            "  Batch 3900/8989 - Loss: 0.1882\n",
            "  Batch 4000/8989 - Loss: 0.1884\n",
            "  Batch 4100/8989 - Loss: 0.1885\n",
            "  Batch 4200/8989 - Loss: 0.1888\n",
            "  Batch 4300/8989 - Loss: 0.1889\n",
            "  Batch 4400/8989 - Loss: 0.1890\n",
            "  Batch 4500/8989 - Loss: 0.1891\n",
            "  Batch 4600/8989 - Loss: 0.1892\n",
            "  Batch 4700/8989 - Loss: 0.1894\n",
            "  Batch 4800/8989 - Loss: 0.1895\n",
            "  Batch 4900/8989 - Loss: 0.1897\n",
            "  Batch 5000/8989 - Loss: 0.1898\n",
            "  Batch 5100/8989 - Loss: 0.1898\n",
            "  Batch 5200/8989 - Loss: 0.1900\n",
            "  Batch 5300/8989 - Loss: 0.1902\n",
            "  Batch 5400/8989 - Loss: 0.1904\n",
            "  Batch 5500/8989 - Loss: 0.1904\n",
            "  Batch 5600/8989 - Loss: 0.1906\n",
            "  Batch 5700/8989 - Loss: 0.1907\n",
            "  Batch 5800/8989 - Loss: 0.1907\n",
            "  Batch 5900/8989 - Loss: 0.1908\n",
            "  Batch 6000/8989 - Loss: 0.1909\n",
            "  Batch 6100/8989 - Loss: 0.1909\n",
            "  Batch 6200/8989 - Loss: 0.1911\n",
            "  Batch 6300/8989 - Loss: 0.1913\n",
            "  Batch 6400/8989 - Loss: 0.1914\n",
            "  Batch 6500/8989 - Loss: 0.1915\n",
            "  Batch 6600/8989 - Loss: 0.1916\n",
            "  Batch 6700/8989 - Loss: 0.1918\n",
            "  Batch 6800/8989 - Loss: 0.1919\n",
            "  Batch 6900/8989 - Loss: 0.1920\n",
            "  Batch 7000/8989 - Loss: 0.1921\n",
            "  Batch 7100/8989 - Loss: 0.1922\n",
            "  Batch 7200/8989 - Loss: 0.1923\n",
            "  Batch 7300/8989 - Loss: 0.1925\n",
            "  Batch 7400/8989 - Loss: 0.1926\n",
            "  Batch 7500/8989 - Loss: 0.1927\n",
            "  Batch 7600/8989 - Loss: 0.1927\n",
            "  Batch 7700/8989 - Loss: 0.1928\n",
            "  Batch 7800/8989 - Loss: 0.1929\n",
            "  Batch 7900/8989 - Loss: 0.1929\n",
            "  Batch 8000/8989 - Loss: 0.1931\n",
            "  Batch 8100/8989 - Loss: 0.1932\n",
            "  Batch 8200/8989 - Loss: 0.1933\n",
            "  Batch 8300/8989 - Loss: 0.1934\n",
            "  Batch 8400/8989 - Loss: 0.1935\n",
            "  Batch 8500/8989 - Loss: 0.1936\n",
            "  Batch 8600/8989 - Loss: 0.1938\n",
            "  Batch 8700/8989 - Loss: 0.1939\n",
            "  Batch 8800/8989 - Loss: 0.1940\n",
            "  Batch 8900/8989 - Loss: 0.1941\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1942\n",
            "  HR@10: 0.7254\n",
            "  NDCG@10: 0.4458\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 12/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1758\n",
            "  Batch 200/8989 - Loss: 0.1744\n",
            "  Batch 300/8989 - Loss: 0.1762\n",
            "  Batch 400/8989 - Loss: 0.1768\n",
            "  Batch 500/8989 - Loss: 0.1762\n",
            "  Batch 600/8989 - Loss: 0.1752\n",
            "  Batch 700/8989 - Loss: 0.1752\n",
            "  Batch 800/8989 - Loss: 0.1757\n",
            "  Batch 900/8989 - Loss: 0.1758\n",
            "  Batch 1000/8989 - Loss: 0.1761\n",
            "  Batch 1100/8989 - Loss: 0.1769\n",
            "  Batch 1200/8989 - Loss: 0.1769\n",
            "  Batch 1300/8989 - Loss: 0.1774\n",
            "  Batch 1400/8989 - Loss: 0.1776\n",
            "  Batch 1500/8989 - Loss: 0.1781\n",
            "  Batch 1600/8989 - Loss: 0.1783\n",
            "  Batch 1700/8989 - Loss: 0.1786\n",
            "  Batch 1800/8989 - Loss: 0.1786\n",
            "  Batch 1900/8989 - Loss: 0.1788\n",
            "  Batch 2000/8989 - Loss: 0.1791\n",
            "  Batch 2100/8989 - Loss: 0.1793\n",
            "  Batch 2200/8989 - Loss: 0.1799\n",
            "  Batch 2300/8989 - Loss: 0.1799\n",
            "  Batch 2400/8989 - Loss: 0.1801\n",
            "  Batch 2500/8989 - Loss: 0.1805\n",
            "  Batch 2600/8989 - Loss: 0.1804\n",
            "  Batch 2700/8989 - Loss: 0.1808\n",
            "  Batch 2800/8989 - Loss: 0.1810\n",
            "  Batch 2900/8989 - Loss: 0.1812\n",
            "  Batch 3000/8989 - Loss: 0.1813\n",
            "  Batch 3100/8989 - Loss: 0.1814\n",
            "  Batch 3200/8989 - Loss: 0.1815\n",
            "  Batch 3300/8989 - Loss: 0.1817\n",
            "  Batch 3400/8989 - Loss: 0.1818\n",
            "  Batch 3500/8989 - Loss: 0.1821\n",
            "  Batch 3600/8989 - Loss: 0.1823\n",
            "  Batch 3700/8989 - Loss: 0.1824\n",
            "  Batch 3800/8989 - Loss: 0.1824\n",
            "  Batch 3900/8989 - Loss: 0.1826\n",
            "  Batch 4000/8989 - Loss: 0.1828\n",
            "  Batch 4100/8989 - Loss: 0.1830\n",
            "  Batch 4200/8989 - Loss: 0.1831\n",
            "  Batch 4300/8989 - Loss: 0.1833\n",
            "  Batch 4400/8989 - Loss: 0.1835\n",
            "  Batch 4500/8989 - Loss: 0.1837\n",
            "  Batch 4600/8989 - Loss: 0.1838\n",
            "  Batch 4700/8989 - Loss: 0.1839\n",
            "  Batch 4800/8989 - Loss: 0.1841\n",
            "  Batch 4900/8989 - Loss: 0.1842\n",
            "  Batch 5000/8989 - Loss: 0.1843\n",
            "  Batch 5100/8989 - Loss: 0.1844\n",
            "  Batch 5200/8989 - Loss: 0.1847\n",
            "  Batch 5300/8989 - Loss: 0.1847\n",
            "  Batch 5400/8989 - Loss: 0.1849\n",
            "  Batch 5500/8989 - Loss: 0.1851\n",
            "  Batch 5600/8989 - Loss: 0.1852\n",
            "  Batch 5700/8989 - Loss: 0.1853\n",
            "  Batch 5800/8989 - Loss: 0.1855\n",
            "  Batch 5900/8989 - Loss: 0.1856\n",
            "  Batch 6000/8989 - Loss: 0.1857\n",
            "  Batch 6100/8989 - Loss: 0.1859\n",
            "  Batch 6200/8989 - Loss: 0.1860\n",
            "  Batch 6300/8989 - Loss: 0.1861\n",
            "  Batch 6400/8989 - Loss: 0.1862\n",
            "  Batch 6500/8989 - Loss: 0.1864\n",
            "  Batch 6600/8989 - Loss: 0.1866\n",
            "  Batch 6700/8989 - Loss: 0.1867\n",
            "  Batch 6800/8989 - Loss: 0.1868\n",
            "  Batch 6900/8989 - Loss: 0.1868\n",
            "  Batch 7000/8989 - Loss: 0.1870\n",
            "  Batch 7100/8989 - Loss: 0.1871\n",
            "  Batch 7200/8989 - Loss: 0.1873\n",
            "  Batch 7300/8989 - Loss: 0.1875\n",
            "  Batch 7400/8989 - Loss: 0.1875\n",
            "  Batch 7500/8989 - Loss: 0.1878\n",
            "  Batch 7600/8989 - Loss: 0.1879\n",
            "  Batch 7700/8989 - Loss: 0.1881\n",
            "  Batch 7800/8989 - Loss: 0.1882\n",
            "  Batch 7900/8989 - Loss: 0.1883\n",
            "  Batch 8000/8989 - Loss: 0.1884\n",
            "  Batch 8100/8989 - Loss: 0.1886\n",
            "  Batch 8200/8989 - Loss: 0.1886\n",
            "  Batch 8300/8989 - Loss: 0.1887\n",
            "  Batch 8400/8989 - Loss: 0.1888\n",
            "  Batch 8500/8989 - Loss: 0.1890\n",
            "  Batch 8600/8989 - Loss: 0.1890\n",
            "  Batch 8700/8989 - Loss: 0.1891\n",
            "  Batch 8800/8989 - Loss: 0.1893\n",
            "  Batch 8900/8989 - Loss: 0.1894\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1895\n",
            "  HR@10: 0.7224\n",
            "  NDCG@10: 0.4440\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 13/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1736\n",
            "  Batch 200/8989 - Loss: 0.1743\n",
            "  Batch 300/8989 - Loss: 0.1737\n",
            "  Batch 400/8989 - Loss: 0.1739\n",
            "  Batch 500/8989 - Loss: 0.1738\n",
            "  Batch 600/8989 - Loss: 0.1740\n",
            "  Batch 700/8989 - Loss: 0.1742\n",
            "  Batch 800/8989 - Loss: 0.1745\n",
            "  Batch 900/8989 - Loss: 0.1744\n",
            "  Batch 1000/8989 - Loss: 0.1742\n",
            "  Batch 1100/8989 - Loss: 0.1741\n",
            "  Batch 1200/8989 - Loss: 0.1744\n",
            "  Batch 1300/8989 - Loss: 0.1746\n",
            "  Batch 1400/8989 - Loss: 0.1748\n",
            "  Batch 1500/8989 - Loss: 0.1746\n",
            "  Batch 1600/8989 - Loss: 0.1749\n",
            "  Batch 1700/8989 - Loss: 0.1753\n",
            "  Batch 1800/8989 - Loss: 0.1754\n",
            "  Batch 1900/8989 - Loss: 0.1755\n",
            "  Batch 2000/8989 - Loss: 0.1756\n",
            "  Batch 2100/8989 - Loss: 0.1759\n",
            "  Batch 2200/8989 - Loss: 0.1761\n",
            "  Batch 2300/8989 - Loss: 0.1763\n",
            "  Batch 2400/8989 - Loss: 0.1765\n",
            "  Batch 2500/8989 - Loss: 0.1767\n",
            "  Batch 2600/8989 - Loss: 0.1766\n",
            "  Batch 2700/8989 - Loss: 0.1768\n",
            "  Batch 2800/8989 - Loss: 0.1771\n",
            "  Batch 2900/8989 - Loss: 0.1772\n",
            "  Batch 3000/8989 - Loss: 0.1773\n",
            "  Batch 3100/8989 - Loss: 0.1776\n",
            "  Batch 3200/8989 - Loss: 0.1777\n",
            "  Batch 3300/8989 - Loss: 0.1780\n",
            "  Batch 3400/8989 - Loss: 0.1780\n",
            "  Batch 3500/8989 - Loss: 0.1782\n",
            "  Batch 3600/8989 - Loss: 0.1783\n",
            "  Batch 3700/8989 - Loss: 0.1786\n",
            "  Batch 3800/8989 - Loss: 0.1788\n",
            "  Batch 3900/8989 - Loss: 0.1789\n",
            "  Batch 4000/8989 - Loss: 0.1790\n",
            "  Batch 4100/8989 - Loss: 0.1792\n",
            "  Batch 4200/8989 - Loss: 0.1793\n",
            "  Batch 4300/8989 - Loss: 0.1793\n",
            "  Batch 4400/8989 - Loss: 0.1795\n",
            "  Batch 4500/8989 - Loss: 0.1796\n",
            "  Batch 4600/8989 - Loss: 0.1798\n",
            "  Batch 4700/8989 - Loss: 0.1800\n",
            "  Batch 4800/8989 - Loss: 0.1801\n",
            "  Batch 4900/8989 - Loss: 0.1803\n",
            "  Batch 5000/8989 - Loss: 0.1804\n",
            "  Batch 5100/8989 - Loss: 0.1806\n",
            "  Batch 5200/8989 - Loss: 0.1807\n",
            "  Batch 5300/8989 - Loss: 0.1808\n",
            "  Batch 5400/8989 - Loss: 0.1810\n",
            "  Batch 5500/8989 - Loss: 0.1811\n",
            "  Batch 5600/8989 - Loss: 0.1812\n",
            "  Batch 5700/8989 - Loss: 0.1814\n",
            "  Batch 5800/8989 - Loss: 0.1816\n",
            "  Batch 5900/8989 - Loss: 0.1818\n",
            "  Batch 6000/8989 - Loss: 0.1819\n",
            "  Batch 6100/8989 - Loss: 0.1820\n",
            "  Batch 6200/8989 - Loss: 0.1822\n",
            "  Batch 6300/8989 - Loss: 0.1823\n",
            "  Batch 6400/8989 - Loss: 0.1824\n",
            "  Batch 6500/8989 - Loss: 0.1825\n",
            "  Batch 6600/8989 - Loss: 0.1826\n",
            "  Batch 6700/8989 - Loss: 0.1828\n",
            "  Batch 6800/8989 - Loss: 0.1829\n",
            "  Batch 6900/8989 - Loss: 0.1830\n",
            "  Batch 7000/8989 - Loss: 0.1831\n",
            "  Batch 7100/8989 - Loss: 0.1832\n",
            "  Batch 7200/8989 - Loss: 0.1833\n",
            "  Batch 7300/8989 - Loss: 0.1834\n",
            "  Batch 7400/8989 - Loss: 0.1836\n",
            "  Batch 7500/8989 - Loss: 0.1838\n",
            "  Batch 7600/8989 - Loss: 0.1839\n",
            "  Batch 7700/8989 - Loss: 0.1840\n",
            "  Batch 7800/8989 - Loss: 0.1840\n",
            "  Batch 7900/8989 - Loss: 0.1842\n",
            "  Batch 8000/8989 - Loss: 0.1844\n",
            "  Batch 8100/8989 - Loss: 0.1845\n",
            "  Batch 8200/8989 - Loss: 0.1846\n",
            "  Batch 8300/8989 - Loss: 0.1846\n",
            "  Batch 8400/8989 - Loss: 0.1848\n",
            "  Batch 8500/8989 - Loss: 0.1849\n",
            "  Batch 8600/8989 - Loss: 0.1850\n",
            "  Batch 8700/8989 - Loss: 0.1851\n",
            "  Batch 8800/8989 - Loss: 0.1852\n",
            "  Batch 8900/8989 - Loss: 0.1853\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1854\n",
            "  HR@10: 0.7210\n",
            "  NDCG@10: 0.4408\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 14/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1635\n",
            "  Batch 200/8989 - Loss: 0.1666\n",
            "  Batch 300/8989 - Loss: 0.1675\n",
            "  Batch 400/8989 - Loss: 0.1681\n",
            "  Batch 500/8989 - Loss: 0.1686\n",
            "  Batch 600/8989 - Loss: 0.1683\n",
            "  Batch 700/8989 - Loss: 0.1687\n",
            "  Batch 800/8989 - Loss: 0.1689\n",
            "  Batch 900/8989 - Loss: 0.1686\n",
            "  Batch 1000/8989 - Loss: 0.1689\n",
            "  Batch 1100/8989 - Loss: 0.1693\n",
            "  Batch 1200/8989 - Loss: 0.1696\n",
            "  Batch 1300/8989 - Loss: 0.1701\n",
            "  Batch 1400/8989 - Loss: 0.1703\n",
            "  Batch 1500/8989 - Loss: 0.1705\n",
            "  Batch 1600/8989 - Loss: 0.1708\n",
            "  Batch 1700/8989 - Loss: 0.1708\n",
            "  Batch 1800/8989 - Loss: 0.1709\n",
            "  Batch 1900/8989 - Loss: 0.1711\n",
            "  Batch 2000/8989 - Loss: 0.1711\n",
            "  Batch 2100/8989 - Loss: 0.1714\n",
            "  Batch 2200/8989 - Loss: 0.1716\n",
            "  Batch 2300/8989 - Loss: 0.1720\n",
            "  Batch 2400/8989 - Loss: 0.1722\n",
            "  Batch 2500/8989 - Loss: 0.1723\n",
            "  Batch 2600/8989 - Loss: 0.1726\n",
            "  Batch 2700/8989 - Loss: 0.1728\n",
            "  Batch 2800/8989 - Loss: 0.1730\n",
            "  Batch 2900/8989 - Loss: 0.1732\n",
            "  Batch 3000/8989 - Loss: 0.1735\n",
            "  Batch 3100/8989 - Loss: 0.1737\n",
            "  Batch 3200/8989 - Loss: 0.1738\n",
            "  Batch 3300/8989 - Loss: 0.1739\n",
            "  Batch 3400/8989 - Loss: 0.1742\n",
            "  Batch 3500/8989 - Loss: 0.1742\n",
            "  Batch 3600/8989 - Loss: 0.1744\n",
            "  Batch 3700/8989 - Loss: 0.1745\n",
            "  Batch 3800/8989 - Loss: 0.1746\n",
            "  Batch 3900/8989 - Loss: 0.1749\n",
            "  Batch 4000/8989 - Loss: 0.1749\n",
            "  Batch 4100/8989 - Loss: 0.1750\n",
            "  Batch 4200/8989 - Loss: 0.1753\n",
            "  Batch 4300/8989 - Loss: 0.1755\n",
            "  Batch 4400/8989 - Loss: 0.1756\n",
            "  Batch 4500/8989 - Loss: 0.1757\n",
            "  Batch 4600/8989 - Loss: 0.1759\n",
            "  Batch 4700/8989 - Loss: 0.1761\n",
            "  Batch 4800/8989 - Loss: 0.1762\n",
            "  Batch 4900/8989 - Loss: 0.1763\n",
            "  Batch 5000/8989 - Loss: 0.1764\n",
            "  Batch 5100/8989 - Loss: 0.1765\n",
            "  Batch 5200/8989 - Loss: 0.1767\n",
            "  Batch 5300/8989 - Loss: 0.1768\n",
            "  Batch 5400/8989 - Loss: 0.1770\n",
            "  Batch 5500/8989 - Loss: 0.1771\n",
            "  Batch 5600/8989 - Loss: 0.1773\n",
            "  Batch 5700/8989 - Loss: 0.1774\n",
            "  Batch 5800/8989 - Loss: 0.1775\n",
            "  Batch 5900/8989 - Loss: 0.1776\n",
            "  Batch 6000/8989 - Loss: 0.1777\n",
            "  Batch 6100/8989 - Loss: 0.1778\n",
            "  Batch 6200/8989 - Loss: 0.1779\n",
            "  Batch 6300/8989 - Loss: 0.1780\n",
            "  Batch 6400/8989 - Loss: 0.1782\n",
            "  Batch 6500/8989 - Loss: 0.1784\n",
            "  Batch 6600/8989 - Loss: 0.1785\n",
            "  Batch 6700/8989 - Loss: 0.1787\n",
            "  Batch 6800/8989 - Loss: 0.1788\n",
            "  Batch 6900/8989 - Loss: 0.1789\n",
            "  Batch 7000/8989 - Loss: 0.1790\n",
            "  Batch 7100/8989 - Loss: 0.1790\n",
            "  Batch 7200/8989 - Loss: 0.1792\n",
            "  Batch 7300/8989 - Loss: 0.1794\n",
            "  Batch 7400/8989 - Loss: 0.1796\n",
            "  Batch 7500/8989 - Loss: 0.1797\n",
            "  Batch 7600/8989 - Loss: 0.1797\n",
            "  Batch 7700/8989 - Loss: 0.1799\n",
            "  Batch 7800/8989 - Loss: 0.1800\n",
            "  Batch 7900/8989 - Loss: 0.1801\n",
            "  Batch 8000/8989 - Loss: 0.1802\n",
            "  Batch 8100/8989 - Loss: 0.1803\n",
            "  Batch 8200/8989 - Loss: 0.1804\n",
            "  Batch 8300/8989 - Loss: 0.1805\n",
            "  Batch 8400/8989 - Loss: 0.1807\n",
            "  Batch 8500/8989 - Loss: 0.1808\n",
            "  Batch 8600/8989 - Loss: 0.1809\n",
            "  Batch 8700/8989 - Loss: 0.1810\n",
            "  Batch 8800/8989 - Loss: 0.1811\n",
            "  Batch 8900/8989 - Loss: 0.1812\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1813\n",
            "  HR@10: 0.7209\n",
            "  NDCG@10: 0.4430\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 15/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1683\n",
            "  Batch 200/8989 - Loss: 0.1630\n",
            "  Batch 300/8989 - Loss: 0.1641\n",
            "  Batch 400/8989 - Loss: 0.1638\n",
            "  Batch 500/8989 - Loss: 0.1644\n",
            "  Batch 600/8989 - Loss: 0.1649\n",
            "  Batch 700/8989 - Loss: 0.1657\n",
            "  Batch 800/8989 - Loss: 0.1659\n",
            "  Batch 900/8989 - Loss: 0.1658\n",
            "  Batch 1000/8989 - Loss: 0.1662\n",
            "  Batch 1100/8989 - Loss: 0.1663\n",
            "  Batch 1200/8989 - Loss: 0.1665\n",
            "  Batch 1300/8989 - Loss: 0.1668\n",
            "  Batch 1400/8989 - Loss: 0.1671\n",
            "  Batch 1500/8989 - Loss: 0.1671\n",
            "  Batch 1600/8989 - Loss: 0.1673\n",
            "  Batch 1700/8989 - Loss: 0.1674\n",
            "  Batch 1800/8989 - Loss: 0.1675\n",
            "  Batch 1900/8989 - Loss: 0.1677\n",
            "  Batch 2000/8989 - Loss: 0.1677\n",
            "  Batch 2100/8989 - Loss: 0.1679\n",
            "  Batch 2200/8989 - Loss: 0.1680\n",
            "  Batch 2300/8989 - Loss: 0.1682\n",
            "  Batch 2400/8989 - Loss: 0.1683\n",
            "  Batch 2500/8989 - Loss: 0.1686\n",
            "  Batch 2600/8989 - Loss: 0.1689\n",
            "  Batch 2700/8989 - Loss: 0.1691\n",
            "  Batch 2800/8989 - Loss: 0.1692\n",
            "  Batch 2900/8989 - Loss: 0.1693\n",
            "  Batch 3000/8989 - Loss: 0.1694\n",
            "  Batch 3100/8989 - Loss: 0.1696\n",
            "  Batch 3200/8989 - Loss: 0.1697\n",
            "  Batch 3300/8989 - Loss: 0.1698\n",
            "  Batch 3400/8989 - Loss: 0.1701\n",
            "  Batch 3500/8989 - Loss: 0.1703\n",
            "  Batch 3600/8989 - Loss: 0.1704\n",
            "  Batch 3700/8989 - Loss: 0.1705\n",
            "  Batch 3800/8989 - Loss: 0.1707\n",
            "  Batch 3900/8989 - Loss: 0.1708\n",
            "  Batch 4000/8989 - Loss: 0.1708\n",
            "  Batch 4100/8989 - Loss: 0.1710\n",
            "  Batch 4200/8989 - Loss: 0.1711\n",
            "  Batch 4300/8989 - Loss: 0.1713\n",
            "  Batch 4400/8989 - Loss: 0.1716\n",
            "  Batch 4500/8989 - Loss: 0.1717\n",
            "  Batch 4600/8989 - Loss: 0.1719\n",
            "  Batch 4700/8989 - Loss: 0.1720\n",
            "  Batch 4800/8989 - Loss: 0.1723\n",
            "  Batch 4900/8989 - Loss: 0.1725\n",
            "  Batch 5000/8989 - Loss: 0.1727\n",
            "  Batch 5100/8989 - Loss: 0.1728\n",
            "  Batch 5200/8989 - Loss: 0.1729\n",
            "  Batch 5300/8989 - Loss: 0.1730\n",
            "  Batch 5400/8989 - Loss: 0.1732\n",
            "  Batch 5500/8989 - Loss: 0.1734\n",
            "  Batch 5600/8989 - Loss: 0.1735\n",
            "  Batch 5700/8989 - Loss: 0.1736\n",
            "  Batch 5800/8989 - Loss: 0.1737\n",
            "  Batch 5900/8989 - Loss: 0.1738\n",
            "  Batch 6000/8989 - Loss: 0.1740\n",
            "  Batch 6100/8989 - Loss: 0.1740\n",
            "  Batch 6200/8989 - Loss: 0.1742\n",
            "  Batch 6300/8989 - Loss: 0.1743\n",
            "  Batch 6400/8989 - Loss: 0.1744\n",
            "  Batch 6500/8989 - Loss: 0.1746\n",
            "  Batch 6600/8989 - Loss: 0.1747\n",
            "  Batch 6700/8989 - Loss: 0.1748\n",
            "  Batch 6800/8989 - Loss: 0.1749\n",
            "  Batch 6900/8989 - Loss: 0.1751\n",
            "  Batch 7000/8989 - Loss: 0.1752\n",
            "  Batch 7100/8989 - Loss: 0.1753\n",
            "  Batch 7200/8989 - Loss: 0.1754\n",
            "  Batch 7300/8989 - Loss: 0.1756\n",
            "  Batch 7400/8989 - Loss: 0.1757\n",
            "  Batch 7500/8989 - Loss: 0.1758\n",
            "  Batch 7600/8989 - Loss: 0.1759\n",
            "  Batch 7700/8989 - Loss: 0.1761\n",
            "  Batch 7800/8989 - Loss: 0.1763\n",
            "  Batch 7900/8989 - Loss: 0.1764\n",
            "  Batch 8000/8989 - Loss: 0.1765\n",
            "  Batch 8100/8989 - Loss: 0.1765\n",
            "  Batch 8200/8989 - Loss: 0.1766\n",
            "  Batch 8300/8989 - Loss: 0.1767\n",
            "  Batch 8400/8989 - Loss: 0.1768\n",
            "  Batch 8500/8989 - Loss: 0.1769\n",
            "  Batch 8600/8989 - Loss: 0.1770\n",
            "  Batch 8700/8989 - Loss: 0.1771\n",
            "  Batch 8800/8989 - Loss: 0.1772\n",
            "  Batch 8900/8989 - Loss: 0.1773\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1774\n",
            "  HR@10: 0.7180\n",
            "  NDCG@10: 0.4410\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 16/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1595\n",
            "  Batch 200/8989 - Loss: 0.1609\n",
            "  Batch 300/8989 - Loss: 0.1618\n",
            "  Batch 400/8989 - Loss: 0.1625\n",
            "  Batch 500/8989 - Loss: 0.1623\n",
            "  Batch 600/8989 - Loss: 0.1618\n",
            "  Batch 700/8989 - Loss: 0.1615\n",
            "  Batch 800/8989 - Loss: 0.1618\n",
            "  Batch 900/8989 - Loss: 0.1615\n",
            "  Batch 1000/8989 - Loss: 0.1619\n",
            "  Batch 1100/8989 - Loss: 0.1621\n",
            "  Batch 1200/8989 - Loss: 0.1622\n",
            "  Batch 1300/8989 - Loss: 0.1626\n",
            "  Batch 1400/8989 - Loss: 0.1629\n",
            "  Batch 1500/8989 - Loss: 0.1632\n",
            "  Batch 1600/8989 - Loss: 0.1632\n",
            "  Batch 1700/8989 - Loss: 0.1634\n",
            "  Batch 1800/8989 - Loss: 0.1639\n",
            "  Batch 1900/8989 - Loss: 0.1642\n",
            "  Batch 2000/8989 - Loss: 0.1641\n",
            "  Batch 2100/8989 - Loss: 0.1642\n",
            "  Batch 2200/8989 - Loss: 0.1642\n",
            "  Batch 2300/8989 - Loss: 0.1643\n",
            "  Batch 2400/8989 - Loss: 0.1642\n",
            "  Batch 2500/8989 - Loss: 0.1644\n",
            "  Batch 2600/8989 - Loss: 0.1647\n",
            "  Batch 2700/8989 - Loss: 0.1650\n",
            "  Batch 2800/8989 - Loss: 0.1652\n",
            "  Batch 2900/8989 - Loss: 0.1654\n",
            "  Batch 3000/8989 - Loss: 0.1655\n",
            "  Batch 3100/8989 - Loss: 0.1655\n",
            "  Batch 3200/8989 - Loss: 0.1657\n",
            "  Batch 3300/8989 - Loss: 0.1660\n",
            "  Batch 3400/8989 - Loss: 0.1660\n",
            "  Batch 3500/8989 - Loss: 0.1663\n",
            "  Batch 3600/8989 - Loss: 0.1665\n",
            "  Batch 3700/8989 - Loss: 0.1665\n",
            "  Batch 3800/8989 - Loss: 0.1667\n",
            "  Batch 3900/8989 - Loss: 0.1667\n",
            "  Batch 4000/8989 - Loss: 0.1668\n",
            "  Batch 4100/8989 - Loss: 0.1671\n",
            "  Batch 4200/8989 - Loss: 0.1672\n",
            "  Batch 4300/8989 - Loss: 0.1674\n",
            "  Batch 4400/8989 - Loss: 0.1675\n",
            "  Batch 4500/8989 - Loss: 0.1677\n",
            "  Batch 4600/8989 - Loss: 0.1679\n",
            "  Batch 4700/8989 - Loss: 0.1680\n",
            "  Batch 4800/8989 - Loss: 0.1682\n",
            "  Batch 4900/8989 - Loss: 0.1684\n",
            "  Batch 5000/8989 - Loss: 0.1686\n",
            "  Batch 5100/8989 - Loss: 0.1687\n",
            "  Batch 5200/8989 - Loss: 0.1687\n",
            "  Batch 5300/8989 - Loss: 0.1689\n",
            "  Batch 5400/8989 - Loss: 0.1690\n",
            "  Batch 5500/8989 - Loss: 0.1691\n",
            "  Batch 5600/8989 - Loss: 0.1692\n",
            "  Batch 5700/8989 - Loss: 0.1692\n",
            "  Batch 5800/8989 - Loss: 0.1693\n",
            "  Batch 5900/8989 - Loss: 0.1695\n",
            "  Batch 6000/8989 - Loss: 0.1696\n",
            "  Batch 6100/8989 - Loss: 0.1696\n",
            "  Batch 6200/8989 - Loss: 0.1698\n",
            "  Batch 6300/8989 - Loss: 0.1699\n",
            "  Batch 6400/8989 - Loss: 0.1700\n",
            "  Batch 6500/8989 - Loss: 0.1702\n",
            "  Batch 6600/8989 - Loss: 0.1703\n",
            "  Batch 6700/8989 - Loss: 0.1705\n",
            "  Batch 6800/8989 - Loss: 0.1706\n",
            "  Batch 6900/8989 - Loss: 0.1707\n",
            "  Batch 7000/8989 - Loss: 0.1708\n",
            "  Batch 7100/8989 - Loss: 0.1710\n",
            "  Batch 7200/8989 - Loss: 0.1711\n",
            "  Batch 7300/8989 - Loss: 0.1712\n",
            "  Batch 7400/8989 - Loss: 0.1713\n",
            "  Batch 7500/8989 - Loss: 0.1715\n",
            "  Batch 7600/8989 - Loss: 0.1716\n",
            "  Batch 7700/8989 - Loss: 0.1717\n",
            "  Batch 7800/8989 - Loss: 0.1718\n",
            "  Batch 7900/8989 - Loss: 0.1719\n",
            "  Batch 8000/8989 - Loss: 0.1721\n",
            "  Batch 8100/8989 - Loss: 0.1722\n",
            "  Batch 8200/8989 - Loss: 0.1724\n",
            "  Batch 8300/8989 - Loss: 0.1725\n",
            "  Batch 8400/8989 - Loss: 0.1725\n",
            "  Batch 8500/8989 - Loss: 0.1727\n",
            "  Batch 8600/8989 - Loss: 0.1728\n",
            "  Batch 8700/8989 - Loss: 0.1729\n",
            "  Batch 8800/8989 - Loss: 0.1731\n",
            "  Batch 8900/8989 - Loss: 0.1731\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1732\n",
            "  HR@10: 0.7159\n",
            "  NDCG@10: 0.4370\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 17/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1596\n",
            "  Batch 200/8989 - Loss: 0.1589\n",
            "  Batch 300/8989 - Loss: 0.1581\n",
            "  Batch 400/8989 - Loss: 0.1579\n",
            "  Batch 500/8989 - Loss: 0.1577\n",
            "  Batch 600/8989 - Loss: 0.1564\n",
            "  Batch 700/8989 - Loss: 0.1568\n",
            "  Batch 800/8989 - Loss: 0.1573\n",
            "  Batch 900/8989 - Loss: 0.1578\n",
            "  Batch 1000/8989 - Loss: 0.1581\n",
            "  Batch 1100/8989 - Loss: 0.1582\n",
            "  Batch 1200/8989 - Loss: 0.1583\n",
            "  Batch 1300/8989 - Loss: 0.1586\n",
            "  Batch 1400/8989 - Loss: 0.1589\n",
            "  Batch 1500/8989 - Loss: 0.1591\n",
            "  Batch 1600/8989 - Loss: 0.1591\n",
            "  Batch 1700/8989 - Loss: 0.1590\n",
            "  Batch 1800/8989 - Loss: 0.1593\n",
            "  Batch 1900/8989 - Loss: 0.1593\n",
            "  Batch 2000/8989 - Loss: 0.1594\n",
            "  Batch 2100/8989 - Loss: 0.1596\n",
            "  Batch 2200/8989 - Loss: 0.1597\n",
            "  Batch 2300/8989 - Loss: 0.1598\n",
            "  Batch 2400/8989 - Loss: 0.1600\n",
            "  Batch 2500/8989 - Loss: 0.1602\n",
            "  Batch 2600/8989 - Loss: 0.1605\n",
            "  Batch 2700/8989 - Loss: 0.1607\n",
            "  Batch 2800/8989 - Loss: 0.1610\n",
            "  Batch 2900/8989 - Loss: 0.1613\n",
            "  Batch 3000/8989 - Loss: 0.1616\n",
            "  Batch 3100/8989 - Loss: 0.1618\n",
            "  Batch 3200/8989 - Loss: 0.1619\n",
            "  Batch 3300/8989 - Loss: 0.1621\n",
            "  Batch 3400/8989 - Loss: 0.1622\n",
            "  Batch 3500/8989 - Loss: 0.1626\n",
            "  Batch 3600/8989 - Loss: 0.1627\n",
            "  Batch 3700/8989 - Loss: 0.1629\n",
            "  Batch 3800/8989 - Loss: 0.1629\n",
            "  Batch 3900/8989 - Loss: 0.1630\n",
            "  Batch 4000/8989 - Loss: 0.1631\n",
            "  Batch 4100/8989 - Loss: 0.1633\n",
            "  Batch 4200/8989 - Loss: 0.1634\n",
            "  Batch 4300/8989 - Loss: 0.1636\n",
            "  Batch 4400/8989 - Loss: 0.1638\n",
            "  Batch 4500/8989 - Loss: 0.1640\n",
            "  Batch 4600/8989 - Loss: 0.1643\n",
            "  Batch 4700/8989 - Loss: 0.1644\n",
            "  Batch 4800/8989 - Loss: 0.1645\n",
            "  Batch 4900/8989 - Loss: 0.1647\n",
            "  Batch 5000/8989 - Loss: 0.1650\n",
            "  Batch 5100/8989 - Loss: 0.1653\n",
            "  Batch 5200/8989 - Loss: 0.1654\n",
            "  Batch 5300/8989 - Loss: 0.1654\n",
            "  Batch 5400/8989 - Loss: 0.1655\n",
            "  Batch 5500/8989 - Loss: 0.1655\n",
            "  Batch 5600/8989 - Loss: 0.1657\n",
            "  Batch 5700/8989 - Loss: 0.1659\n",
            "  Batch 5800/8989 - Loss: 0.1660\n",
            "  Batch 5900/8989 - Loss: 0.1661\n",
            "  Batch 6000/8989 - Loss: 0.1662\n",
            "  Batch 6100/8989 - Loss: 0.1664\n",
            "  Batch 6200/8989 - Loss: 0.1664\n",
            "  Batch 6300/8989 - Loss: 0.1666\n",
            "  Batch 6400/8989 - Loss: 0.1667\n",
            "  Batch 6500/8989 - Loss: 0.1669\n",
            "  Batch 6600/8989 - Loss: 0.1670\n",
            "  Batch 6700/8989 - Loss: 0.1672\n",
            "  Batch 6800/8989 - Loss: 0.1673\n",
            "  Batch 6900/8989 - Loss: 0.1675\n",
            "  Batch 7000/8989 - Loss: 0.1676\n",
            "  Batch 7100/8989 - Loss: 0.1677\n",
            "  Batch 7200/8989 - Loss: 0.1678\n",
            "  Batch 7300/8989 - Loss: 0.1680\n",
            "  Batch 7400/8989 - Loss: 0.1681\n",
            "  Batch 7500/8989 - Loss: 0.1682\n",
            "  Batch 7600/8989 - Loss: 0.1683\n",
            "  Batch 7700/8989 - Loss: 0.1684\n",
            "  Batch 7800/8989 - Loss: 0.1685\n",
            "  Batch 7900/8989 - Loss: 0.1686\n",
            "  Batch 8000/8989 - Loss: 0.1687\n",
            "  Batch 8100/8989 - Loss: 0.1688\n",
            "  Batch 8200/8989 - Loss: 0.1689\n",
            "  Batch 8300/8989 - Loss: 0.1690\n",
            "  Batch 8400/8989 - Loss: 0.1692\n",
            "  Batch 8500/8989 - Loss: 0.1692\n",
            "  Batch 8600/8989 - Loss: 0.1693\n",
            "  Batch 8700/8989 - Loss: 0.1695\n",
            "  Batch 8800/8989 - Loss: 0.1696\n",
            "  Batch 8900/8989 - Loss: 0.1697\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1698\n",
            "  HR@10: 0.7137\n",
            "  NDCG@10: 0.4348\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 18/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1532\n",
            "  Batch 200/8989 - Loss: 0.1553\n",
            "  Batch 300/8989 - Loss: 0.1542\n",
            "  Batch 400/8989 - Loss: 0.1542\n",
            "  Batch 500/8989 - Loss: 0.1543\n",
            "  Batch 600/8989 - Loss: 0.1534\n",
            "  Batch 700/8989 - Loss: 0.1539\n",
            "  Batch 800/8989 - Loss: 0.1539\n",
            "  Batch 900/8989 - Loss: 0.1542\n",
            "  Batch 1000/8989 - Loss: 0.1549\n",
            "  Batch 1100/8989 - Loss: 0.1549\n",
            "  Batch 1200/8989 - Loss: 0.1552\n",
            "  Batch 1300/8989 - Loss: 0.1557\n",
            "  Batch 1400/8989 - Loss: 0.1558\n",
            "  Batch 1500/8989 - Loss: 0.1562\n",
            "  Batch 1600/8989 - Loss: 0.1565\n",
            "  Batch 1700/8989 - Loss: 0.1569\n",
            "  Batch 1800/8989 - Loss: 0.1572\n",
            "  Batch 1900/8989 - Loss: 0.1574\n",
            "  Batch 2000/8989 - Loss: 0.1576\n",
            "  Batch 2100/8989 - Loss: 0.1577\n",
            "  Batch 2200/8989 - Loss: 0.1580\n",
            "  Batch 2300/8989 - Loss: 0.1580\n",
            "  Batch 2400/8989 - Loss: 0.1581\n",
            "  Batch 2500/8989 - Loss: 0.1581\n",
            "  Batch 2600/8989 - Loss: 0.1582\n",
            "  Batch 2700/8989 - Loss: 0.1583\n",
            "  Batch 2800/8989 - Loss: 0.1583\n",
            "  Batch 2900/8989 - Loss: 0.1585\n",
            "  Batch 3000/8989 - Loss: 0.1588\n",
            "  Batch 3100/8989 - Loss: 0.1591\n",
            "  Batch 3200/8989 - Loss: 0.1593\n",
            "  Batch 3300/8989 - Loss: 0.1595\n",
            "  Batch 3400/8989 - Loss: 0.1596\n",
            "  Batch 3500/8989 - Loss: 0.1597\n",
            "  Batch 3600/8989 - Loss: 0.1598\n",
            "  Batch 3700/8989 - Loss: 0.1600\n",
            "  Batch 3800/8989 - Loss: 0.1601\n",
            "  Batch 3900/8989 - Loss: 0.1603\n",
            "  Batch 4000/8989 - Loss: 0.1603\n",
            "  Batch 4100/8989 - Loss: 0.1603\n",
            "  Batch 4200/8989 - Loss: 0.1603\n",
            "  Batch 4300/8989 - Loss: 0.1605\n",
            "  Batch 4400/8989 - Loss: 0.1607\n",
            "  Batch 4500/8989 - Loss: 0.1608\n",
            "  Batch 4600/8989 - Loss: 0.1609\n",
            "  Batch 4700/8989 - Loss: 0.1612\n",
            "  Batch 4800/8989 - Loss: 0.1613\n",
            "  Batch 4900/8989 - Loss: 0.1615\n",
            "  Batch 5000/8989 - Loss: 0.1617\n",
            "  Batch 5100/8989 - Loss: 0.1617\n",
            "  Batch 5200/8989 - Loss: 0.1619\n",
            "  Batch 5300/8989 - Loss: 0.1620\n",
            "  Batch 5400/8989 - Loss: 0.1621\n",
            "  Batch 5500/8989 - Loss: 0.1622\n",
            "  Batch 5600/8989 - Loss: 0.1624\n",
            "  Batch 5700/8989 - Loss: 0.1625\n",
            "  Batch 5800/8989 - Loss: 0.1627\n",
            "  Batch 5900/8989 - Loss: 0.1629\n",
            "  Batch 6000/8989 - Loss: 0.1629\n",
            "  Batch 6100/8989 - Loss: 0.1631\n",
            "  Batch 6200/8989 - Loss: 0.1633\n",
            "  Batch 6300/8989 - Loss: 0.1634\n",
            "  Batch 6400/8989 - Loss: 0.1636\n",
            "  Batch 6500/8989 - Loss: 0.1636\n",
            "  Batch 6600/8989 - Loss: 0.1638\n",
            "  Batch 6700/8989 - Loss: 0.1638\n",
            "  Batch 6800/8989 - Loss: 0.1640\n",
            "  Batch 6900/8989 - Loss: 0.1641\n",
            "  Batch 7000/8989 - Loss: 0.1643\n",
            "  Batch 7100/8989 - Loss: 0.1644\n",
            "  Batch 7200/8989 - Loss: 0.1645\n",
            "  Batch 7300/8989 - Loss: 0.1646\n",
            "  Batch 7400/8989 - Loss: 0.1647\n",
            "  Batch 7500/8989 - Loss: 0.1648\n",
            "  Batch 7600/8989 - Loss: 0.1650\n",
            "  Batch 7700/8989 - Loss: 0.1651\n",
            "  Batch 7800/8989 - Loss: 0.1651\n",
            "  Batch 7900/8989 - Loss: 0.1651\n",
            "  Batch 8000/8989 - Loss: 0.1653\n",
            "  Batch 8100/8989 - Loss: 0.1654\n",
            "  Batch 8200/8989 - Loss: 0.1656\n",
            "  Batch 8300/8989 - Loss: 0.1657\n",
            "  Batch 8400/8989 - Loss: 0.1657\n",
            "  Batch 8500/8989 - Loss: 0.1658\n",
            "  Batch 8600/8989 - Loss: 0.1659\n",
            "  Batch 8700/8989 - Loss: 0.1661\n",
            "  Batch 8800/8989 - Loss: 0.1661\n",
            "  Batch 8900/8989 - Loss: 0.1662\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1663\n",
            "  HR@10: 0.7110\n",
            "  NDCG@10: 0.4321\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 19/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1514\n",
            "  Batch 200/8989 - Loss: 0.1498\n",
            "  Batch 300/8989 - Loss: 0.1516\n",
            "  Batch 400/8989 - Loss: 0.1520\n",
            "  Batch 500/8989 - Loss: 0.1513\n",
            "  Batch 600/8989 - Loss: 0.1513\n",
            "  Batch 700/8989 - Loss: 0.1514\n",
            "  Batch 800/8989 - Loss: 0.1518\n",
            "  Batch 900/8989 - Loss: 0.1522\n",
            "  Batch 1000/8989 - Loss: 0.1527\n",
            "  Batch 1100/8989 - Loss: 0.1522\n",
            "  Batch 1200/8989 - Loss: 0.1528\n",
            "  Batch 1300/8989 - Loss: 0.1528\n",
            "  Batch 1400/8989 - Loss: 0.1530\n",
            "  Batch 1500/8989 - Loss: 0.1534\n",
            "  Batch 1600/8989 - Loss: 0.1536\n",
            "  Batch 1700/8989 - Loss: 0.1538\n",
            "  Batch 1800/8989 - Loss: 0.1539\n",
            "  Batch 1900/8989 - Loss: 0.1540\n",
            "  Batch 2000/8989 - Loss: 0.1541\n",
            "  Batch 2100/8989 - Loss: 0.1541\n",
            "  Batch 2200/8989 - Loss: 0.1543\n",
            "  Batch 2300/8989 - Loss: 0.1545\n",
            "  Batch 2400/8989 - Loss: 0.1548\n",
            "  Batch 2500/8989 - Loss: 0.1550\n",
            "  Batch 2600/8989 - Loss: 0.1551\n",
            "  Batch 2700/8989 - Loss: 0.1554\n",
            "  Batch 2800/8989 - Loss: 0.1554\n",
            "  Batch 2900/8989 - Loss: 0.1557\n",
            "  Batch 3000/8989 - Loss: 0.1558\n",
            "  Batch 3100/8989 - Loss: 0.1559\n",
            "  Batch 3200/8989 - Loss: 0.1560\n",
            "  Batch 3300/8989 - Loss: 0.1562\n",
            "  Batch 3400/8989 - Loss: 0.1565\n",
            "  Batch 3500/8989 - Loss: 0.1567\n",
            "  Batch 3600/8989 - Loss: 0.1568\n",
            "  Batch 3700/8989 - Loss: 0.1569\n",
            "  Batch 3800/8989 - Loss: 0.1571\n",
            "  Batch 3900/8989 - Loss: 0.1573\n",
            "  Batch 4000/8989 - Loss: 0.1575\n",
            "  Batch 4100/8989 - Loss: 0.1577\n",
            "  Batch 4200/8989 - Loss: 0.1577\n",
            "  Batch 4300/8989 - Loss: 0.1578\n",
            "  Batch 4400/8989 - Loss: 0.1579\n",
            "  Batch 4500/8989 - Loss: 0.1580\n",
            "  Batch 4600/8989 - Loss: 0.1581\n",
            "  Batch 4700/8989 - Loss: 0.1582\n",
            "  Batch 4800/8989 - Loss: 0.1583\n",
            "  Batch 4900/8989 - Loss: 0.1584\n",
            "  Batch 5000/8989 - Loss: 0.1586\n",
            "  Batch 5100/8989 - Loss: 0.1585\n",
            "  Batch 5200/8989 - Loss: 0.1588\n",
            "  Batch 5300/8989 - Loss: 0.1589\n",
            "  Batch 5400/8989 - Loss: 0.1590\n",
            "  Batch 5500/8989 - Loss: 0.1591\n",
            "  Batch 5600/8989 - Loss: 0.1593\n",
            "  Batch 5700/8989 - Loss: 0.1594\n",
            "  Batch 5800/8989 - Loss: 0.1594\n",
            "  Batch 5900/8989 - Loss: 0.1595\n",
            "  Batch 6000/8989 - Loss: 0.1596\n",
            "  Batch 6100/8989 - Loss: 0.1597\n",
            "  Batch 6200/8989 - Loss: 0.1598\n",
            "  Batch 6300/8989 - Loss: 0.1600\n",
            "  Batch 6400/8989 - Loss: 0.1601\n",
            "  Batch 6500/8989 - Loss: 0.1603\n",
            "  Batch 6600/8989 - Loss: 0.1604\n",
            "  Batch 6700/8989 - Loss: 0.1605\n",
            "  Batch 6800/8989 - Loss: 0.1607\n",
            "  Batch 6900/8989 - Loss: 0.1608\n",
            "  Batch 7000/8989 - Loss: 0.1609\n",
            "  Batch 7100/8989 - Loss: 0.1609\n",
            "  Batch 7200/8989 - Loss: 0.1611\n",
            "  Batch 7300/8989 - Loss: 0.1612\n",
            "  Batch 7400/8989 - Loss: 0.1613\n",
            "  Batch 7500/8989 - Loss: 0.1614\n",
            "  Batch 7600/8989 - Loss: 0.1615\n",
            "  Batch 7700/8989 - Loss: 0.1616\n",
            "  Batch 7800/8989 - Loss: 0.1617\n",
            "  Batch 7900/8989 - Loss: 0.1617\n",
            "  Batch 8000/8989 - Loss: 0.1618\n",
            "  Batch 8100/8989 - Loss: 0.1619\n",
            "  Batch 8200/8989 - Loss: 0.1620\n",
            "  Batch 8300/8989 - Loss: 0.1622\n",
            "  Batch 8400/8989 - Loss: 0.1623\n",
            "  Batch 8500/8989 - Loss: 0.1624\n",
            "  Batch 8600/8989 - Loss: 0.1624\n",
            "  Batch 8700/8989 - Loss: 0.1625\n",
            "  Batch 8800/8989 - Loss: 0.1627\n",
            "  Batch 8900/8989 - Loss: 0.1628\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:40\n",
            "  Loss: 0.1629\n",
            "  HR@10: 0.7103\n",
            "  NDCG@10: 0.4338\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 20/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1475\n",
            "  Batch 200/8989 - Loss: 0.1472\n",
            "  Batch 300/8989 - Loss: 0.1472\n",
            "  Batch 400/8989 - Loss: 0.1479\n",
            "  Batch 500/8989 - Loss: 0.1482\n",
            "  Batch 600/8989 - Loss: 0.1489\n",
            "  Batch 700/8989 - Loss: 0.1490\n",
            "  Batch 800/8989 - Loss: 0.1494\n",
            "  Batch 900/8989 - Loss: 0.1496\n",
            "  Batch 1000/8989 - Loss: 0.1496\n",
            "  Batch 1100/8989 - Loss: 0.1499\n",
            "  Batch 1200/8989 - Loss: 0.1498\n",
            "  Batch 1300/8989 - Loss: 0.1501\n",
            "  Batch 1400/8989 - Loss: 0.1503\n",
            "  Batch 1500/8989 - Loss: 0.1504\n",
            "  Batch 1600/8989 - Loss: 0.1508\n",
            "  Batch 1700/8989 - Loss: 0.1508\n",
            "  Batch 1800/8989 - Loss: 0.1512\n",
            "  Batch 1900/8989 - Loss: 0.1511\n",
            "  Batch 2000/8989 - Loss: 0.1512\n",
            "  Batch 2100/8989 - Loss: 0.1512\n",
            "  Batch 2200/8989 - Loss: 0.1513\n",
            "  Batch 2300/8989 - Loss: 0.1516\n",
            "  Batch 2400/8989 - Loss: 0.1517\n",
            "  Batch 2500/8989 - Loss: 0.1518\n",
            "  Batch 2600/8989 - Loss: 0.1521\n",
            "  Batch 2700/8989 - Loss: 0.1522\n",
            "  Batch 2800/8989 - Loss: 0.1525\n",
            "  Batch 2900/8989 - Loss: 0.1524\n",
            "  Batch 3000/8989 - Loss: 0.1525\n",
            "  Batch 3100/8989 - Loss: 0.1526\n",
            "  Batch 3200/8989 - Loss: 0.1528\n",
            "  Batch 3300/8989 - Loss: 0.1530\n",
            "  Batch 3400/8989 - Loss: 0.1532\n",
            "  Batch 3500/8989 - Loss: 0.1535\n",
            "  Batch 3600/8989 - Loss: 0.1536\n",
            "  Batch 3700/8989 - Loss: 0.1536\n",
            "  Batch 3800/8989 - Loss: 0.1536\n",
            "  Batch 3900/8989 - Loss: 0.1538\n",
            "  Batch 4000/8989 - Loss: 0.1541\n",
            "  Batch 4100/8989 - Loss: 0.1542\n",
            "  Batch 4200/8989 - Loss: 0.1543\n",
            "  Batch 4300/8989 - Loss: 0.1544\n",
            "  Batch 4400/8989 - Loss: 0.1546\n",
            "  Batch 4500/8989 - Loss: 0.1547\n",
            "  Batch 4600/8989 - Loss: 0.1548\n",
            "  Batch 4700/8989 - Loss: 0.1550\n",
            "  Batch 4800/8989 - Loss: 0.1551\n",
            "  Batch 4900/8989 - Loss: 0.1553\n",
            "  Batch 5000/8989 - Loss: 0.1554\n",
            "  Batch 5100/8989 - Loss: 0.1556\n",
            "  Batch 5200/8989 - Loss: 0.1557\n",
            "  Batch 5300/8989 - Loss: 0.1558\n",
            "  Batch 5400/8989 - Loss: 0.1559\n",
            "  Batch 5500/8989 - Loss: 0.1560\n",
            "  Batch 5600/8989 - Loss: 0.1562\n",
            "  Batch 5700/8989 - Loss: 0.1564\n",
            "  Batch 5800/8989 - Loss: 0.1565\n",
            "  Batch 5900/8989 - Loss: 0.1566\n",
            "  Batch 6000/8989 - Loss: 0.1568\n",
            "  Batch 6100/8989 - Loss: 0.1570\n",
            "  Batch 6200/8989 - Loss: 0.1570\n",
            "  Batch 6300/8989 - Loss: 0.1572\n",
            "  Batch 6400/8989 - Loss: 0.1573\n",
            "  Batch 6500/8989 - Loss: 0.1573\n",
            "  Batch 6600/8989 - Loss: 0.1574\n",
            "  Batch 6700/8989 - Loss: 0.1576\n",
            "  Batch 6800/8989 - Loss: 0.1577\n",
            "  Batch 6900/8989 - Loss: 0.1578\n",
            "  Batch 7000/8989 - Loss: 0.1579\n",
            "  Batch 7100/8989 - Loss: 0.1581\n",
            "  Batch 7200/8989 - Loss: 0.1582\n",
            "  Batch 7300/8989 - Loss: 0.1583\n",
            "  Batch 7400/8989 - Loss: 0.1584\n",
            "  Batch 7500/8989 - Loss: 0.1585\n",
            "  Batch 7600/8989 - Loss: 0.1587\n",
            "  Batch 7700/8989 - Loss: 0.1587\n",
            "  Batch 7800/8989 - Loss: 0.1588\n",
            "  Batch 7900/8989 - Loss: 0.1589\n",
            "  Batch 8000/8989 - Loss: 0.1590\n",
            "  Batch 8100/8989 - Loss: 0.1591\n",
            "  Batch 8200/8989 - Loss: 0.1591\n",
            "  Batch 8300/8989 - Loss: 0.1593\n",
            "  Batch 8400/8989 - Loss: 0.1594\n",
            "  Batch 8500/8989 - Loss: 0.1595\n",
            "  Batch 8600/8989 - Loss: 0.1595\n",
            "  Batch 8700/8989 - Loss: 0.1596\n",
            "  Batch 8800/8989 - Loss: 0.1597\n",
            "  Batch 8900/8989 - Loss: 0.1598\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1598\n",
            "  HR@10: 0.7081\n",
            "  NDCG@10: 0.4312\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best model at epoch 3:\n",
            "  HR@10: 0.7404\n",
            "  NDCG@10: 0.4565\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.2 TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.2: Starting Training\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training for {epochs} epochs...\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track best performance\n",
        "best_hr = 0.0\n",
        "best_ndcg = 0.0\n",
        "best_epoch = 0\n",
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'hr': [],\n",
        "    'ndcg': [],\n",
        "    'time': []\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ========================================================================\n",
        "    # TRAINING PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to training mode\n",
        "    # This enables dropout and other training-specific behaviors\n",
        "    ncf_model.train()\n",
        "    \n",
        "    # Start timer for this epoch\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Generate negative samples for this epoch\n",
        "    # Important: We generate fresh negatives each epoch for better learning\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 70)\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    # Track loss for this epoch\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        # Move data to device (GPU or CPU)\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        # ================================================================\n",
        "        # FORWARD PASS\n",
        "        # ================================================================\n",
        "        # Clear gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get model predictions (raw scores/logits)\n",
        "        prediction = ncf_model(user, item)  # [batch_size]\n",
        "        \n",
        "        # ================================================================\n",
        "        # COMPUTE LOSS\n",
        "        # ================================================================\n",
        "        # Compare predictions with true labels (1 for positive, 0 for negative)\n",
        "        loss = loss_function(prediction, label)\n",
        "        \n",
        "        # ================================================================\n",
        "        # BACKWARD PASS\n",
        "        # ================================================================\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track loss\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Print progress every 100 batches\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Calculate average loss for this epoch\n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EVALUATION PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    # This disables dropout and uses deterministic behavior\n",
        "    ncf_model.eval()\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"  Evaluating on test set...\")\n",
        "    HR, NDCG = evaluate_metrics(ncf_model, test_loader, top_k, device)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['hr'].append(HR)\n",
        "    training_history['ndcg'].append(NDCG)\n",
        "    training_history['time'].append(elapsed_time)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"  Time: {time_str}\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  HR@{top_k}: {HR:.4f}\")\n",
        "    print(f\"  NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # SAVE BEST MODEL\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Check if this is the best model so far\n",
        "    if HR > best_hr:\n",
        "        best_hr = HR\n",
        "        best_ndcg = NDCG\n",
        "        best_epoch = epoch + 1\n",
        "        \n",
        "        print(f\"  ✓ New best model! (HR@{top_k}: {HR:.4f})\")\n",
        "        \n",
        "        # Save model if enabled\n",
        "        if save_model:\n",
        "            if not os.path.exists(model_path):\n",
        "                os.makedirs(model_path)\n",
        "            \n",
        "            model_filename = os.path.join(model_path, f'{model_name}.pth')\n",
        "            torch.save(ncf_model, model_filename)\n",
        "            print(f\"  ✓ Model saved to {model_filename}\")\n",
        "    else:\n",
        "        print(f\"  (Best: HR@{top_k}: {best_hr:.4f} at epoch {best_epoch})\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ========================================================================\n",
        "# TRAINING COMPLETE\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best model at epoch {best_epoch}:\")\n",
        "print(f\"  HR@{top_k}: {best_hr:.4f}\")\n",
        "print(f\"  NDCG@{top_k}: {best_ndcg:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store the NeuMF-end model for later comparison\n",
        "ncf_model_neumf_end = ncf_model\n",
        "best_hr_neumf_end = best_hr\n",
        "best_ndcg_neumf_end = best_ndcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7.5: Train Models Separately (GMF, MLP, NeuMF-pre)\n",
        "\n",
        "In this step, we'll train each model architecture separately:\n",
        "1. **Train GMF model** - Generalized Matrix Factorization only\n",
        "2. **Train MLP model** - Multi-Layer Perceptron only  \n",
        "3. **Train NeuMF-pre** - Neural Matrix Factorization using pre-trained GMF and MLP weights\n",
        "\n",
        "This approach (NeuMF-pre) typically gives the best performance as it leverages pre-trained components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.5: Training Models Separately\n",
            "======================================================================\n",
            "This will train:\n",
            "  1. GMF model (Generalized Matrix Factorization)\n",
            "  2. MLP model (Multi-Layer Perceptron)\n",
            "  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 7.5.1: Training GMF Model\n",
            "======================================================================\n",
            "Training GMF for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:20 - Loss: 0.3577 - HR@10: 0.6525 - NDCG@10: 0.3838\n",
            "  ✓ Saved best GMF model (HR@10: 0.6525)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:20 - Loss: 0.2979 - HR@10: 0.7042 - NDCG@10: 0.4259\n",
            "  ✓ Saved best GMF model (HR@10: 0.7042)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:19 - Loss: 0.2781 - HR@10: 0.7249 - NDCG@10: 0.4432\n",
            "  ✓ Saved best GMF model (HR@10: 0.7249)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:19 - Loss: 0.2663 - HR@10: 0.7370 - NDCG@10: 0.4539\n",
            "  ✓ Saved best GMF model (HR@10: 0.7370)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:20 - Loss: 0.2566 - HR@10: 0.7437 - NDCG@10: 0.4598\n",
            "  ✓ Saved best GMF model (HR@10: 0.7437)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:20 - Loss: 0.2489 - HR@10: 0.7481 - NDCG@10: 0.4633\n",
            "  ✓ Saved best GMF model (HR@10: 0.7481)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:20 - Loss: 0.2413 - HR@10: 0.7512 - NDCG@10: 0.4661\n",
            "  ✓ Saved best GMF model (HR@10: 0.7512)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:19 - Loss: 0.2345 - HR@10: 0.7499 - NDCG@10: 0.4651\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:20 - Loss: 0.2290 - HR@10: 0.7485 - NDCG@10: 0.4647\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:20 - Loss: 0.2246 - HR@10: 0.7470 - NDCG@10: 0.4631\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:19 - Loss: 0.2216 - HR@10: 0.7471 - NDCG@10: 0.4631\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:19 - Loss: 0.2187 - HR@10: 0.7462 - NDCG@10: 0.4626\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:20 - Loss: 0.2162 - HR@10: 0.7452 - NDCG@10: 0.4622\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:19 - Loss: 0.2145 - HR@10: 0.7450 - NDCG@10: 0.4614\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:20 - Loss: 0.2127 - HR@10: 0.7459 - NDCG@10: 0.4613\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:19 - Loss: 0.2111 - HR@10: 0.7450 - NDCG@10: 0.4610\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:20 - Loss: 0.2092 - HR@10: 0.7458 - NDCG@10: 0.4618\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:20 - Loss: 0.2085 - HR@10: 0.7446 - NDCG@10: 0.4609\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:19 - Loss: 0.2076 - HR@10: 0.7453 - NDCG@10: 0.4620\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:19 - Loss: 0.2065 - HR@10: 0.7459 - NDCG@10: 0.4622\n",
            "\n",
            "✓ GMF Training Complete!\n",
            "  Best epoch: 7\n",
            "  Best HR@10: 0.7512\n",
            "  Best NDCG@10: 0.4661\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7.5: TRAIN MODELS SEPARATELY (GMF, MLP, NeuMF-pre)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.5: Training Models Separately\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will train:\")\n",
        "print(\"  1. GMF model (Generalized Matrix Factorization)\")\n",
        "print(\"  2. MLP model (Multi-Layer Perceptron)\")\n",
        "print(\"  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store trained models\n",
        "trained_models = {}\n",
        "\n",
        "# ============================================================================\n",
        "# 7.5.1 TRAIN GMF MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.1: Training GMF Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create GMF model\n",
        "gmf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='GMF',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gmf_model = gmf_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "gmf_optimizer = optim.Adam(gmf_model.parameters(), lr=learning_rate)\n",
        "gmf_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for GMF\n",
        "print(f\"Training GMF for {epochs} epochs...\")\n",
        "best_hr_gmf = 0.0\n",
        "best_ndcg_gmf = 0.0\n",
        "best_epoch_gmf = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gmf_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        gmf_optimizer.zero_grad()\n",
        "        prediction = gmf_model(user, item)\n",
        "        loss = gmf_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        gmf_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    gmf_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(gmf_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_gmf:\n",
        "        best_hr_gmf = HR\n",
        "        best_ndcg_gmf = NDCG\n",
        "        best_epoch_gmf = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(gmf_model, GMF_model_path)\n",
        "            print(f\"  ✓ Saved best GMF model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ GMF Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_gmf}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_gmf:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_gmf:.4f}\")\n",
        "\n",
        "trained_models['GMF'] = {\n",
        "    'model': gmf_model,\n",
        "    'hr': best_hr_gmf,\n",
        "    'ndcg': best_ndcg_gmf,\n",
        "    'epoch': best_epoch_gmf\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.2: Training MLP Model\n",
            "======================================================================\n",
            "Training MLP for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:33 - Loss: 0.3491 - HR@10: 0.5890 - NDCG@10: 0.3364\n",
            "  ✓ Saved best MLP model (HR@10: 0.5890)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:32 - Loss: 0.3182 - HR@10: 0.6518 - NDCG@10: 0.3826\n",
            "  ✓ Saved best MLP model (HR@10: 0.6518)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:33 - Loss: 0.2971 - HR@10: 0.6923 - NDCG@10: 0.4143\n",
            "  ✓ Saved best MLP model (HR@10: 0.6923)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:32 - Loss: 0.2796 - HR@10: 0.7145 - NDCG@10: 0.4338\n",
            "  ✓ Saved best MLP model (HR@10: 0.7145)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:33 - Loss: 0.2693 - HR@10: 0.7271 - NDCG@10: 0.4442\n",
            "  ✓ Saved best MLP model (HR@10: 0.7271)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:33 - Loss: 0.2616 - HR@10: 0.7334 - NDCG@10: 0.4502\n",
            "  ✓ Saved best MLP model (HR@10: 0.7334)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:32 - Loss: 0.2552 - HR@10: 0.7377 - NDCG@10: 0.4554\n",
            "  ✓ Saved best MLP model (HR@10: 0.7377)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:32 - Loss: 0.2492 - HR@10: 0.7404 - NDCG@10: 0.4575\n",
            "  ✓ Saved best MLP model (HR@10: 0.7404)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:33 - Loss: 0.2433 - HR@10: 0.7424 - NDCG@10: 0.4603\n",
            "  ✓ Saved best MLP model (HR@10: 0.7424)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:32 - Loss: 0.2376 - HR@10: 0.7439 - NDCG@10: 0.4619\n",
            "  ✓ Saved best MLP model (HR@10: 0.7439)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:33 - Loss: 0.2325 - HR@10: 0.7435 - NDCG@10: 0.4606\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:32 - Loss: 0.2272 - HR@10: 0.7432 - NDCG@10: 0.4637\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:33 - Loss: 0.2220 - HR@10: 0.7410 - NDCG@10: 0.4624\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:32 - Loss: 0.2174 - HR@10: 0.7417 - NDCG@10: 0.4617\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:33 - Loss: 0.2123 - HR@10: 0.7388 - NDCG@10: 0.4613\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:33 - Loss: 0.2084 - HR@10: 0.7370 - NDCG@10: 0.4600\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:32 - Loss: 0.2032 - HR@10: 0.7359 - NDCG@10: 0.4579\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:32 - Loss: 0.1991 - HR@10: 0.7337 - NDCG@10: 0.4576\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:33 - Loss: 0.1949 - HR@10: 0.7279 - NDCG@10: 0.4533\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:32 - Loss: 0.1911 - HR@10: 0.7258 - NDCG@10: 0.4523\n",
            "\n",
            "✓ MLP Training Complete!\n",
            "  Best epoch: 10\n",
            "  Best HR@10: 0.7439\n",
            "  Best NDCG@10: 0.4619\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.2 TRAIN MLP MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.2: Training MLP Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create MLP model\n",
        "mlp_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='MLP',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mlp_model = mlp_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "mlp_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for MLP\n",
        "print(f\"Training MLP for {epochs} epochs...\")\n",
        "best_hr_mlp = 0.0\n",
        "best_ndcg_mlp = 0.0\n",
        "best_epoch_mlp = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        mlp_optimizer.zero_grad()\n",
        "        prediction = mlp_model(user, item)\n",
        "        loss = mlp_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    mlp_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(mlp_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_mlp:\n",
        "        best_hr_mlp = HR\n",
        "        best_ndcg_mlp = NDCG\n",
        "        best_epoch_mlp = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(mlp_model, MLP_model_path)\n",
        "            print(f\"  ✓ Saved best MLP model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ MLP Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_mlp}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_mlp:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_mlp:.4f}\")\n",
        "\n",
        "trained_models['MLP'] = {\n",
        "    'model': mlp_model,\n",
        "    'hr': best_hr_mlp,\n",
        "    'ndcg': best_ndcg_mlp,\n",
        "    'epoch': best_epoch_mlp\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.3: Training NeuMF-pre Model\n",
            "======================================================================\n",
            "Creating NeuMF model with pre-trained GMF and MLP weights...\n",
            "Training NeuMF-pre for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:28 - Loss: 0.1674 - HR@10: 0.7554 - NDCG@10: 0.4782\n",
            "  ✓ Saved best NeuMF-pre model (HR@10: 0.7554)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:28 - Loss: 0.1663 - HR@10: 0.7531 - NDCG@10: 0.4764\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:28 - Loss: 0.1648 - HR@10: 0.7517 - NDCG@10: 0.4752\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:28 - Loss: 0.1642 - HR@10: 0.7508 - NDCG@10: 0.4744\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:28 - Loss: 0.1638 - HR@10: 0.7501 - NDCG@10: 0.4739\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:28 - Loss: 0.1635 - HR@10: 0.7497 - NDCG@10: 0.4733\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:28 - Loss: 0.1633 - HR@10: 0.7492 - NDCG@10: 0.4729\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:28 - Loss: 0.1627 - HR@10: 0.7488 - NDCG@10: 0.4728\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:28 - Loss: 0.1628 - HR@10: 0.7487 - NDCG@10: 0.4724\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:28 - Loss: 0.1627 - HR@10: 0.7483 - NDCG@10: 0.4722\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:28 - Loss: 0.1622 - HR@10: 0.7481 - NDCG@10: 0.4721\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:28 - Loss: 0.1623 - HR@10: 0.7481 - NDCG@10: 0.4719\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:28 - Loss: 0.1623 - HR@10: 0.7481 - NDCG@10: 0.4720\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:28 - Loss: 0.1618 - HR@10: 0.7478 - NDCG@10: 0.4717\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:28 - Loss: 0.1619 - HR@10: 0.7477 - NDCG@10: 0.4715\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:28 - Loss: 0.1616 - HR@10: 0.7476 - NDCG@10: 0.4714\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:28 - Loss: 0.1615 - HR@10: 0.7474 - NDCG@10: 0.4713\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:28 - Loss: 0.1615 - HR@10: 0.7470 - NDCG@10: 0.4710\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:28 - Loss: 0.1616 - HR@10: 0.7468 - NDCG@10: 0.4708\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:28 - Loss: 0.1614 - HR@10: 0.7466 - NDCG@10: 0.4707\n",
            "\n",
            "✓ NeuMF-pre Training Complete!\n",
            "  Best epoch: 1\n",
            "  Best HR@10: 0.7554\n",
            "  Best NDCG@10: 0.4782\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Model           HR@{top_k}   NDCG@{top_k} Best Epoch  \n",
            "----------------------------------------------------------------------\n",
            "GMF             0.7512       0.4661       7           \n",
            "MLP             0.7439       0.4619       10          \n",
            "NeuMF-end       0.7404       0.4565       3           \n",
            "NeuMF-pre       0.7554       0.4782       1           \n",
            "======================================================================\n",
            "\n",
            "🏆 Best Model: NeuMF-pre\n",
            "   HR@10: 0.7554\n",
            "   NDCG@10: 0.4782\n",
            "\n",
            "✓ Using NeuMF-pre model for recommendations\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.3 TRAIN NeuMF-pre MODEL (Using Pre-trained GMF and MLP)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.3: Training NeuMF-pre Model\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Creating NeuMF model with pre-trained GMF and MLP weights...\")\n",
        "\n",
        "# Create NeuMF-pre model using pre-trained GMF and MLP\n",
        "neumf_pre_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='NeuMF-pre',\n",
        "    GMF_model=gmf_model,\n",
        "    MLP_model=mlp_model\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    neumf_pre_model = neumf_pre_model.cuda()\n",
        "\n",
        "# Setup optimizer (SGD is typically used for NeuMF-pre)\n",
        "neumf_pre_optimizer = optim.SGD(neumf_pre_model.parameters(), lr=learning_rate)\n",
        "neumf_pre_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for NeuMF-pre\n",
        "print(f\"Training NeuMF-pre for {epochs} epochs...\")\n",
        "best_hr_neumf_pre = 0.0\n",
        "best_ndcg_neumf_pre = 0.0\n",
        "best_epoch_neumf_pre = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    neumf_pre_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        neumf_pre_optimizer.zero_grad()\n",
        "        prediction = neumf_pre_model(user, item)\n",
        "        loss = neumf_pre_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        neumf_pre_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    neumf_pre_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(neumf_pre_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_neumf_pre:\n",
        "        best_hr_neumf_pre = HR\n",
        "        best_ndcg_neumf_pre = NDCG\n",
        "        best_epoch_neumf_pre = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(neumf_pre_model, NeuMF_model_path)\n",
        "            print(f\"  ✓ Saved best NeuMF-pre model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ NeuMF-pre Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_neumf_pre}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_neumf_pre:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_neumf_pre:.4f}\")\n",
        "\n",
        "trained_models['NeuMF-pre'] = {\n",
        "    'model': neumf_pre_model,\n",
        "    'hr': best_hr_neumf_pre,\n",
        "    'ndcg': best_ndcg_neumf_pre,\n",
        "    'epoch': best_epoch_neumf_pre\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON OF ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'HR@{top_k}':<12} {'NDCG@{top_k}':<12} {'Best Epoch':<12}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'GMF':<15} {best_hr_gmf:<12.4f} {best_ndcg_gmf:<12.4f} {best_epoch_gmf:<12}\")\n",
        "print(f\"{'MLP':<15} {best_hr_mlp:<12.4f} {best_ndcg_mlp:<12.4f} {best_epoch_mlp:<12}\")\n",
        "print(f\"{'NeuMF-end':<15} {best_hr_neumf_end:<12.4f} {best_ndcg_neumf_end:<12.4f} {best_epoch:<12}\")\n",
        "print(f\"{'NeuMF-pre':<15} {best_hr_neumf_pre:<12.4f} {best_ndcg_neumf_pre:<12.4f} {best_epoch_neumf_pre:<12}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find best model\n",
        "all_results = [\n",
        "    ('GMF', best_hr_gmf, best_ndcg_gmf),\n",
        "    ('MLP', best_hr_mlp, best_ndcg_mlp),\n",
        "    ('NeuMF-end', best_hr_neumf_end, best_ndcg_neumf_end),\n",
        "    ('NeuMF-pre', best_hr_neumf_pre, best_ndcg_neumf_pre)\n",
        "]\n",
        "best_model_name, best_hr_overall, best_ndcg_overall = max(all_results, key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   HR@{top_k}: {best_hr_overall:.4f}\")\n",
        "print(f\"   NDCG@{top_k}: {best_ndcg_overall:.4f}\")\n",
        "\n",
        "# Set the best model as the main model for recommendations\n",
        "if best_model_name == 'GMF':\n",
        "    ncf_model = gmf_model\n",
        "elif best_model_name == 'MLP':\n",
        "    ncf_model = mlp_model\n",
        "elif best_model_name == 'NeuMF-pre':\n",
        "    ncf_model = neumf_pre_model\n",
        "else:\n",
        "    ncf_model = ncf_model_neumf_end\n",
        "\n",
        "print(f\"\\n✓ Using {best_model_name} model for recommendations\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Step 7.5:\n",
        "\n",
        "**Why Train Separately?**\n",
        "\n",
        "1. **GMF (Generalized Matrix Factorization)**:\n",
        "   - Simple linear model\n",
        "   - Fast to train\n",
        "   - Good baseline performance\n",
        "   - Captures linear user-item interactions\n",
        "\n",
        "2. **MLP (Multi-Layer Perceptron)**:\n",
        "   - Deep non-linear model\n",
        "   - Can learn complex patterns\n",
        "   - Complements GMF's linear approach\n",
        "\n",
        "3. **NeuMF-pre (Pre-trained Neural Matrix Factorization)**:\n",
        "   - Combines pre-trained GMF and MLP\n",
        "   - Initializes with learned embeddings from both models\n",
        "   - Typically achieves best performance\n",
        "   - Fine-tunes the combined model\n",
        "\n",
        "**Training Strategy:**\n",
        "- Train GMF and MLP independently\n",
        "- Use their learned embeddings to initialize NeuMF\n",
        "- Fine-tune NeuMF with SGD (more stable than Adam for pre-trained models)\n",
        "- This gives better performance than training NeuMF from scratch\n",
        "\n",
        "**Model Comparison:**\n",
        "- GMF: Fast, simple, good baseline\n",
        "- MLP: Complex patterns, non-linear\n",
        "- NeuMF-end: Trained from scratch, good balance\n",
        "- NeuMF-pre: Best performance, uses pre-trained components\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 7.5 Complete!**\n",
        "\n",
        "All models have been trained separately. The best model is now available for recommendations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 7:\n",
        "\n",
        "#### 7.1 Loss Function and Optimizer\n",
        "\n",
        "**Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss):**\n",
        "\n",
        "**What it does:**\n",
        "- Combines sigmoid activation + binary cross-entropy loss\n",
        "- More numerically stable than applying sigmoid separately\n",
        "- Formula: `loss = -[y*log(σ(x)) + (1-y)*log(1-σ(x))]`\n",
        "  - Where σ(x) = sigmoid(x), y = true label (0 or 1), x = model output\n",
        "\n",
        "**Why this loss?**\n",
        "- Our task: Predict if user will like item (binary classification)\n",
        "- Labels: 1 (positive interaction) or 0 (negative interaction)\n",
        "- Model outputs: Raw scores (logits), not probabilities\n",
        "- BCEWithLogitsLoss handles the conversion internally\n",
        "\n",
        "**Numerical Stability:**\n",
        "- Direct sigmoid can cause overflow/underflow\n",
        "- BCEWithLogitsLoss uses log-sum-exp trick for stability\n",
        "- Prevents NaN/inf values during training\n",
        "\n",
        "**Adam Optimizer:**\n",
        "\n",
        "**What is Adam?**\n",
        "- Adaptive Moment Estimation\n",
        "- Combines benefits of:\n",
        "  - Momentum: Uses moving average of gradients (smoother updates)\n",
        "  - RMSprop: Adapts learning rate per parameter\n",
        "  - Bias correction: Accounts for initialization bias\n",
        "\n",
        "**Why Adam?**\n",
        "- Works well out-of-the-box (default hyperparameters)\n",
        "- Adapts learning rate automatically\n",
        "- Faster convergence than SGD for most problems\n",
        "- Good for recommendation systems\n",
        "\n",
        "**Learning Rate:**\n",
        "- 0.001 is a safe default for Adam\n",
        "- Too high: Training unstable, loss might explode\n",
        "- Too low: Training very slow, might not converge\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.2 Training Loop - Step by Step\n",
        "\n",
        "**Epoch Structure:**\n",
        "\n",
        "1. **Set Training Mode**:\n",
        "   ```python\n",
        "   model.train()  # Enables dropout, batch norm training mode\n",
        "   ```\n",
        "\n",
        "2. **Generate Negative Samples**:\n",
        "   ```python\n",
        "   train_dataset.ng_sample()  # Fresh negatives each epoch\n",
        "   ```\n",
        "   - Important: New negatives each epoch improves learning\n",
        "   - Prevents overfitting to specific negative samples\n",
        "\n",
        "3. **Iterate Through Batches**:\n",
        "   ```python\n",
        "   for user, item, label in train_loader:\n",
        "       # Process batch\n",
        "   ```\n",
        "\n",
        "4. **Forward Pass**:\n",
        "   ```python\n",
        "   prediction = model(user, item)  # Get model predictions\n",
        "   ```\n",
        "   - Model outputs raw scores (logits)\n",
        "   - Higher score = more likely user will like item\n",
        "\n",
        "5. **Compute Loss**:\n",
        "   ```python\n",
        "   loss = loss_function(prediction, label)\n",
        "   ```\n",
        "   - Compares predictions with true labels\n",
        "   - Measures how wrong the model is\n",
        "\n",
        "6. **Backward Pass**:\n",
        "   ```python\n",
        "   loss.backward()  # Compute gradients\n",
        "   optimizer.step()  # Update weights\n",
        "   ```\n",
        "   - Gradients tell us how to adjust weights\n",
        "   - Optimizer updates weights to reduce loss\n",
        "\n",
        "7. **Evaluation**:\n",
        "   ```python\n",
        "   model.eval()  # Disable dropout\n",
        "   HR, NDCG = evaluate_metrics(model, test_loader, top_k)\n",
        "   ```\n",
        "   - Test model on held-out test set\n",
        "   - Measure performance with Hit Rate and NDCG\n",
        "\n",
        "8. **Save Best Model**:\n",
        "   ```python\n",
        "   if HR > best_hr:\n",
        "       torch.save(model, 'best_model.pth')\n",
        "   ```\n",
        "   - Save model with best validation performance\n",
        "   - Prevents losing good models if training degrades\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.3 Understanding Training Progress\n",
        "\n",
        "**What to Watch:**\n",
        "\n",
        "1. **Loss Decreasing**:\n",
        "   - Should decrease over epochs\n",
        "   - If increases: learning rate too high or model unstable\n",
        "   - If plateaus: model converged or needs more capacity\n",
        "\n",
        "2. **HR and NDCG Increasing**:\n",
        "   - Should improve over epochs\n",
        "   - Early epochs: rapid improvement\n",
        "   - Later epochs: slower improvement (diminishing returns)\n",
        "\n",
        "3. **Best Model Tracking**:\n",
        "   - Model might overfit (train loss decreases, test HR decreases)\n",
        "   - We save best model based on test HR\n",
        "   - This prevents using overfitted model\n",
        "\n",
        "**Typical Training Curve:**\n",
        "```\n",
        "Epoch 1:  HR: 0.200, NDCG: 0.100  (random)\n",
        "Epoch 5:  HR: 0.500, NDCG: 0.300  (learning)\n",
        "Epoch 10: HR: 0.650, NDCG: 0.420  (improving)\n",
        "Epoch 15: HR: 0.680, NDCG: 0.440  (slowing)\n",
        "Epoch 20: HR: 0.690, NDCG: 0.445  (converged)\n",
        "```\n",
        "\n",
        "**When to Stop:**\n",
        "- If HR stops improving for several epochs\n",
        "- If test HR starts decreasing (overfitting)\n",
        "- After specified number of epochs\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.4 Model Saving\n",
        "\n",
        "**Why Save Models?**\n",
        "- Training takes time (minutes to hours)\n",
        "- Best model might not be the last epoch\n",
        "- Allows loading and using model later\n",
        "- Enables comparison of different runs\n",
        "\n",
        "**What Gets Saved:**\n",
        "- All model parameters (weights and biases)\n",
        "- Model architecture (can reconstruct model)\n",
        "- Optimizer state (optional, for resuming training)\n",
        "\n",
        "**Loading Saved Model:**\n",
        "```python\n",
        "model = torch.load('best_model.pth')\n",
        "model.eval()  # Set to evaluation mode\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.5 Training Tips\n",
        "\n",
        "**If Training is Slow:**\n",
        "- Reduce batch size (if memory allows)\n",
        "- Use GPU if available\n",
        "- Reduce number of epochs (if converged early)\n",
        "\n",
        "**If Loss Not Decreasing:**\n",
        "- Check learning rate (might be too high/low)\n",
        "- Check data loading (make sure data is correct)\n",
        "- Check model architecture (might have bugs)\n",
        "\n",
        "**If Overfitting (train good, test bad):**\n",
        "- Increase dropout rate\n",
        "- Reduce model capacity (fewer layers/embeddings)\n",
        "- Get more training data\n",
        "- Use early stopping\n",
        "\n",
        "**If Underfitting (both train and test bad):**\n",
        "- Increase model capacity\n",
        "- Train for more epochs\n",
        "- Reduce dropout\n",
        "- Check if learning rate is appropriate\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 7 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Complete training loop implemented\n",
        "- Model training and evaluation\n",
        "- Best model saving\n",
        "- Progress tracking\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8: Using the Trained Model for Recommendations\n",
        "\n",
        "Now that we have a trained model, let's learn how to use it to make recommendations! This step shows:\n",
        "1. How to load a saved model\n",
        "2. How to get recommendations for a user\n",
        "3. How to predict user-item interaction scores\n",
        "4. Practical examples\n",
        "\n",
        "This is where the model becomes useful for real-world applications!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 8: Using Trained Model for Recommendations\n",
            "======================================================================\n",
            "Using the trained model from Step 7...\n",
            "✓ Model ready for inference\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: USING THE TRAINED MODEL FOR RECOMMENDATIONS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step demonstrates how to use the trained NCF model to make recommendations.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 8.1 LOAD SAVED MODEL (Optional - if you want to load from disk)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 8: Using Trained Model for Recommendations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# If you saved a model and want to load it later, use this:\n",
        "# model_path_to_load = os.path.join(model_path, f'{model_name}.pth')\n",
        "# if os.path.exists(model_path_to_load):\n",
        "#     print(f\"Loading model from {model_path_to_load}...\")\n",
        "#     ncf_model = torch.load(model_path_to_load)\n",
        "#     if torch.cuda.is_available():\n",
        "#         ncf_model = ncf_model.cuda()\n",
        "#     ncf_model.eval()\n",
        "#     print(\"✓ Model loaded successfully!\")\n",
        "# else:\n",
        "#     print(\"Using currently trained model...\")\n",
        "\n",
        "# For now, we'll use the model we just trained\n",
        "print(\"Using the trained model from Step 7...\")\n",
        "model_path_to_load = os.path.join(model_path, f'{model_name}.pth')\n",
        "if os.path.exists(model_path_to_load):\n",
        "\n",
        "    print(\"✓ Model ready for inference\")\n",
        "    model = torch.load('models/NeuMF-end.pth' , weights_only=False)\n",
        "    model.eval()  # Important: set to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.2: Recommendation Function\n",
            "======================================================================\n",
            "✓ get_top_k_recommendations() function defined\n",
            "  - Takes user ID and candidate items\n",
            "  - Returns top-K recommendations with scores\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.2 GET TOP-K RECOMMENDATIONS FOR A USER\n",
        "# ============================================================================\n",
        "\n",
        "def get_top_k_recommendations(model, user_id, item_ids, k=10, device='cpu'):\n",
        "    \"\"\"\n",
        "    Get top-K item recommendations for a given user.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - user_id: ID of the user (integer)\n",
        "    - item_ids: List of item IDs to consider (e.g., all items or candidate items)\n",
        "    - k: Number of recommendations to return\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - top_k_items: List of top-K recommended item IDs\n",
        "    - top_k_scores: List of corresponding prediction scores\n",
        "    \"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    # Convert to tensors\n",
        "    user_tensor = torch.LongTensor([user_id] * len(item_ids))\n",
        "    item_tensor = torch.LongTensor(item_ids)\n",
        "    \n",
        "    # Move to device\n",
        "    if device == 'cuda' and torch.cuda.is_available():\n",
        "        user_tensor = user_tensor.cuda()\n",
        "        item_tensor = item_tensor.cuda()\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        scores = model(user_tensor, item_tensor)\n",
        "        scores = scores.cpu().numpy()\n",
        "    \n",
        "    # Get top-K items\n",
        "    top_k_indices = np.argsort(scores)[::-1][:k]  # Sort descending, take top K\n",
        "    top_k_items = [item_ids[i] for i in top_k_indices]\n",
        "    top_k_scores = scores[top_k_indices].tolist()\n",
        "    \n",
        "    return top_k_items, top_k_scores\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.2: Recommendation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ get_top_k_recommendations() function defined\")\n",
        "print(\"  - Takes user ID and candidate items\")\n",
        "print(\"  - Returns top-K recommendations with scores\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.3: Prediction Function\n",
            "======================================================================\n",
            "✓ predict_interaction_score() function defined\n",
            "  - Predicts score for a single user-item pair\n",
            "  - Returns a single score value\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.3 PREDICT USER-ITEM INTERACTION SCORE\n",
        "# ============================================================================\n",
        "\n",
        "def predict_interaction_score(model, user_id, item_id, device='cpu'):\n",
        "    \"\"\"\n",
        "    Predict the interaction score for a specific user-item pair.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - user_id: ID of the user (integer)\n",
        "    - item_id: ID of the item (integer)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - score: Prediction score (higher = more likely user will like item)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Convert to tensors\n",
        "    user_tensor = torch.LongTensor([user_id])\n",
        "    item_tensor = torch.LongTensor([item_id])\n",
        "    \n",
        "    # Move to device\n",
        "    if device == 'cuda' and torch.cuda.is_available():\n",
        "        user_tensor = user_tensor.cuda()\n",
        "        item_tensor = item_tensor.cuda()\n",
        "    \n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        score = model(user_tensor, item_tensor)\n",
        "        score = score.cpu().item()\n",
        "    \n",
        "    return score\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.3: Prediction Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ predict_interaction_score() function defined\")\n",
        "print(\"  - Predicts score for a single user-item pair\")\n",
        "print(\"  - Returns a single score value\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.4: Example Usage\n",
            "======================================================================\n",
            "\n",
            "Getting top-10 recommendations for User 0...\n",
            "\n",
            "✓ Top-10 Recommendations for User 0:\n",
            "----------------------------------------------------------------------\n",
            "  1. Item   565 - Score:  4.2599\n",
            "  2. Item   559 - Score:  3.9050\n",
            "  3. Item    33 - Score:  3.8329\n",
            "  4. Item   563 - Score:  3.7590\n",
            "  5. Item  1125 - Score:  3.5868\n",
            "  6. Item  1113 - Score:  3.5455\n",
            "  7. Item   344 - Score:  3.4968\n",
            "  8. Item  1811 - Score:  3.4611\n",
            "  9. Item   970 - Score:  3.4368\n",
            "  10. Item   299 - Score:  3.3611\n",
            "\n",
            "======================================================================\n",
            "Example: Predicting interaction score\n",
            "======================================================================\n",
            "User 0 - Item 565: Score = 4.2599\n",
            "  → Higher score means user is more likely to like this item\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.4 EXAMPLE: GET RECOMMENDATIONS FOR A USER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.4: Example Usage\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example: Get recommendations for user 0\n",
        "example_user_id = 0\n",
        "print(f\"\\nGetting top-{top_k} recommendations for User {example_user_id}...\")\n",
        "\n",
        "# Get all item IDs (excluding items user already interacted with in training)\n",
        "# In practice, you might want to filter out items the user has already seen\n",
        "all_item_ids = list(range(item_num))\n",
        "\n",
        "# Get top-K recommendations\n",
        "recommended_items, recommended_scores = get_top_k_recommendations(\n",
        "    model,\n",
        "    example_user_id,\n",
        "    all_item_ids,\n",
        "    k=top_k,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Top-{top_k} Recommendations for User {example_user_id}:\")\n",
        "print(\"-\" * 70)\n",
        "for i, (item_id, score) in enumerate(zip(recommended_items, recommended_scores), 1):\n",
        "    print(f\"  {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "\n",
        "# Example: Predict score for a specific user-item pair\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"Example: Predicting interaction score\")\n",
        "print(\"=\" * 70)\n",
        "example_item_id = recommended_items[0]  # Use the top recommended item\n",
        "score = predict_interaction_score(model, example_user_id, example_item_id, device=device)\n",
        "print(f\"User {example_user_id} - Item {example_item_id}: Score = {score:.4f}\")\n",
        "print(f\"  → Higher score means user is more likely to like this item\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.5: Filtered Recommendations\n",
            "======================================================================\n",
            "\n",
            "Getting filtered recommendations for User 10...\n",
            "(Excluding items user already interacted with in training)\n",
            "\n",
            "✓ Top-10 NEW Recommendations for User 10:\n",
            "----------------------------------------------------------------------\n",
            "  1. Item  2526 - Score:  3.8752\n",
            "  2. Item  2659 - Score:  3.4374\n",
            "  3. Item   939 - Score:  3.2481\n",
            "  4. Item  1325 - Score:  3.1528\n",
            "  5. Item  1650 - Score:  3.1314\n",
            "  6. Item   211 - Score:  2.9503\n",
            "  7. Item  2093 - Score:  2.9223\n",
            "  8. Item  1499 - Score:  2.9186\n",
            "  9. Item   278 - Score:  2.8437\n",
            "  10. Item   324 - Score:  2.7961\n",
            "\n",
            "======================================================================\n",
            "✓ Recommendation system ready!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.5 FILTER OUT ITEMS USER ALREADY INTERACTED WITH\n",
        "# ============================================================================\n",
        "\n",
        "def get_recommendations_excluding_training(user_id, model, train_mat, all_items, k=10, device='cpu'):\n",
        "    \"\"\"\n",
        "    Get recommendations for a user, excluding items they've already interacted with.\n",
        "    \n",
        "    This is more realistic - we don't want to recommend items the user already knows about.\n",
        "    \n",
        "    Parameters:\n",
        "    - user_id: ID of the user\n",
        "    - model: Trained NCF model\n",
        "    - train_mat: Training interaction matrix (to check what user already interacted with)\n",
        "    - all_items: List of all item IDs\n",
        "    - k: Number of recommendations\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - top_k_items: Recommended item IDs (excluding training items)\n",
        "    - top_k_scores: Corresponding scores\n",
        "    \"\"\"\n",
        "    # Filter out items user already interacted with\n",
        "    candidate_items = [item_id for item_id in all_items \n",
        "                       if (user_id, item_id) not in train_mat]\n",
        "    \n",
        "    if len(candidate_items) < k:\n",
        "        print(f\"Warning: Only {len(candidate_items)} candidate items available (requested {k})\")\n",
        "        k = len(candidate_items)\n",
        "    \n",
        "    # Get recommendations from candidate items\n",
        "    top_k_items, top_k_scores = get_top_k_recommendations(\n",
        "        model, user_id, candidate_items, k=k, device=device\n",
        "    )\n",
        "    \n",
        "    return top_k_items, top_k_scores\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.5: Filtered Recommendations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example with filtering\n",
        "example_user_id = 10\n",
        "print(f\"\\nGetting filtered recommendations for User {example_user_id}...\")\n",
        "print(\"(Excluding items user already interacted with in training)\")\n",
        "\n",
        "filtered_items, filtered_scores = get_recommendations_excluding_training(\n",
        "    example_user_id,\n",
        "    ncf_model,\n",
        "    train_mat,\n",
        "    list(range(item_num)),\n",
        "    k=top_k,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Top-{top_k} NEW Recommendations for User {example_user_id}:\")\n",
        "print(\"-\" * 70)\n",
        "for i, (item_id, score) in enumerate(zip(filtered_items, filtered_scores), 1):\n",
        "    print(f\"  {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Recommendation system ready!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 8:\n",
        "\n",
        "#### 8.1 Loading a Saved Model\n",
        "\n",
        "**When to Load:**\n",
        "- After training, if you saved the model and want to use it later\n",
        "- When deploying the model in production\n",
        "- When sharing the model with others\n",
        "\n",
        "**How to Load:**\n",
        "```python\n",
        "model = torch.load('path/to/model.pth')\n",
        "model.eval()  # Important: set to evaluation mode\n",
        "```\n",
        "\n",
        "**Why `model.eval()`?**\n",
        "- Disables dropout (uses all neurons)\n",
        "- Uses deterministic behavior\n",
        "- Required for consistent predictions\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.2 Getting Top-K Recommendations\n",
        "\n",
        "**How It Works:**\n",
        "1. **Input**: User ID and list of candidate items\n",
        "2. **Process**: \n",
        "   - Get model predictions for all candidate items\n",
        "   - Sort items by prediction score (descending)\n",
        "   - Return top-K items\n",
        "3. **Output**: List of recommended item IDs and their scores\n",
        "\n",
        "**Example Use Case:**\n",
        "```python\n",
        "# Recommend 10 movies for user 123\n",
        "recommendations, scores = get_top_k_recommendations(\n",
        "    model, user_id=123, item_ids=all_movie_ids, k=10\n",
        ")\n",
        "```\n",
        "\n",
        "**Performance Considerations:**\n",
        "- If you have many items (millions), consider:\n",
        "  - Pre-filtering candidates (e.g., by genre, popularity)\n",
        "  - Using approximate nearest neighbor search\n",
        "  - Caching embeddings for faster computation\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.3 Predicting Single Interaction Score\n",
        "\n",
        "**When to Use:**\n",
        "- Check if a specific user will like a specific item\n",
        "- Rank a small set of items\n",
        "- A/B testing different items\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "score = predict_interaction_score(model, user_id=123, item_id=456)\n",
        "if score > 0.5:  # Threshold (adjust based on your data)\n",
        "    print(\"User will likely like this item\")\n",
        "```\n",
        "\n",
        "**Interpreting Scores:**\n",
        "- Higher score = more likely user will like item\n",
        "- Scores are logits (not probabilities)\n",
        "- To get probabilities: `prob = torch.sigmoid(torch.tensor(score))`\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.4 Filtering Training Items\n",
        "\n",
        "**Why Filter?**\n",
        "- Users have already seen/interacted with training items\n",
        "- We want to recommend NEW items\n",
        "- More realistic recommendation scenario\n",
        "\n",
        "**How It Works:**\n",
        "1. Check training matrix: `(user_id, item_id) in train_mat`\n",
        "2. Exclude items user already interacted with\n",
        "3. Get recommendations from remaining items\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Get new recommendations (excluding items user already knows)\n",
        "recommendations = get_recommendations_excluding_training(\n",
        "    user_id, model, train_mat, all_items, k=10\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.5 Real-World Usage Tips\n",
        "\n",
        "**1. Pre-compute Embeddings (Optional):**\n",
        "```python\n",
        "# For faster recommendations, pre-compute item embeddings\n",
        "with torch.no_grad():\n",
        "    item_embeddings = model.embed_item_GMF(torch.arange(item_num))\n",
        "# Then use these for faster similarity search\n",
        "```\n",
        "\n",
        "**2. Batch Predictions:**\n",
        "```python\n",
        "# Predict for multiple users at once (faster)\n",
        "user_ids = torch.LongTensor([1, 2, 3, 4, 5])\n",
        "item_ids = torch.LongTensor([10, 20, 30, 40, 50])\n",
        "scores = model(user_ids, item_ids)  # Batch prediction\n",
        "```\n",
        "\n",
        "**3. Cold Start Problem:**\n",
        "- New users: No interaction history\n",
        "- Solutions: Use popularity-based recommendations, ask for preferences\n",
        "- New items: No interaction history\n",
        "- Solutions: Use content-based features, wait for initial interactions\n",
        "\n",
        "**4. Evaluation in Production:**\n",
        "- A/B testing: Compare different models\n",
        "- Online metrics: Click-through rate, conversion rate\n",
        "- Offline metrics: HR, NDCG (what we used)\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 8 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Functions to get recommendations\n",
        "- Functions to predict interaction scores\n",
        "- Example usage code\n",
        "- Understanding of how to use the model in practice\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 Congratulations! You've completed the full NCF Tutorial!**\n",
        "\n",
        "**What You've Learned:**\n",
        "1. ✅ Environment setup and imports\n",
        "2. ✅ Configuration and hyperparameters\n",
        "3. ✅ Data downloading and preprocessing\n",
        "4. ✅ PyTorch Dataset class implementation\n",
        "5. ✅ NCF model architecture (with detailed visualization)\n",
        "6. ✅ Evaluation metrics (Hit Rate and NDCG)\n",
        "7. ✅ Training loop implementation\n",
        "8. ✅ Using the model for recommendations\n",
        "\n",
        "**The Complete Pipeline:**\n",
        "```\n",
        "Data → Preprocessing → Dataset → Model → Training → Evaluation → Recommendations\n",
        "```\n",
        "\n",
        "**Next Steps:**\n",
        "- Experiment with different hyperparameters\n",
        "- Try different model architectures (GMF, MLP, NeuMF)\n",
        "- Test on different datasets\n",
        "- Deploy for production use\n",
        "- Explore advanced techniques (attention mechanisms, graph neural networks)\n",
        "\n",
        "**Thank you for following along! Happy recommending! 🚀**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 9: Loading and Using Saved Models\n",
        "\n",
        "This final step shows how to load the saved models and use them for recommendations. All models (GMF, MLP, NeuMF-end, NeuMF-pre) have been saved and can be loaded independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 9: Loading and Using Saved Models\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 9.1: Loading Saved Models\n",
            "======================================================================\n",
            "Loading GMF model from ./models/GMF.pth...\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.NCF was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.NCF])` or the `torch.serialization.safe_globals([__main__.NCF])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(GMF_model_path):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GMF model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGMF_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     gmf_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGMF_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m         gmf_loaded \u001b[38;5;241m=\u001b[39m gmf_loaded\u001b[38;5;241m.\u001b[39mcpu()\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/NCF/.venv/lib/python3.9/site-packages/torch/serialization.py:1529\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1522\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1523\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1527\u001b[0m                 )\n\u001b[1;32m   1528\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1529\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1531\u001b[0m             opened_zipfile,\n\u001b[1;32m   1532\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1536\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.NCF was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.NCF])` or the `torch.serialization.safe_globals([__main__.NCF])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 9: LOADING AND USING SAVED MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 9: Loading and Using Saved Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================================\n",
        "# 9.1 LOAD ALL SAVED MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 9.1: Loading Saved Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "# Load GMF model\n",
        "if os.path.exists(GMF_model_path):\n",
        "    print(f\"Loading GMF model from {GMF_model_path}...\")\n",
        "    gmf_loaded = torch.load(GMF_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        gmf_loaded = gmf_loaded.cpu()\n",
        "    gmf_loaded.eval()\n",
        "    loaded_models['GMF'] = gmf_loaded\n",
        "    print(\"✓ GMF model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ GMF model not found at {GMF_model_path}\")\n",
        "\n",
        "# Load MLP model\n",
        "if os.path.exists(MLP_model_path):\n",
        "    print(f\"Loading MLP model from {MLP_model_path}...\")\n",
        "    mlp_loaded = torch.load(MLP_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        mlp_loaded = mlp_loaded.cpu()\n",
        "    mlp_loaded.eval()\n",
        "    loaded_models['MLP'] = mlp_loaded\n",
        "    print(\"✓ MLP model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ MLP model not found at {MLP_model_path}\")\n",
        "\n",
        "# Load NeuMF-end model\n",
        "neumf_end_path = os.path.join(model_path, 'NeuMF-end.pth')\n",
        "if os.path.exists(neumf_end_path):\n",
        "    print(f\"Loading NeuMF-end model from {neumf_end_path}...\")\n",
        "    neumf_end_loaded = torch.load(neumf_end_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        neumf_end_loaded = neumf_end_loaded.cpu()\n",
        "    neumf_end_loaded.eval()\n",
        "    loaded_models['NeuMF-end'] = neumf_end_loaded\n",
        "    print(\"✓ NeuMF-end model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ NeuMF-end model not found at {neumf_end_path}\")\n",
        "\n",
        "# Load NeuMF-pre model\n",
        "if os.path.exists(NeuMF_model_path):\n",
        "    print(f\"Loading NeuMF-pre model from {NeuMF_model_path}...\")\n",
        "    neumf_pre_loaded = torch.load(NeuMF_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        neumf_pre_loaded = neumf_pre_loaded.cpu()\n",
        "    neumf_pre_loaded.eval()\n",
        "    loaded_models['NeuMF-pre'] = neumf_pre_loaded\n",
        "    print(\"✓ NeuMF-pre model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ NeuMF-pre model not found at {NeuMF_model_path}\")\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(loaded_models)} model(s)\")\n",
        "print(f\"  Available models: {list(loaded_models.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 9.2 COMPARE RECOMMENDATIONS FROM ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 9.2: Comparing Recommendations from All Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example user\n",
        "example_user_id = 0\n",
        "print(f\"\\nGetting recommendations for User {example_user_id} using all models...\")\n",
        "\n",
        "# Get recommendations from each model\n",
        "all_recommendations = {}\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    if model_name in loaded_models:\n",
        "        print(f\"\\n{model_name} recommendations:\")\n",
        "        recommendations, scores = get_top_k_recommendations(\n",
        "            model, example_user_id, list(range(item_num)), k=top_k, device=device\n",
        "        )\n",
        "        all_recommendations[model_name] = (recommendations, scores)\n",
        "        \n",
        "        print(f\"  Top-{top_k} items:\")\n",
        "        for i, (item_id, score) in enumerate(zip(recommendations[:5], scores[:5]), 1):\n",
        "            print(f\"    {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "        if top_k > 5:\n",
        "            print(f\"    ... and {top_k - 5} more\")\n",
        "\n",
        "# Compare overlap between models\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Recommendation Overlap Analysis\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if len(all_recommendations) >= 2:\n",
        "    model_names = list(all_recommendations.keys())\n",
        "    for i, model1 in enumerate(model_names):\n",
        "        for model2 in model_names[i+1:]:\n",
        "            items1 = set(all_recommendations[model1][0])\n",
        "            items2 = set(all_recommendations[model2][0])\n",
        "            overlap = len(items1 & items2)\n",
        "            print(f\"{model1} vs {model2}: {overlap}/{top_k} items overlap ({overlap/top_k*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
