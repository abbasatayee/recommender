{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Collaborative Filtering (NCF) - Complete Step-by-Step Tutorial\n",
        "\n",
        "## ⚠️ Important Notes Before Starting\n",
        "\n",
        "**If you encounter multiprocessing/pickling errors:**\n",
        "1. **Restart the kernel** (Kernel → Restart Kernel)\n",
        "2. **Re-run all cells from the beginning** (Cell → Run All)\n",
        "3. This is necessary because DataLoaders with `num_workers > 0` cause issues in Jupyter notebooks\n",
        "4. The notebook is configured with `num_workers=0` to avoid these errors\n",
        "\n",
        "**Best Practice:**\n",
        "- Run cells sequentially from top to bottom\n",
        "- If you modify any cell, restart kernel and re-run from that point\n",
        "- This ensures all variables are properly initialized\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This notebook provides a comprehensive, step-by-step walkthrough of the Neural Collaborative Filtering (NCF) implementation. NCF is a deep learning approach to collaborative filtering for recommendation systems, introduced in the paper \"Neural Collaborative Filtering\" by He et al. at WWW'17.\n",
        "\n",
        "### What is Neural Collaborative Filtering?\n",
        "\n",
        "Traditional collaborative filtering methods (like Matrix Factorization) use linear models to learn user-item interactions. NCF extends this by using neural networks to model non-linear interactions between users and items, potentially capturing more complex patterns in user preferences.\n",
        "\n",
        "### Key Components of NCF:\n",
        "\n",
        "1. **GMF (Generalized Matrix Factorization)**: Uses element-wise product of user and item embeddings (similar to traditional matrix factorization)\n",
        "2. **MLP (Multi-Layer Perceptron)**: Uses deep neural networks to learn non-linear user-item interactions\n",
        "3. **NeuMF (Neural Matrix Factorization)**: Combines both GMF and MLP to leverage the strengths of both approaches\n",
        "\n",
        "### Project Structure:\n",
        "\n",
        "- `config.py`: Configuration settings (dataset paths, model type, etc.)\n",
        "- `data_utils.py`: Data loading and preprocessing utilities\n",
        "- `model.py`: NCF neural network architecture\n",
        "- `evaluate.py`: Evaluation metrics (Hit Rate and NDCG)\n",
        "- `main.py`: Main training script\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Environment Setup and Imports\n",
        "\n",
        "In this first step, we'll set up our environment by importing all necessary libraries and understanding what each one does.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All libraries imported successfully!\n",
            "✓ PyTorch version: 2.8.0\n",
            "✓ CUDA available: False\n",
            "⚠ CUDA not available - training will be slower on CPU\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: ENVIRONMENT SETUP AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This cell imports all the necessary libraries for our NCF implementation.\n",
        "Let's understand what each library does:\n",
        "\n",
        "1. os: Operating system interface - used for file path operations and environment variables\n",
        "2. time: Time-related functions - used to measure training time\n",
        "3. numpy: Numerical computing library - used for array operations and mathematical functions\n",
        "4. pandas: Data manipulation library - used to load and process CSV data files\n",
        "5. scipy.sparse: Sparse matrix operations - used to efficiently store user-item interaction matrices\n",
        "6. torch: PyTorch deep learning framework - the core library for building and training neural networks\n",
        "7. torch.nn: Neural network modules - provides layers, loss functions, and activation functions\n",
        "8. torch.optim: Optimization algorithms - provides optimizers like Adam, SGD\n",
        "9. torch.utils.data: Data loading utilities - provides Dataset and DataLoader classes\n",
        "10. torch.backends.cudnn: CUDA deep neural network library backend - optimizes GPU operations\n",
        "\n",
        "NOTE: We will write ALL code directly in this notebook - no external module imports!\n",
        "This makes it easier to understand and modify each component.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import shutil\n",
        "import urllib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"✓ All libraries imported successfully!\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"⚠ CUDA not available - training will be slower on CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Step 1:\n",
        "\n",
        "**Why these imports matter:**\n",
        "\n",
        "1. **Standard Libraries (os, time)**: \n",
        "   - `os` helps us manage file paths and set environment variables (like CUDA device selection)\n",
        "   - `time` allows us to track how long training takes, which is important for performance monitoring\n",
        "\n",
        "2. **Data Processing Libraries (numpy, pandas, scipy.sparse, urllib.request, zipfile, shutil)**:\n",
        "   - `numpy`: The foundation for numerical computing in Python. All PyTorch tensors can be converted to/from numpy arrays\n",
        "   - `pandas`: Makes it easy to load CSV files containing user-item interactions\n",
        "   - `scipy.sparse`: Critical for recommendation systems! User-item interaction matrices are typically very sparse (most users haven't interacted with most items). Sparse matrices save memory and computation time\n",
        "   - `urllib.request`: Used to download dataset files from the internet\n",
        "   - `zipfile`: Used to extract compressed dataset files if needed\n",
        "   - `shutil`: Used for file operations like moving/copying downloaded files\n",
        "\n",
        "3. **PyTorch Core (torch, torch.nn, torch.optim, torch.utils.data, torch.nn.functional)**:\n",
        "   - `torch`: The main PyTorch library - provides tensor operations, automatic differentiation, and GPU support\n",
        "   - `torch.nn`: Contains pre-built neural network layers (Embedding, Linear, Dropout, etc.) and loss functions\n",
        "   - `torch.optim`: Provides optimization algorithms (Adam, SGD) that update model weights during training\n",
        "   - `torch.utils.data`: Provides `Dataset` and `DataLoader` classes that handle batching, shuffling, and parallel data loading\n",
        "   - `torch.nn.functional`: Contains functional versions of neural network operations (activation functions, etc.)\n",
        "\n",
        "4. **CUDA Optimization (torch.backends.cudnn)**:\n",
        "   - `cudnn` is NVIDIA's library for deep neural network operations\n",
        "   - Enabling `cudnn.benchmark = True` allows PyTorch to optimize convolution operations for your specific GPU, making training faster\n",
        "\n",
        "**Important Note:**\n",
        "- **We will NOT import any custom modules** - all code (configuration, data loading, model architecture, evaluation) will be written directly in this notebook\n",
        "- This approach makes it easier to understand, modify, and experiment with each component\n",
        "- Everything will be self-contained and fully explained step by step\n",
        "\n",
        "**What happens when you run this cell:**\n",
        "- All libraries are loaded into memory\n",
        "- We check if CUDA (GPU) is available, which will significantly speed up training\n",
        "- If CUDA is available, we can use GPU acceleration; otherwise, training will run on CPU (much slower)\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 1 Complete!** \n",
        "\n",
        "You've successfully set up the environment. In the next step, we'll define the configuration settings and hyperparameters directly in the notebook.\n",
        "\n",
        "---\n",
        "\n",
        "**⏸️ PAUSE: Please review Step 1 and let me know when you're ready to proceed to Step 2!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 2: Configuration and Hyperparameters\n",
        "\n",
        "In this step, we'll define all the configuration settings and hyperparameters needed for our NCF model. These settings control:\n",
        "- Which dataset to use\n",
        "- Which model architecture to train (MLP, GMF, or NeuMF)\n",
        "- File paths for data and model saving\n",
        "- Training hyperparameters (learning rate, batch size, epochs, etc.)\n",
        "- Hardware settings (GPU selection)\n",
        "\n",
        "Let's define all these settings step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dataset selected: ml-1m\n",
            "✓ Model architecture: NeuMF-end\n",
            "✓ Directories configured\n",
            "  - Data directory: ./data/ (will be created/used for downloaded data)\n",
            "  - Model save path: ./models/\n",
            "\n",
            "✓ Training hyperparameters configured:\n",
            "  - Learning rate: 0.001\n",
            "  - Dropout rate: 0.0\n",
            "  - Batch size: 256\n",
            "  - Epochs: 20\n",
            "  - Top-K for evaluation: 10\n",
            "  - Embedding dimension (factor_num): 32\n",
            "  - MLP layers: 3\n",
            "  - Training negative samples: 4\n",
            "  - Test negative samples: 99\n",
            "  - Save model: True\n",
            "  - GPU ID: 0\n",
            "\n",
            "✓ GPU configuration set\n",
            "  - Using CPU (GPU not available)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: CONFIGURATION AND HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This cell defines all configuration settings for our NCF implementation.\n",
        "We'll break it down into several sections for clarity.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 2.1 DATASET CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Choose which dataset to use\n",
        "# Options: 'ml-1m' (MovieLens 1M) or 'pinterest-20' (Pinterest dataset)\n",
        "dataset = 'ml-1m'\n",
        "assert dataset in ['ml-1m', 'pinterest-20'], \\\n",
        "    f\"Dataset must be 'ml-1m' or 'pinterest-20', got '{dataset}'\"\n",
        "\n",
        "print(f\"✓ Dataset selected: {dataset}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.2 MODEL ARCHITECTURE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Choose which model architecture to use\n",
        "# Options:\n",
        "#   - 'MLP': Multi-Layer Perceptron only (non-linear interactions)\n",
        "#   - 'GMF': Generalized Matrix Factorization only (linear interactions)\n",
        "#   - 'NeuMF-end': Neural Matrix Factorization trained from scratch (end-to-end)\n",
        "#   - 'NeuMF-pre': Neural Matrix Factorization with pre-trained GMF and MLP models\n",
        "model_name = 'NeuMF-end'\n",
        "assert model_name in ['MLP', 'GMF', 'NeuMF-end', 'NeuMF-pre'], \\\n",
        "    f\"Model must be 'MLP', 'GMF', 'NeuMF-end', or 'NeuMF-pre', got '{model_name}'\"\n",
        "\n",
        "print(f\"✓ Model architecture: {model_name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.3 DATA AND MODEL PATHS CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Data will be downloaded automatically during training\n",
        "# We'll create a local data directory to store downloaded files\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Model saving directory\n",
        "model_path = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "GMF_model_path = os.path.join(model_path, 'GMF.pth')\n",
        "MLP_model_path = os.path.join(model_path, 'MLP.pth')\n",
        "NeuMF_model_path = os.path.join(model_path, 'NeuMF.pth')\n",
        "\n",
        "print(f\"✓ Directories configured\")\n",
        "print(f\"  - Data directory: {data_dir} (will be created/used for downloaded data)\")\n",
        "print(f\"  - Model save path: {model_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.4 TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Learning rate: Controls how big steps the optimizer takes during training\n",
        "# Too high: training might be unstable or diverge\n",
        "# Too low: training will be very slow\n",
        "# Typical range: 0.0001 to 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dropout rate: Regularization technique to prevent overfitting\n",
        "# Randomly sets some neurons to zero during training\n",
        "# Range: 0.0 (no dropout) to 0.9 (very aggressive dropout)\n",
        "# 0.0 means no dropout (all neurons active)\n",
        "dropout_rate = 0.0\n",
        "\n",
        "# Batch size: Number of training examples processed together in one iteration\n",
        "# Larger batch size: more stable gradients, but requires more memory\n",
        "# Smaller batch size: less memory, but noisier gradients\n",
        "# Typical values: 64, 128, 256, 512\n",
        "batch_size = 256\n",
        "\n",
        "# Number of training epochs: How many times we'll iterate through the entire dataset\n",
        "# More epochs: better learning, but risk of overfitting\n",
        "# Too few epochs: model might not learn enough\n",
        "epochs = 20\n",
        "\n",
        "# Top-K for evaluation: When evaluating, we recommend top K items to each user\n",
        "# We measure if the true item is in the top K recommendations\n",
        "# Common values: 5, 10, 20\n",
        "top_k = 10\n",
        "\n",
        "# Factor number: Dimension of the embedding vectors for users and items\n",
        "# Larger: more capacity to learn complex patterns, but more parameters\n",
        "# Smaller: fewer parameters, faster training, but less capacity\n",
        "# Common values: 8, 16, 32, 64\n",
        "factor_num = 32\n",
        "\n",
        "# Number of MLP layers: Depth of the Multi-Layer Perceptron component\n",
        "# More layers: can learn more complex non-linear patterns\n",
        "# Fewer layers: simpler model, faster training\n",
        "# Typical range: 1 to 5 layers\n",
        "num_layers = 3\n",
        "\n",
        "# Number of negative samples for training: For each positive (user, item) pair,\n",
        "# we sample this many negative items (items the user hasn't interacted with)\n",
        "# More negatives: better learning signal, but slower training\n",
        "# Fewer negatives: faster training, but potentially weaker learning\n",
        "# Common values: 1, 4, 8\n",
        "num_ng = 4\n",
        "\n",
        "# Number of negative samples for testing: During evaluation, for each test item,\n",
        "# we also provide this many negative items. The model should rank the true item higher.\n",
        "# Typically 99 negatives + 1 positive = 100 items total per test case\n",
        "test_num_ng = 99\n",
        "\n",
        "# Whether to save the trained model\n",
        "save_model = True\n",
        "\n",
        "# GPU device ID: Which GPU to use (if multiple GPUs available)\n",
        "# Set to \"0\" for first GPU, \"1\" for second, etc.\n",
        "# Set to \"-1\" or use CPU if no GPU available\n",
        "gpu_id = \"0\"\n",
        "\n",
        "print(f\"\\n✓ Training hyperparameters configured:\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Dropout rate: {dropout_rate}\")\n",
        "print(f\"  - Batch size: {batch_size}\")\n",
        "print(f\"  - Epochs: {epochs}\")\n",
        "print(f\"  - Top-K for evaluation: {top_k}\")\n",
        "print(f\"  - Embedding dimension (factor_num): {factor_num}\")\n",
        "print(f\"  - MLP layers: {num_layers}\")\n",
        "print(f\"  - Training negative samples: {num_ng}\")\n",
        "print(f\"  - Test negative samples: {test_num_ng}\")\n",
        "print(f\"  - Save model: {save_model}\")\n",
        "print(f\"  - GPU ID: {gpu_id}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.5 GPU CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Set which GPU to use (if available)\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "\n",
        "# Enable cuDNN benchmarking for faster training (if using GPU)\n",
        "# This allows PyTorch to optimize operations for your specific GPU\n",
        "cudnn.benchmark = True\n",
        "\n",
        "print(f\"\\n✓ GPU configuration set\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  - Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(f\"  - Using CPU (GPU not available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 2:\n",
        "\n",
        "#### 2.1 Dataset Configuration\n",
        "\n",
        "**Why we need this:**\n",
        "- Different datasets have different characteristics (number of users, items, sparsity)\n",
        "- The code needs to know which dataset files to load\n",
        "- MovieLens 1M (`ml-1m`) is a classic movie recommendation dataset\n",
        "- Pinterest-20 is a larger dataset with different interaction patterns\n",
        "\n",
        "**What happens:**\n",
        "- We select which dataset to use\n",
        "- The assertion ensures we only use supported datasets\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.2 Model Architecture Configuration\n",
        "\n",
        "**The four model types explained:**\n",
        "\n",
        "1. **MLP (Multi-Layer Perceptron)**:\n",
        "   - Uses only deep neural networks to learn user-item interactions\n",
        "   - Captures non-linear patterns\n",
        "   - Good for complex recommendation scenarios\n",
        "\n",
        "2. **GMF (Generalized Matrix Factorization)**:\n",
        "   - Uses only element-wise product of embeddings (like traditional matrix factorization)\n",
        "   - Captures linear interactions\n",
        "   - Simpler and faster than MLP\n",
        "\n",
        "3. **NeuMF-end (Neural Matrix Factorization - End-to-End)**:\n",
        "   - Combines both GMF and MLP\n",
        "   - Trained from scratch (no pre-training)\n",
        "   - Best of both worlds: linear + non-linear interactions\n",
        "   - This is the recommended approach for most cases\n",
        "\n",
        "4. **NeuMF-pre (Neural Matrix Factorization - Pre-trained)**:\n",
        "   - Also combines GMF and MLP\n",
        "   - But first trains GMF and MLP separately\n",
        "   - Then initializes NeuMF with these pre-trained weights\n",
        "   - Usually gives best performance but requires more training time\n",
        "\n",
        "**Why we use 'NeuMF-end':**\n",
        "- Good balance between performance and training time\n",
        "- No need to pre-train separate models\n",
        "- Still achieves excellent results\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.3 Data and Model Paths Configuration\n",
        "\n",
        "**Data Download Approach:**\n",
        "- We will **download the data automatically** during training (in Step 3)\n",
        "- No need to manually download or specify data paths\n",
        "- Data will be stored in a local `./data/` directory\n",
        "- The download function will handle fetching the dataset files\n",
        "\n",
        "**What data files we'll download:**\n",
        "- `{dataset}.train.rating`: Training data with user-item pairs\n",
        "- `{dataset}.test.rating`: Test data with user-item pairs  \n",
        "- `{dataset}.test.negative`: Negative samples for testing (99 negatives per test case)\n",
        "\n",
        "**Model paths:**\n",
        "- Where to save trained models\n",
        "- Separate paths for GMF, MLP, and NeuMF models\n",
        "- Models will be saved in `./models/` directory\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.4 Training Hyperparameters - Deep Dive\n",
        "\n",
        "**Learning Rate (0.001):**\n",
        "- Controls step size in gradient descent\n",
        "- Too high: loss might explode or oscillate\n",
        "- Too low: training takes forever\n",
        "- 0.001 is a safe default for Adam optimizer\n",
        "\n",
        "**Dropout Rate (0.0):**\n",
        "- Regularization to prevent overfitting\n",
        "- 0.0 means no dropout (all neurons always active)\n",
        "- Increase to 0.2-0.5 if you see overfitting (training loss decreases but test performance doesn't improve)\n",
        "\n",
        "**Batch Size (256):**\n",
        "- Number of examples processed together\n",
        "- Larger = more stable, but needs more memory\n",
        "- 256 is a good balance for most GPUs\n",
        "\n",
        "**Epochs (20):**\n",
        "- One epoch = one full pass through training data\n",
        "- 20 epochs is usually enough, but depends on dataset size\n",
        "- Monitor validation metrics to stop early if needed\n",
        "\n",
        "**Top-K (10):**\n",
        "- Evaluation metric: \"Is the true item in top 10 recommendations?\"\n",
        "- Common in recommendation systems (users only see top results)\n",
        "- We'll compute Hit Rate@10 and NDCG@10\n",
        "\n",
        "**Factor Num (32):**\n",
        "- Embedding dimension for users and items\n",
        "- Each user/item gets a 32-dimensional vector\n",
        "- 32 is a good default (not too small, not too large)\n",
        "\n",
        "**Num Layers (3):**\n",
        "- Depth of MLP component\n",
        "- 3 layers means: input → hidden1 → hidden2 → hidden3 → output\n",
        "- More layers = more complex patterns, but harder to train\n",
        "\n",
        "**Num NG (4):**\n",
        "- For each positive (user, item) pair, sample 4 negative items\n",
        "- Creates 4 negative examples per positive example\n",
        "- Helps model learn to distinguish good vs bad recommendations\n",
        "\n",
        "**Test Num NG (99):**\n",
        "- During evaluation: 1 positive item + 99 negative items = 100 total\n",
        "- Model should rank the positive item in top 10\n",
        "- This simulates real-world recommendation scenario\n",
        "\n",
        "---\n",
        "\n",
        "#### 2.5 GPU Configuration\n",
        "\n",
        "**CUDA_VISIBLE_DEVICES:**\n",
        "- Tells PyTorch which GPU to use\n",
        "- Useful when you have multiple GPUs\n",
        "- Set to \"0\" for first GPU, \"1\" for second, etc.\n",
        "\n",
        "**cudnn.benchmark:**\n",
        "- Enables automatic optimization of neural network operations\n",
        "- PyTorch finds fastest algorithms for your specific GPU\n",
        "- Only works if input sizes don't change (which is true for our case)\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 2 Complete!**\n",
        "\n",
        "All configuration settings are now defined. In the next step, we'll implement the data loading utilities to read and preprocess the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**⏸️ PAUSE: Please review Step 2 and let me know when you're ready to proceed to Step 3!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 3: Data Downloading and Preprocessing\n",
        "\n",
        "In this step, we'll:\n",
        "1. Download the MovieLens 1M dataset automatically\n",
        "2. Process the raw data into the NCF format\n",
        "3. Split the data into training and testing sets\n",
        "4. Generate negative samples for evaluation\n",
        "5. Create the data structures needed for training\n",
        "\n",
        "The NCF format requires:\n",
        "- `train.rating`: Training user-item pairs (tab-separated: user_id, item_id)\n",
        "- `test.rating`: Test user-item pairs (tab-separated: user_id, item_id)\n",
        "- `test.negative`: Test data with negative samples (format: (user_id, item_id)\\tneg1\\tneg2\\t...\\tneg99)\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 3.1: Downloading MovieLens 1M Dataset\n",
            "======================================================================\n",
            "✓ Dataset already exists at ./data/ml-1m/ratings.dat\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: DATA DOWNLOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step downloads the MovieLens 1M dataset and processes it into the NCF format.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 3.1 DOWNLOAD MOVIELENS 1M DATASET\n",
        "# ============================================================================\n",
        "\n",
        "def download_ml1m_dataset(data_dir='./data'):\n",
        "    \"\"\"\n",
        "    Downloads the MovieLens 1M dataset from the official source.\n",
        "    \n",
        "    Parameters:\n",
        "    - data_dir: Directory where data will be stored\n",
        "    \n",
        "    Returns:\n",
        "    - Path to the ratings.dat file\n",
        "    \"\"\"\n",
        "    # Create data directory if it doesn't exist\n",
        "    if not os.path.exists(data_dir):\n",
        "        os.makedirs(data_dir)\n",
        "    \n",
        "    # Dataset URL and paths\n",
        "    dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
        "    zip_path = os.path.join(data_dir, 'ml-1m.zip')\n",
        "    extract_path = os.path.join(data_dir, 'ml-1m')\n",
        "    \n",
        "    # The zip file contains a folder 'ml-1m', so after extraction:\n",
        "    # Option 1: ./data/ml-1m/ratings.dat (if zip extracts to data_dir)\n",
        "    # Option 2: ./data/ml-1m/ml-1m/ratings.dat (if zip structure is nested)\n",
        "    # Let's check both possibilities\n",
        "    ratings_file_option1 = os.path.join(extract_path, 'ratings.dat')\n",
        "    ratings_file_option2 = os.path.join(extract_path, 'ml-1m', 'ratings.dat')\n",
        "    \n",
        "    # Check if already downloaded and extracted\n",
        "    if os.path.exists(ratings_file_option1):\n",
        "        print(f\"✓ Dataset already exists at {ratings_file_option1}\")\n",
        "        return ratings_file_option1\n",
        "    elif os.path.exists(ratings_file_option2):\n",
        "        print(f\"✓ Dataset already exists at {ratings_file_option2}\")\n",
        "        return ratings_file_option2\n",
        "    \n",
        "    # Download the dataset\n",
        "    if not os.path.exists(zip_path):\n",
        "        print(f\"Downloading MovieLens 1M dataset from {dataset_url}...\")\n",
        "        print(\"This may take a few minutes...\")\n",
        "        urllib.request.urlretrieve(dataset_url, zip_path)\n",
        "        print(\"✓ Download complete!\")\n",
        "    else:\n",
        "        print(f\"✓ Zip file already exists at {zip_path}\")\n",
        "    \n",
        "    # Extract the dataset\n",
        "    if not os.path.exists(ratings_file_option1) and not os.path.exists(ratings_file_option2):\n",
        "        print(f\"Extracting {zip_path}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(data_dir)\n",
        "        print(\"✓ Extraction complete!\")\n",
        "        \n",
        "        # Clean up zip file to save space\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "            print(\"✓ Removed zip file to save space\")\n",
        "    else:\n",
        "        print(f\"✓ Dataset already extracted\")\n",
        "    \n",
        "    # Find the ratings file (check both possible locations)\n",
        "    ratings_file = None\n",
        "    if os.path.exists(ratings_file_option1):\n",
        "        ratings_file = ratings_file_option1\n",
        "    elif os.path.exists(ratings_file_option2):\n",
        "        ratings_file = ratings_file_option2\n",
        "    else:\n",
        "        # If still not found, search for ratings.dat in the extracted directory\n",
        "        for root, dirs, files in os.walk(extract_path):\n",
        "            if 'ratings.dat' in files:\n",
        "                ratings_file = os.path.join(root, 'ratings.dat')\n",
        "                break\n",
        "    \n",
        "    # Verify the ratings file exists\n",
        "    if not ratings_file or not os.path.exists(ratings_file):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Expected ratings file not found. Checked:\\n\"\n",
        "            f\"  - {ratings_file_option1}\\n\"\n",
        "            f\"  - {ratings_file_option2}\\n\"\n",
        "            f\"  - Searched in {extract_path}\"\n",
        "        )\n",
        "    \n",
        "    print(f\"✓ Dataset ready. Ratings file at: {ratings_file}\")\n",
        "    return ratings_file\n",
        "\n",
        "# Download the dataset\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 3.1: Downloading MovieLens 1M Dataset\")\n",
        "print(\"=\" * 70)\n",
        "ratings_file = download_ml1m_dataset(data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 3.2: Preprocessing Data into NCF Format\n",
            "======================================================================\n",
            "Loading ratings data...\n",
            "✓ Loaded 1000209 ratings\n",
            "  - Unique users: 6040\n",
            "  - Unique movies: 3706\n",
            "\n",
            "Filtering positive interactions (ratings >= 4)...\n",
            "✓ 575281 positive interactions (out of 1000209 total)\n",
            "\n",
            "Remapping user and item IDs to be contiguous...\n",
            "✓ Remapped to 6038 users and 3533 items\n",
            "\n",
            "Splitting data (train: 80%, test: 20%)...\n",
            "✓ Training pairs: 460225\n",
            "✓ Test pairs: 115056\n",
            "\n",
            "Saving training data to ./data/ml-1m.train.rating...\n",
            "✓ Saved 460225 training pairs\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Generating test negative samples (99 negatives per test case)...\n",
            "✓ Generated test negative samples: 115056 test cases\n",
            "✓ Saved 115056 test pairs\n",
            "\n",
            "======================================================================\n",
            "✓ Data preprocessing complete!\n",
            "======================================================================\n",
            "Generated files:\n",
            "  - ./data/ml-1m.train.rating\n",
            "  - ./data/ml-1m.test.rating\n",
            "  - ./data/ml-1m.test.negative\n",
            "\n",
            "✓ Final statistics:\n",
            "  - Number of users: 6038\n",
            "  - Number of items: 3533\n",
            "  - Training interactions: 460225\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 3.2 LOAD AND PREPROCESS THE DATA\n",
        "# ============================================================================\n",
        "\n",
        "def preprocess_ml1m_to_ncf_format(ratings_file, data_dir, test_ratio=0.2, test_negatives=99):\n",
        "    \"\"\"\n",
        "    Processes the MovieLens 1M dataset into NCF format.\n",
        "    \n",
        "    Parameters:\n",
        "    - ratings_file: Path to the ratings.dat file\n",
        "    - data_dir: Directory to save processed files\n",
        "    - test_ratio: Ratio of data to use for testing (default 0.2 = 20%)\n",
        "    - test_negatives: Number of negative samples per test case (default 99)\n",
        "    \n",
        "    Returns:\n",
        "    - Paths to the generated files\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3.2: Preprocessing Data into NCF Format\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Load ratings data\n",
        "    print(\"Loading ratings data...\")\n",
        "    ratings = pd.read_csv(\n",
        "        ratings_file,\n",
        "        sep='::',\n",
        "        engine='python',\n",
        "        names=['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
        "        dtype={'UserID': np.int32, 'MovieID': np.int32, 'Rating': np.float32, 'Timestamp': np.int32}\n",
        "    )\n",
        "    \n",
        "    print(f\"✓ Loaded {len(ratings)} ratings\")\n",
        "    print(f\"  - Unique users: {ratings['UserID'].nunique()}\")\n",
        "    print(f\"  - Unique movies: {ratings['MovieID'].nunique()}\")\n",
        "    \n",
        "    # Filter ratings >= 4 (positive interactions)\n",
        "    # In recommendation systems, we typically treat ratings >= 4 as positive\n",
        "    print(\"\\nFiltering positive interactions (ratings >= 4)...\")\n",
        "    positive_ratings = ratings[ratings['Rating'] >= 4].copy()\n",
        "    print(f\"✓ {len(positive_ratings)} positive interactions (out of {len(ratings)} total)\")\n",
        "    \n",
        "    # Remap user and item IDs to be contiguous (0-indexed)\n",
        "    print(\"\\nRemapping user and item IDs to be contiguous...\")\n",
        "    unique_users = sorted(positive_ratings['UserID'].unique())\n",
        "    unique_items = sorted(positive_ratings['MovieID'].unique())\n",
        "    \n",
        "    user_map = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
        "    item_map = {old_id: new_id for new_id, old_id in enumerate(unique_items)}\n",
        "    \n",
        "    positive_ratings['user'] = positive_ratings['UserID'].map(user_map)\n",
        "    positive_ratings['item'] = positive_ratings['MovieID'].map(item_map)\n",
        "    \n",
        "    user_num = len(unique_users)\n",
        "    item_num = len(unique_items)\n",
        "    print(f\"✓ Remapped to {user_num} users and {item_num} items\")\n",
        "    \n",
        "    # Create user-item pairs\n",
        "    user_item_pairs = positive_ratings[['user', 'item']].values\n",
        "    \n",
        "    # Split into train and test sets\n",
        "    print(f\"\\nSplitting data (train: {1-test_ratio:.0%}, test: {test_ratio:.0%})...\")\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    n_total = len(user_item_pairs)\n",
        "    n_test = int(n_total * test_ratio)\n",
        "    \n",
        "    # Shuffle indices\n",
        "    indices = np.random.permutation(n_total)\n",
        "    test_indices = indices[:n_test]\n",
        "    train_indices = indices[n_test:]\n",
        "    \n",
        "    train_pairs = user_item_pairs[train_indices]\n",
        "    test_pairs = user_item_pairs[test_indices]\n",
        "    \n",
        "    print(f\"✓ Training pairs: {len(train_pairs)}\")\n",
        "    print(f\"✓ Test pairs: {len(test_pairs)}\")\n",
        "    \n",
        "    # Save training data\n",
        "    train_file = os.path.join(data_dir, 'ml-1m.train.rating')\n",
        "    print(f\"\\nSaving training data to {train_file}...\")\n",
        "    train_df = pd.DataFrame(train_pairs, columns=['user', 'item'])\n",
        "    train_df.to_csv(train_file, sep='\\t', header=False, index=False)\n",
        "    print(f\"✓ Saved {len(train_df)} training pairs\")\n",
        "    \n",
        "    # Create training matrix (for negative sampling)\n",
        "    print(\"\\nCreating training interaction matrix...\")\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for u, i in train_pairs:\n",
        "        train_mat[u, i] = 1.0\n",
        "    print(f\"✓ Training matrix created: {train_mat.nnz} interactions\")\n",
        "    \n",
        "    # Generate test negative samples\n",
        "    print(f\"\\nGenerating test negative samples ({test_negatives} negatives per test case)...\")\n",
        "    test_negative_file = os.path.join(data_dir, 'ml-1m.test.negative')\n",
        "    \n",
        "    with open(test_negative_file, 'w') as f:\n",
        "        for u, i in test_pairs:\n",
        "            # Write the positive pair\n",
        "            negatives = []\n",
        "            attempts = 0\n",
        "            max_attempts = test_negatives * 10  # Safety limit\n",
        "            \n",
        "            # Sample negative items (items not in training set for this user)\n",
        "            while len(negatives) < test_negatives and attempts < max_attempts:\n",
        "                neg_item = np.random.randint(item_num)\n",
        "                # Make sure it's not in training set for this user\n",
        "                if (u, neg_item) not in train_mat:\n",
        "                    negatives.append(neg_item)\n",
        "                attempts += 1\n",
        "            \n",
        "            # If we couldn't find enough negatives, pad with random items\n",
        "            while len(negatives) < test_negatives:\n",
        "                neg_item = np.random.randint(item_num)\n",
        "                if neg_item not in negatives:\n",
        "                    negatives.append(neg_item)\n",
        "            \n",
        "            # Write in NCF format: (user, item)\\tneg1\\tneg2\\t...\\tneg99\n",
        "            line = f\"({u}, {i})\" + \"\\t\" + \"\\t\".join(map(str, negatives)) + \"\\n\"\n",
        "            f.write(line)\n",
        "    \n",
        "    print(f\"✓ Generated test negative samples: {len(test_pairs)} test cases\")\n",
        "    \n",
        "    # Save test data (for reference, though NCF mainly uses test.negative)\n",
        "    test_file = os.path.join(data_dir, 'ml-1m.test.rating')\n",
        "    test_df = pd.DataFrame(test_pairs, columns=['user', 'item'])\n",
        "    test_df.to_csv(test_file, sep='\\t', header=False, index=False)\n",
        "    print(f\"✓ Saved {len(test_df)} test pairs\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ Data preprocessing complete!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Generated files:\")\n",
        "    print(f\"  - {train_file}\")\n",
        "    print(f\"  - {test_file}\")\n",
        "    print(f\"  - {test_negative_file}\")\n",
        "    \n",
        "    return train_file, test_file, test_negative_file, user_num, item_num, train_mat\n",
        "\n",
        "# Process the data\n",
        "train_rating_path, test_rating_path, test_negative_path, user_num, item_num, train_mat = \\\n",
        "    preprocess_ml1m_to_ncf_format(ratings_file, data_dir, test_ratio=0.2, test_negatives=99)\n",
        "\n",
        "print(f\"\\n✓ Final statistics:\")\n",
        "print(f\"  - Number of users: {user_num}\")\n",
        "print(f\"  - Number of items: {item_num}\")\n",
        "print(f\"  - Training interactions: {train_mat.nnz}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 3:\n",
        "\n",
        "#### 3.1 Downloading the Dataset\n",
        "\n",
        "**What is MovieLens 1M?**\n",
        "- A benchmark dataset for recommendation systems\n",
        "- Contains 1,000,209 ratings from 6,040 users on 3,900 movies\n",
        "- Ratings are on a scale of 1-5 stars\n",
        "- Widely used in research and benchmarking\n",
        "\n",
        "**Download Process:**\n",
        "1. **Check if data exists**: Avoids redundant downloads\n",
        "2. **Download zip file**: Fetches from the official GroupLens website\n",
        "3. **Extract contents**: Unzips the dataset files\n",
        "4. **Cleanup**: Removes zip file to save disk space\n",
        "5. **Verification**: Ensures the ratings file exists\n",
        "\n",
        "**Files in the dataset:**\n",
        "- `ratings.dat`: User-item ratings (what we need)\n",
        "- `movies.dat`: Movie information (not used in NCF)\n",
        "- `users.dat`: User information (not used in NCF)\n",
        "\n",
        "---\n",
        "\n",
        "#### 3.2 Data Preprocessing - Deep Dive\n",
        "\n",
        "**Step 1: Load Ratings**\n",
        "- Reads the `ratings.dat` file with `::` separator\n",
        "- Columns: UserID, MovieID, Rating, Timestamp\n",
        "- We only need UserID, MovieID, and Rating\n",
        "\n",
        "**Step 2: Filter Positive Interactions**\n",
        "- **Why filter?**: In implicit feedback (click/no-click), we only have positive signals\n",
        "- **Threshold**: Ratings >= 4 are considered positive (user liked the item)\n",
        "- This converts explicit ratings to implicit feedback format\n",
        "- Result: Binary interaction matrix (1 = liked, 0 = not interacted)\n",
        "\n",
        "**Step 3: Remap IDs**\n",
        "- **Why remap?**: Original IDs might not be contiguous (e.g., user IDs: 1, 5, 10, 100...)\n",
        "- **Benefit**: Contiguous IDs (0, 1, 2, 3...) are more efficient for embeddings\n",
        "- Creates mapping dictionaries: `{old_id: new_id}`\n",
        "- Final: `user_num` users (0 to user_num-1) and `item_num` items (0 to item_num-1)\n",
        "\n",
        "**Step 4: Train-Test Split**\n",
        "- **Ratio**: 80% training, 20% testing (standard in recommendation systems)\n",
        "- **Method**: Random shuffle with fixed seed (for reproducibility)\n",
        "- **Important**: We split user-item pairs, not users or items\n",
        "- This means some users appear in both train and test sets (realistic scenario)\n",
        "\n",
        "**Step 5: Create Training Matrix**\n",
        "- **Sparse matrix**: Most user-item pairs don't exist (sparse)\n",
        "- **Format**: Dictionary of Keys (DOK) matrix - efficient for sparse data\n",
        "- **Purpose**: Used during training to avoid sampling negative items that user already interacted with\n",
        "- **Memory efficient**: Only stores non-zero entries\n",
        "\n",
        "**Step 6: Generate Test Negative Samples**\n",
        "- **Format**: For each test (user, item) pair, we provide:\n",
        "  - 1 positive item (the true item)\n",
        "  - 99 negative items (items user hasn't interacted with)\n",
        "- **Evaluation**: Model should rank the positive item in top 10\n",
        "- **Sampling strategy**: Randomly sample items not in user's training set\n",
        "- **Output format**: `(user, item)\\tneg1\\tneg2\\t...\\tneg99`\n",
        "\n",
        "**Why 99 negatives?**\n",
        "- Simulates real-world scenario: recommend 1 item from 100 candidates\n",
        "- Standard evaluation protocol in recommendation systems\n",
        "- Makes evaluation more realistic and challenging\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 3 Complete!**\n",
        "\n",
        "The data is now ready for training. We have:\n",
        "- Training data with user-item pairs\n",
        "- Test data with negative samples\n",
        "- Training matrix for efficient negative sampling\n",
        "- User and item counts for model initialization\n",
        "\n",
        "---\n",
        "\n",
        "## Step 4: PyTorch Dataset Class and Data Loading\n",
        "\n",
        "In this step, we'll create:\n",
        "1. A function to load all data files into memory\n",
        "2. A PyTorch Dataset class (`NCFData`) for efficient data loading\n",
        "3. Data loaders for training and testing\n",
        "\n",
        "The Dataset class handles:\n",
        "- Loading training and test data\n",
        "- Negative sampling during training\n",
        "- Batching and shuffling\n",
        "- Integration with PyTorch's DataLoader\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 4.1: Loading Data Files\n",
            "======================================================================\n",
            "Loading training data from ./data/ml-1m.train.rating...\n",
            "✓ Loaded 460225 training pairs\n",
            "  - Users: 6038\n",
            "  - Items: 3533\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Loading test data from ./data/ml-1m.test.negative...\n",
            "✓ Loaded 11505600 test pairs (including negatives)\n",
            "\n",
            "======================================================================\n",
            "✓ Data loading complete!\n",
            "======================================================================\n",
            "\n",
            "✓ Final data statistics:\n",
            "  - Training pairs: 460225\n",
            "  - Test pairs: 11505600\n",
            "  - Users: 6038\n",
            "  - Items: 3533\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: PYTORCH DATASET CLASS AND DATA LOADING\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step creates the PyTorch Dataset class and data loading functions.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 4.1 LOAD ALL DATA FILES\n",
        "# ============================================================================\n",
        "\n",
        "def load_all_data(train_rating_path, test_negative_path):\n",
        "    \"\"\"\n",
        "    Loads all data files into memory for efficient access during training.\n",
        "    \n",
        "    This function loads:\n",
        "    1. Training data: user-item pairs from train.rating file\n",
        "    2. Test data: user-item pairs with negative samples from test.negative file\n",
        "    3. Training matrix: sparse matrix for efficient negative sampling\n",
        "    \n",
        "    Parameters:\n",
        "    - train_rating_path: Path to the training rating file\n",
        "    - test_negative_path: Path to the test negative file\n",
        "    \n",
        "    Returns:\n",
        "    - train_data: List of [user, item] pairs for training\n",
        "    - test_data: List of [user, item] pairs for testing (includes negatives)\n",
        "    - user_num: Total number of users\n",
        "    - item_num: Total number of items\n",
        "    - train_mat: Sparse matrix of training interactions\n",
        "    \"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 4.1: Loading Data Files\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Load training data\n",
        "    print(f\"Loading training data from {train_rating_path}...\")\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating_path,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['user', 'item'],\n",
        "        usecols=[0, 1],\n",
        "        dtype={0: np.int32, 1: np.int32}\n",
        "    )\n",
        "    \n",
        "    # Calculate number of users and items\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "    \n",
        "    print(f\"✓ Loaded {len(train_data)} training pairs\")\n",
        "    print(f\"  - Users: {user_num}\")\n",
        "    print(f\"  - Items: {item_num}\")\n",
        "    \n",
        "    # Convert to list of lists for easier processing\n",
        "    train_data = train_data.values.tolist()\n",
        "    \n",
        "    # Create sparse training matrix (Dictionary of Keys format)\n",
        "    # This is used to quickly check if a user-item pair exists in training data\n",
        "    print(\"\\nCreating training interaction matrix...\")\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for u, i in train_data:\n",
        "        train_mat[u, i] = 1.0\n",
        "    print(f\"✓ Training matrix created: {train_mat.nnz} interactions\")\n",
        "    \n",
        "    # Load test data with negative samples\n",
        "    print(f\"\\nLoading test data from {test_negative_path}...\")\n",
        "    test_data = []\n",
        "    with open(test_negative_path, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line is not None and line != '':\n",
        "            # Format: (user, item)\\tneg1\\tneg2\\t...\\tneg99\n",
        "            arr = line.strip().split('\\t')\n",
        "            \n",
        "            # Parse the positive pair: (user, item)\n",
        "            # eval() converts string \"(123, 456)\" to tuple (123, 456)\n",
        "            positive_pair = eval(arr[0])\n",
        "            u = positive_pair[0]\n",
        "            i = positive_pair[1]\n",
        "            \n",
        "            # Add the positive pair\n",
        "            test_data.append([u, i])\n",
        "            \n",
        "            # Add all negative items for this user\n",
        "            for neg_item in arr[1:]:\n",
        "                if neg_item:  # Skip empty strings\n",
        "                    test_data.append([u, int(neg_item)])\n",
        "            \n",
        "            line = fd.readline()\n",
        "    \n",
        "    print(f\"✓ Loaded {len(test_data)} test pairs (including negatives)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ Data loading complete!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return train_data, test_data, user_num, item_num, train_mat\n",
        "\n",
        "# Load all data\n",
        "train_data, test_data, user_num, item_num, train_mat = load_all_data(\n",
        "    train_rating_path, test_negative_path\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Final data statistics:\")\n",
        "print(f\"  - Training pairs: {len(train_data)}\")\n",
        "print(f\"  - Test pairs: {len(test_data)}\")\n",
        "print(f\"  - Users: {user_num}\")\n",
        "print(f\"  - Items: {item_num}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 4.2: PyTorch Dataset Class Created\n",
            "======================================================================\n",
            "✓ NCFData class defined\n",
            "  - Handles positive and negative sampling\n",
            "  - Compatible with PyTorch DataLoader\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4.2 CREATE PYTORCH DATASET CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class NCFData(data.Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset class for Neural Collaborative Filtering.\n",
        "    \n",
        "    This class handles:\n",
        "    - Loading user-item pairs\n",
        "    - Negative sampling during training\n",
        "    - Providing data to PyTorch DataLoader\n",
        "    \n",
        "    Key concepts:\n",
        "    - Positive samples: Real user-item interactions (label = 1)\n",
        "    - Negative samples: Random items user hasn't interacted with (label = 0)\n",
        "    - During training: We mix positives and negatives\n",
        "    - During testing: We only use the provided test data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, features, num_item, train_mat=None, num_ng=0, is_training=None):\n",
        "        \"\"\"\n",
        "        Initialize the NCF Dataset.\n",
        "        \n",
        "        Parameters:\n",
        "        - features: List of [user, item] pairs (positive interactions)\n",
        "        - num_item: Total number of items (for negative sampling)\n",
        "        - train_mat: Sparse matrix of training interactions (to avoid sampling existing pairs)\n",
        "        - num_ng: Number of negative samples per positive sample (for training)\n",
        "        - is_training: Whether this is training data (True) or test data (False)\n",
        "        \"\"\"\n",
        "        super(NCFData, self).__init__()\n",
        "        \n",
        "        # Store positive samples (user-item pairs from training/test data)\n",
        "        self.features_ps = features\n",
        "        \n",
        "        # Store metadata\n",
        "        self.num_item = num_item\n",
        "        self.train_mat = train_mat  # Used to check if (user, item) exists\n",
        "        self.num_ng = num_ng  # Number of negatives per positive\n",
        "        self.is_training = is_training  # Training or testing mode\n",
        "        \n",
        "        # Initialize labels (will be filled during negative sampling)\n",
        "        self.labels = [0 for _ in range(len(features))]\n",
        "        \n",
        "        # These will be populated by ng_sample() during training\n",
        "        self.features_ng = []  # Negative samples\n",
        "        self.features_fill = []  # Combined positives + negatives\n",
        "        self.labels_fill = []  # Labels for combined features\n",
        "    \n",
        "    def ng_sample(self):\n",
        "        \"\"\"\n",
        "        Generate negative samples for training.\n",
        "        \n",
        "        For each positive (user, item) pair:\n",
        "        - Sample num_ng random items that the user hasn't interacted with\n",
        "        - These become negative examples (label = 0)\n",
        "        \n",
        "        This function is called once per epoch before training starts.\n",
        "        \"\"\"\n",
        "        assert self.is_training, 'Negative sampling only needed during training'\n",
        "        \n",
        "        print(f\"Generating {self.num_ng} negative samples per positive pair...\")\n",
        "        self.features_ng = []\n",
        "        \n",
        "        # For each positive pair, generate num_ng negative samples\n",
        "        for x in self.features_ps:\n",
        "            u = x[0]  # User ID\n",
        "            # Generate num_ng negative items for this user\n",
        "            for t in range(self.num_ng):\n",
        "                # Sample a random item\n",
        "                j = np.random.randint(self.num_item)\n",
        "                \n",
        "                # Make sure this item is NOT in the user's training set\n",
        "                # Keep sampling until we find a negative item\n",
        "                while (u, j) in self.train_mat:\n",
        "                    j = np.random.randint(self.num_item)\n",
        "                \n",
        "                # Add this negative sample\n",
        "                self.features_ng.append([u, j])\n",
        "        \n",
        "        # Create labels: 1 for positives, 0 for negatives\n",
        "        labels_ps = [1 for _ in range(len(self.features_ps))]\n",
        "        labels_ng = [0 for _ in range(len(self.features_ng))]\n",
        "        \n",
        "        # Combine positives and negatives\n",
        "        self.features_fill = self.features_ps + self.features_ng\n",
        "        self.labels_fill = labels_ps + labels_ng\n",
        "        \n",
        "        print(f\"✓ Generated {len(self.features_ng)} negative samples\")\n",
        "        print(f\"  - Total samples (positives + negatives): {len(self.features_fill)}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the total number of samples.\n",
        "        \n",
        "        During training: (num_ng + 1) * num_positives\n",
        "          - 1 positive + num_ng negatives per positive pair\n",
        "        During testing: Just the number of positive pairs\n",
        "        \"\"\"\n",
        "        if self.is_training:\n",
        "            return (self.num_ng + 1) * len(self.labels)\n",
        "        else:\n",
        "            return len(self.features_ps)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single sample (user, item, label) by index.\n",
        "        \n",
        "        This is called by PyTorch DataLoader to get batches of data.\n",
        "        \n",
        "        Parameters:\n",
        "        - idx: Index of the sample to retrieve\n",
        "        \n",
        "        Returns:\n",
        "        - user: User ID (integer)\n",
        "        - item: Item ID (integer)\n",
        "        - label: 1 for positive, 0 for negative (integer)\n",
        "        \"\"\"\n",
        "        # During training: use combined features (positives + negatives)\n",
        "        # During testing: use only positive features\n",
        "        if self.is_training:\n",
        "            features = self.features_fill\n",
        "            labels = self.labels_fill\n",
        "        else:\n",
        "            features = self.features_ps\n",
        "            labels = self.labels\n",
        "        \n",
        "        # Get the specific sample\n",
        "        user = features[idx][0]\n",
        "        item = features[idx][1]\n",
        "        label = labels[idx]\n",
        "        \n",
        "        return user, item, label\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 4.2: PyTorch Dataset Class Created\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NCFData class defined\")\n",
        "print(\"  - Handles positive and negative sampling\")\n",
        "print(\"  - Compatible with PyTorch DataLoader\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 4.3: Creating Data Loaders\n",
            "======================================================================\n",
            "⚠ IMPORTANT: If you get multiprocessing/pickling errors,\n",
            "   restart the kernel and re-run all cells from the beginning.\n",
            "======================================================================\n",
            "✓ Training dataset created: 2301125 samples\n",
            "✓ Test dataset created: 11505600 samples\n",
            "\n",
            "✓ Data loaders created:\n",
            "  - Training batch size: 256\n",
            "  - Test batch size: 100 (1 positive + 99 negatives)\n",
            "  - Training batches per epoch: 8989\n",
            "  - Test batches: 115056\n",
            "  - num_workers: 0 (verified: safe for notebooks)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 4.3 CREATE DATA LOADERS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 4.3: Creating Data Loaders\")\n",
        "print(\"=\" * 70)\n",
        "print(\"⚠ IMPORTANT: If you get multiprocessing/pickling errors,\")\n",
        "print(\"   restart the kernel and re-run all cells from the beginning.\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create training dataset\n",
        "# num_ng: number of negative samples per positive (defined in Step 2)\n",
        "train_dataset = NCFData(\n",
        "    train_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=num_ng,  # From Step 2 configuration\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "# Create test dataset\n",
        "# num_ng=0: no negative sampling needed (negatives already in test_data)\n",
        "test_dataset = NCFData(\n",
        "    test_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=0,  # No negative sampling for testing\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "print(f\"✓ Training dataset created: {len(train_dataset)} samples\")\n",
        "print(f\"✓ Test dataset created: {len(test_dataset)} samples\")\n",
        "\n",
        "# Create data loaders\n",
        "# DataLoader handles batching, shuffling, and parallel loading\n",
        "# \n",
        "# CRITICAL: num_workers MUST be 0 for Jupyter notebooks!\n",
        "# Using num_workers > 0 causes multiprocessing/pickling errors because\n",
        "# classes defined in notebooks can't be pickled by worker processes.\n",
        "# If running as a .py script (not notebook), you can use num_workers=4.\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,  # From Step 2 configuration\n",
        "    shuffle=True,  # Shuffle training data each epoch\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks (avoids pickling errors)\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_num_ng + 1,  # 1 positive + test_num_ng negatives\n",
        "    shuffle=False,  # Don't shuffle test data\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Verify num_workers is 0 (safety check)\n",
        "assert train_loader.num_workers == 0, f\"ERROR: train_loader.num_workers is {train_loader.num_workers}, must be 0!\"\n",
        "assert test_loader.num_workers == 0, f\"ERROR: test_loader.num_workers is {test_loader.num_workers}, must be 0!\"\n",
        "\n",
        "print(f\"\\n✓ Data loaders created:\")\n",
        "print(f\"  - Training batch size: {batch_size}\")\n",
        "print(f\"  - Test batch size: {test_num_ng + 1} (1 positive + {test_num_ng} negatives)\")\n",
        "print(f\"  - Training batches per epoch: {len(train_loader)}\")\n",
        "print(f\"  - Test batches: {len(test_loader)}\")\n",
        "print(f\"  - num_workers: {train_loader.num_workers} (verified: safe for notebooks)\")\n",
        "\n",
        "# Note: We'll call train_dataset.ng_sample() before each training epoch\n",
        "# to generate fresh negative samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 4:\n",
        "\n",
        "#### 4.1 Loading Data Files\n",
        "\n",
        "**Why load all data at once?**\n",
        "- Training data is relatively small (fits in memory)\n",
        "- Loading once is faster than reading from disk repeatedly\n",
        "- Enables efficient negative sampling\n",
        "\n",
        "**What we load:**\n",
        "1. **Training data**: List of [user, item] pairs\n",
        "2. **Test data**: List of [user, item] pairs (includes 1 positive + 99 negatives per test case)\n",
        "3. **Training matrix**: Sparse matrix for O(1) lookup of existing interactions\n",
        "\n",
        "**Training Matrix (DOK format):**\n",
        "- **DOK = Dictionary of Keys**: Efficient sparse matrix format\n",
        "- Only stores non-zero entries: `{(user, item): 1.0}`\n",
        "- Fast lookup: `(u, i) in train_mat` checks if user u interacted with item i\n",
        "- Memory efficient: Only stores interactions, not entire matrix\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.2 PyTorch Dataset Class - Deep Dive\n",
        "\n",
        "**What is a PyTorch Dataset?**\n",
        "- A class that implements `__len__()` and `__getitem__()`\n",
        "- Allows PyTorch to load data in batches\n",
        "- Handles data preprocessing and sampling\n",
        "\n",
        "**NCFData Class Components:**\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "   - Stores positive samples (user-item pairs)\n",
        "   - Stores metadata (num_items, train_mat, etc.)\n",
        "   - Prepares for negative sampling\n",
        "\n",
        "2. **Negative Sampling (`ng_sample`)**:\n",
        "   - **When called**: Once per epoch, before training starts\n",
        "   - **What it does**: For each positive pair, samples `num_ng` negative items\n",
        "   - **How it works**:\n",
        "     - Randomly sample an item\n",
        "     - Check if (user, item) exists in training matrix\n",
        "     - If exists, sample again (avoid positive items)\n",
        "     - If not exists, add as negative sample\n",
        "   - **Result**: Creates balanced dataset (1 positive : num_ng negatives)\n",
        "\n",
        "3. **Length (`__len__`)**:\n",
        "   - Training: `(num_ng + 1) * num_positives`\n",
        "     - Example: 1000 positives × (4 negatives + 1 positive) = 5000 samples\n",
        "   - Testing: Just number of test pairs\n",
        "\n",
        "4. **Get Item (`__getitem__`)**:\n",
        "   - Returns (user_id, item_id, label)\n",
        "   - Training: Returns from combined positives + negatives\n",
        "   - Testing: Returns from test data (already has negatives)\n",
        "\n",
        "**Why negative sampling?**\n",
        "- **Problem**: We only have positive interactions (users liked items)\n",
        "- **Solution**: Generate negative examples (items users haven't interacted with)\n",
        "- **Benefit**: Model learns to distinguish good vs bad recommendations\n",
        "- **Ratio**: Typically 1:4 (1 positive, 4 negatives) for training\n",
        "\n",
        "---\n",
        "\n",
        "#### 4.3 Data Loaders\n",
        "\n",
        "**What is a DataLoader?**\n",
        "- PyTorch utility that batches data from a Dataset\n",
        "- Handles shuffling, parallel loading, and memory management\n",
        "\n",
        "**Training DataLoader:**\n",
        "- **Batch size**: 256 (processes 256 samples at once)\n",
        "- **Shuffle**: True (randomize order each epoch)\n",
        "- **Num workers**: 4 (parallel data loading threads)\n",
        "- **Pin memory**: True if GPU available (faster CPU→GPU transfer)\n",
        "\n",
        "**Test DataLoader:**\n",
        "- **Batch size**: 100 (1 positive + 99 negatives)\n",
        "- **Shuffle**: False (keep test order consistent)\n",
        "- **Num workers**: 0 (simpler, no parallel loading needed)\n",
        "\n",
        "**Why different batch sizes?**\n",
        "- Training: Process many samples efficiently (256)\n",
        "- Testing: Each batch = 1 test case (100 items to rank)\n",
        "\n",
        "**Data Flow:**\n",
        "1. Dataset provides individual samples\n",
        "2. DataLoader groups samples into batches\n",
        "3. Model processes batches during training/testing\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 4 Complete!**\n",
        "\n",
        "We now have:\n",
        "- All data loaded into memory\n",
        "- PyTorch Dataset class for efficient data access\n",
        "- Data loaders ready for training and testing\n",
        "\n",
        "---\n",
        "\n",
        "## Step 5: NCF Model Architecture\n",
        "\n",
        "In this step, we'll implement the Neural Collaborative Filtering (NCF) model. The NCF architecture combines:\n",
        "\n",
        "1. **GMF (Generalized Matrix Factorization)**: Linear interactions via element-wise product\n",
        "2. **MLP (Multi-Layer Perceptron)**: Non-linear interactions via deep neural networks\n",
        "3. **NeuMF**: Combination of both GMF and MLP\n",
        "\n",
        "The model learns user and item embeddings and combines them to predict user-item interaction scores.\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 5: NCF Model Architecture\n",
            "======================================================================\n",
            "✓ NCF model class defined\n",
            "  - Model type: NeuMF-end\n",
            "  - Embedding dimension: 32\n",
            "  - MLP layers: 3\n",
            "  - Dropout rate: 0.0\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: NCF MODEL ARCHITECTURE\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements the Neural Collaborative Filtering (NCF) model.\n",
        "\n",
        "The NCF model has three variants:\n",
        "1. GMF: Only Generalized Matrix Factorization (linear)\n",
        "2. MLP: Only Multi-Layer Perceptron (non-linear)\n",
        "3. NeuMF: Neural Matrix Factorization (combines GMF + MLP)\n",
        "\"\"\"\n",
        "\n",
        "class NCF(nn.Module):\n",
        "    \"\"\"\n",
        "    Neural Collaborative Filtering Model\n",
        "    \n",
        "    This model learns user and item embeddings and combines them using\n",
        "    either GMF (linear) or MLP (non-linear) or both (NeuMF).\n",
        "    \n",
        "    Architecture:\n",
        "    1. Embedding layers: Convert user/item IDs to dense vectors\n",
        "    2. GMF path: Element-wise product of embeddings (linear interaction)\n",
        "    3. MLP path: Deep neural network (non-linear interaction)\n",
        "    4. Prediction layer: Combines GMF and/or MLP outputs to predict score\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, user_num, item_num, factor_num, num_layers,\n",
        "                 dropout, model_name, GMF_model=None, MLP_model=None):\n",
        "        \"\"\"\n",
        "        Initialize the NCF model.\n",
        "        \n",
        "        Parameters:\n",
        "        - user_num: Total number of users\n",
        "        - item_num: Total number of items\n",
        "        - factor_num: Dimension of embedding vectors (e.g., 32)\n",
        "        - num_layers: Number of layers in MLP component\n",
        "        - dropout: Dropout rate for regularization\n",
        "        - model_name: 'MLP', 'GMF', 'NeuMF-end', or 'NeuMF-pre'\n",
        "        - GMF_model: Pre-trained GMF model (for NeuMF-pre)\n",
        "        - MLP_model: Pre-trained MLP model (for NeuMF-pre)\n",
        "        \"\"\"\n",
        "        super(NCF, self).__init__()\n",
        "        \n",
        "        # Store configuration\n",
        "        self.dropout = dropout\n",
        "        self.model_name = model_name\n",
        "        self.GMF_model = GMF_model\n",
        "        self.MLP_model = MLP_model\n",
        "        \n",
        "        # ====================================================================\n",
        "        # EMBEDDING LAYERS\n",
        "        # ====================================================================\n",
        "        # Embeddings convert user/item IDs (integers) to dense vectors\n",
        "        \n",
        "        # GMF embeddings: factor_num dimensions\n",
        "        # Used for Generalized Matrix Factorization (linear interactions)\n",
        "        if model_name != 'MLP':  # MLP doesn't use GMF\n",
        "            self.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
        "            self.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
        "        \n",
        "        # MLP embeddings: Larger dimension for deeper networks\n",
        "        # Dimension = factor_num * 2^(num_layers-1)\n",
        "        # Example: factor_num=32, num_layers=3 → 32 * 2^2 = 128 dimensions\n",
        "        if model_name != 'GMF':  # GMF doesn't use MLP\n",
        "            mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "            self.embed_user_MLP = nn.Embedding(user_num, mlp_embed_dim)\n",
        "            self.embed_item_MLP = nn.Embedding(item_num, mlp_embed_dim)\n",
        "        \n",
        "        # ====================================================================\n",
        "        # MLP LAYERS (Multi-Layer Perceptron)\n",
        "        # ====================================================================\n",
        "        # Build MLP with decreasing dimensions\n",
        "        # Example with factor_num=32, num_layers=3:\n",
        "        #   Input: 128*2 = 256 (concatenated user + item embeddings)\n",
        "        #   Layer 1: 256 → 128\n",
        "        #   Layer 2: 128 → 64\n",
        "        #   Layer 3: 64 → 32\n",
        "        #   Output: 32 dimensions\n",
        "        \n",
        "        if model_name != 'GMF':  # GMF doesn't use MLP\n",
        "            MLP_modules = []\n",
        "            for i in range(num_layers):\n",
        "                # Calculate input size for this layer\n",
        "                input_size = factor_num * (2 ** (num_layers - i))\n",
        "                \n",
        "                # Add dropout for regularization\n",
        "                MLP_modules.append(nn.Dropout(p=self.dropout))\n",
        "                \n",
        "                # Add linear layer (halves the dimension)\n",
        "                MLP_modules.append(nn.Linear(input_size, input_size // 2))\n",
        "                \n",
        "                # Add ReLU activation (non-linearity)\n",
        "                MLP_modules.append(nn.ReLU())\n",
        "            \n",
        "            # Combine all MLP layers into a sequential module\n",
        "            self.MLP_layers = nn.Sequential(*MLP_modules)\n",
        "        \n",
        "        # ====================================================================\n",
        "        # PREDICTION LAYER\n",
        "        # ====================================================================\n",
        "        # Final layer that outputs the interaction score\n",
        "        \n",
        "        if self.model_name in ['MLP', 'GMF']:\n",
        "            # Single path: just factor_num dimensions\n",
        "            predict_size = factor_num\n",
        "        else:\n",
        "            # NeuMF: concatenate GMF (factor_num) + MLP (factor_num) = 2*factor_num\n",
        "            predict_size = factor_num * 2\n",
        "        \n",
        "        self.predict_layer = nn.Linear(predict_size, 1)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self._init_weight_()\n",
        "    \n",
        "    def _init_weight_(self):\n",
        "        \"\"\"\n",
        "        Initialize model weights.\n",
        "        \n",
        "        Different initialization strategies:\n",
        "        - Embeddings: Small random values (std=0.01)\n",
        "        - MLP layers: Xavier uniform (good for ReLU)\n",
        "        - Prediction layer: Kaiming uniform (good for sigmoid)\n",
        "        - Biases: Zero\n",
        "        \"\"\"\n",
        "        if not self.model_name == 'NeuMF-pre':\n",
        "            # Random initialization for training from scratch\n",
        "            \n",
        "            # Embedding initialization: Small random values\n",
        "            # This prevents embeddings from starting too large\n",
        "            if hasattr(self, 'embed_user_GMF'):\n",
        "                nn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
        "            if hasattr(self, 'embed_item_GMF'):\n",
        "                nn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
        "            if hasattr(self, 'embed_user_MLP'):\n",
        "                nn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
        "            if hasattr(self, 'embed_item_MLP'):\n",
        "                nn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
        "            \n",
        "            # MLP layer initialization: Xavier uniform\n",
        "            # Good for layers with ReLU activation\n",
        "            if hasattr(self, 'MLP_layers'):\n",
        "                for m in self.MLP_layers:\n",
        "                    if isinstance(m, nn.Linear):\n",
        "                        nn.init.xavier_uniform_(m.weight)\n",
        "            \n",
        "            # Prediction layer initialization: Kaiming uniform\n",
        "            # Good for layers before sigmoid activation\n",
        "            nn.init.kaiming_uniform_(self.predict_layer.weight, \n",
        "                                    a=1, nonlinearity='sigmoid')\n",
        "            \n",
        "            # Initialize all biases to zero\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "        else:\n",
        "            # Pre-trained initialization (for NeuMF-pre)\n",
        "            # Copy weights from pre-trained GMF and MLP models\n",
        "            \n",
        "            # Copy embedding weights\n",
        "            self.embed_user_GMF.weight.data.copy_(\n",
        "                self.GMF_model.embed_user_GMF.weight)\n",
        "            self.embed_item_GMF.weight.data.copy_(\n",
        "                self.GMF_model.embed_item_GMF.weight)\n",
        "            self.embed_user_MLP.weight.data.copy_(\n",
        "                self.MLP_model.embed_user_MLP.weight)\n",
        "            self.embed_item_MLP.weight.data.copy_(\n",
        "                self.MLP_model.embed_item_MLP.weight)\n",
        "            \n",
        "            # Copy MLP layer weights\n",
        "            for (m1, m2) in zip(self.MLP_layers, self.MLP_model.MLP_layers):\n",
        "                if isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
        "                    m1.weight.data.copy_(m2.weight)\n",
        "                    m1.bias.data.copy_(m2.bias)\n",
        "            \n",
        "            # Combine prediction layer weights from GMF and MLP\n",
        "            predict_weight = torch.cat([\n",
        "                self.GMF_model.predict_layer.weight, \n",
        "                self.MLP_model.predict_layer.weight], dim=1)\n",
        "            predict_bias = (self.GMF_model.predict_layer.bias + \n",
        "                           self.MLP_model.predict_layer.bias) / 2\n",
        "            \n",
        "            self.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
        "            self.predict_layer.bias.data.copy_(predict_bias)\n",
        "    \n",
        "    def forward(self, user, item):\n",
        "        \"\"\"\n",
        "        Forward pass: Predict user-item interaction scores.\n",
        "        \n",
        "        Parameters:\n",
        "        - user: Tensor of user IDs [batch_size]\n",
        "        - item: Tensor of item IDs [batch_size]\n",
        "        \n",
        "        Returns:\n",
        "        - prediction: Tensor of predicted scores [batch_size]\n",
        "        \"\"\"\n",
        "        # ====================================================================\n",
        "        # GMF PATH (Generalized Matrix Factorization)\n",
        "        # ====================================================================\n",
        "        # Linear interaction: element-wise product of embeddings\n",
        "        # Similar to traditional matrix factorization\n",
        "        \n",
        "        if self.model_name != 'MLP':\n",
        "            # Get embeddings\n",
        "            embed_user_GMF = self.embed_user_GMF(user)  # [batch_size, factor_num]\n",
        "            embed_item_GMF = self.embed_item_GMF(item)  # [batch_size, factor_num]\n",
        "            \n",
        "            # Element-wise product (linear interaction)\n",
        "            output_GMF = embed_user_GMF * embed_item_GMF  # [batch_size, factor_num]\n",
        "        \n",
        "        # ====================================================================\n",
        "        # MLP PATH (Multi-Layer Perceptron)\n",
        "        # ====================================================================\n",
        "        # Non-linear interaction: deep neural network\n",
        "        \n",
        "        if self.model_name != 'GMF':\n",
        "            # Get embeddings\n",
        "            embed_user_MLP = self.embed_user_MLP(user)  # [batch_size, mlp_dim]\n",
        "            embed_item_MLP = self.embed_item_MLP(item)   # [batch_size, mlp_dim]\n",
        "            \n",
        "            # Concatenate user and item embeddings\n",
        "            interaction = torch.cat((embed_user_MLP, embed_item_MLP), -1)  # [batch_size, mlp_dim*2]\n",
        "            \n",
        "            # Pass through MLP layers (with dropout and ReLU)\n",
        "            output_MLP = self.MLP_layers(interaction)  # [batch_size, factor_num]\n",
        "        \n",
        "        # ====================================================================\n",
        "        # COMBINE PATHS\n",
        "        # ====================================================================\n",
        "        if self.model_name == 'GMF':\n",
        "            # Only GMF path\n",
        "            concat = output_GMF\n",
        "        elif self.model_name == 'MLP':\n",
        "            # Only MLP path\n",
        "            concat = output_MLP\n",
        "        else:\n",
        "            # NeuMF: Concatenate both paths\n",
        "            concat = torch.cat((output_GMF, output_MLP), -1)  # [batch_size, factor_num*2]\n",
        "        \n",
        "        # ====================================================================\n",
        "        # PREDICTION\n",
        "        # ====================================================================\n",
        "        # Final linear layer outputs interaction score\n",
        "        prediction = self.predict_layer(concat)  # [batch_size, 1]\n",
        "        \n",
        "        # Flatten to [batch_size]\n",
        "        return prediction.view(-1)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 5: NCF Model Architecture\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NCF model class defined\")\n",
        "print(f\"  - Model type: {model_name}\")\n",
        "print(f\"  - Embedding dimension: {factor_num}\")\n",
        "print(f\"  - MLP layers: {num_layers}\")\n",
        "print(f\"  - Dropout rate: {dropout_rate}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5.2: Creating and Initializing Model\n",
            "======================================================================\n",
            "✓ Model on CPU\n",
            "\n",
            "✓ Model created successfully!\n",
            "  - Total parameters: 1,574,657\n",
            "  - Trainable parameters: 1,574,657\n",
            "\n",
            "Model Architecture:\n",
            "  - Users: 6,038\n",
            "  - Items: 3,533\n",
            "  - GMF embeddings: 32 dimensions\n",
            "  - MLP embeddings: 128 dimensions\n",
            "  - MLP layers: 3 (with dropout=0.0)\n",
            "  - Prediction layer: 64 → 1\n",
            "\n",
            "======================================================================\n",
            "STEP 5.3: Detailed Model Architecture\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "COMPLETE MODEL STRUCTURE:\n",
            "======================================================================\n",
            "NCF(\n",
            "  (embed_user_GMF): Embedding(6038, 32)\n",
            "  (embed_item_GMF): Embedding(3533, 32)\n",
            "  (embed_user_MLP): Embedding(6038, 128)\n",
            "  (embed_item_MLP): Embedding(3533, 128)\n",
            "  (MLP_layers): Sequential(\n",
            "    (0): Dropout(p=0.0, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.0, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.0, inplace=False)\n",
            "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (8): ReLU()\n",
            "  )\n",
            "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "LAYER-BY-LAYER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "[GMF Path - Generalized Matrix Factorization]\n",
            "  embed_user_GMF: Embedding(6038, 32)\n",
            "    → Converts user IDs to 32-dimensional vectors\n",
            "  embed_item_GMF: Embedding(3533, 32)\n",
            "    → Converts item IDs to 32-dimensional vectors\n",
            "  Element-wise product: user_emb * item_emb\n",
            "    → Output shape: [batch_size, 32]\n",
            "\n",
            "[MLP Path - Multi-Layer Perceptron]\n",
            "  embed_user_MLP: Embedding(6038, 128)\n",
            "    → Converts user IDs to 128-dimensional vectors\n",
            "  embed_item_MLP: Embedding(3533, 128)\n",
            "    → Converts item IDs to 128-dimensional vectors\n",
            "  Concatenation: [user_emb, item_emb]\n",
            "    → Output shape: [batch_size, 256]\n",
            "\n",
            "  MLP Layers (3 layers):\n",
            "    Layer 1:\n",
            "      Dropout(p=0.0)\n",
            "      Linear(256, 128)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 128]\n",
            "    Layer 2:\n",
            "      Dropout(p=0.0)\n",
            "      Linear(128, 64)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 64]\n",
            "    Layer 3:\n",
            "      Dropout(p=0.0)\n",
            "      Linear(64, 32)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 32]\n",
            "\n",
            "[Prediction Layer]\n",
            "  Input: Concatenated GMF + MLP [64 dimensions]\n",
            "    → GMF: [32 dims] + MLP: [32 dims]\n",
            "  Linear(64, 1)\n",
            "    → Output: [batch_size, 1] (interaction score)\n",
            "    → Higher score = more likely user will like item\n",
            "\n",
            "======================================================================\n",
            "PARAMETER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "GMF Embeddings:\n",
            "  User embeddings: 6,038 × 32 = 193,216 parameters\n",
            "  Item embeddings: 3,533 × 32 = 113,056 parameters\n",
            "  GMF Total: 306,272 parameters\n",
            "\n",
            "MLP Embeddings:\n",
            "  User embeddings: 6,038 × 128 = 772,864 parameters\n",
            "  Item embeddings: 3,533 × 128 = 452,224 parameters\n",
            "  MLP Embeddings Total: 1,225,088 parameters\n",
            "\n",
            "MLP Layers:\n",
            "  Layer 1 (Linear(256, 128)): 32,896 parameters\n",
            "  Layer 2 (Linear(128, 64)): 8,256 parameters\n",
            "  Layer 3 (Linear(64, 32)): 2,080 parameters\n",
            "  MLP Layers Total: 43,232 parameters\n",
            "\n",
            "Prediction Layer:\n",
            "  Linear(64, 1): 65 parameters\n",
            "\n",
            "======================================================================\n",
            "TOTAL MODEL PARAMETERS: 1,574,657\n",
            "Model Size (float32): ~6.01 MB\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "ARCHITECTURE DIAGRAM:\n",
            "======================================================================\n",
            "\n",
            "    User ID ──┐\n",
            "              ├─→ Embedding (GMF) ──→ [32] ─┐\n",
            "              │                                ├─→ Element-wise Product ─→ [32] ─┐\n",
            "    Item ID ──┤                                │                                  │\n",
            "              ├─→ Embedding (GMF) ──→ [32] ─┘                                  │\n",
            "              │                                                                  │\n",
            "              ├─→ Embedding (MLP) ──→ [128] ─┐                                 │\n",
            "              │                                 ├─→ Concatenate ─→ [256]         │\n",
            "              └─→ Embedding (MLP) ──→ [128] ─┘                                 │\n",
            "                                                                                 │\n",
            "                                                                                 │\n",
            "    [256] ─→ Dropout ─→ Linear(256→128) ─→ ReLU ─→ [128]                       │\n",
            "    [128] ─→ Dropout ─→ Linear(128→64)  ─→ ReLU ─→ [64]                        │\n",
            "    [64]  ─→ Dropout ─→ Linear(64→32)   ─→ ReLU ─→ [32] ────────────────────────┤\n",
            "                                                                                 │\n",
            "                                                                                 ├─→ Concatenate ─→ [64] ─→ Linear(64→1) ─→ Score\n",
            "                                                                                 │\n",
            "    \n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 CREATE AND INITIALIZE THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5.2: Creating and Initializing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we need pre-trained models (for NeuMF-pre)\n",
        "if model_name == 'NeuMF-pre':\n",
        "    # For NeuMF-pre, we would load pre-trained GMF and MLP models\n",
        "    # For now, we'll use NeuMF-end (training from scratch)\n",
        "    print(\"⚠ NeuMF-pre requires pre-trained models.\")\n",
        "    print(\"  Switching to NeuMF-end (training from scratch)...\")\n",
        "    model_name = 'NeuMF-end'\n",
        "\n",
        "# Create the model\n",
        "ncf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name=model_name,\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    ncf_model = ncf_model.cuda()\n",
        "    print(\"✓ Model moved to GPU\")\n",
        "else:\n",
        "    print(\"✓ Model on CPU\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model created successfully!\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - Users: {user_num:,}\")\n",
        "print(f\"  - Items: {item_num:,}\")\n",
        "if model_name != 'MLP':\n",
        "    print(f\"  - GMF embeddings: {factor_num} dimensions\")\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(f\"  - MLP embeddings: {mlp_embed_dim} dimensions\")\n",
        "    print(f\"  - MLP layers: {num_layers} (with dropout={dropout_rate})\")\n",
        "print(f\"  - Prediction layer: {factor_num if model_name in ['MLP', 'GMF'] else factor_num * 2} → 1\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5.3 DETAILED MODEL ARCHITECTURE VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5.3: Detailed Model Architecture\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print the full model structure\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPLETE MODEL STRUCTURE:\")\n",
        "print(\"=\" * 70)\n",
        "print(ncf_model)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print detailed layer information\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LAYER-BY-LAYER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_name != 'MLP':\n",
        "    print(\"\\n[GMF Path - Generalized Matrix Factorization]\")\n",
        "    print(f\"  embed_user_GMF: Embedding({user_num}, {factor_num})\")\n",
        "    print(f\"    → Converts user IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  embed_item_GMF: Embedding({item_num}, {factor_num})\")\n",
        "    print(f\"    → Converts item IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  Element-wise product: user_emb * item_emb\")\n",
        "    print(f\"    → Output shape: [batch_size, {factor_num}]\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(\"\\n[MLP Path - Multi-Layer Perceptron]\")\n",
        "    print(f\"  embed_user_MLP: Embedding({user_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts user IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  embed_item_MLP: Embedding({item_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts item IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  Concatenation: [user_emb, item_emb]\")\n",
        "    print(f\"    → Output shape: [batch_size, {mlp_embed_dim * 2}]\")\n",
        "    \n",
        "    print(f\"\\n  MLP Layers ({num_layers} layers):\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        print(f\"    Layer {i+1}:\")\n",
        "        print(f\"      Dropout(p={dropout_rate})\")\n",
        "        print(f\"      Linear({input_size}, {output_size})\")\n",
        "        print(f\"      ReLU()\")\n",
        "        print(f\"      → Output shape: [batch_size, {output_size}]\")\n",
        "\n",
        "print(\"\\n[Prediction Layer]\")\n",
        "if model_name == 'GMF':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: GMF output [{factor_num} dimensions]\")\n",
        "elif model_name == 'MLP':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: MLP output [{factor_num} dimensions]\")\n",
        "else:  # NeuMF\n",
        "    predict_input = factor_num * 2\n",
        "    print(f\"  Input: Concatenated GMF + MLP [{factor_num * 2} dimensions]\")\n",
        "    print(f\"    → GMF: [{factor_num} dims] + MLP: [{factor_num} dims]\")\n",
        "\n",
        "print(f\"  Linear({predict_input}, 1)\")\n",
        "print(f\"    → Output: [batch_size, 1] (interaction score)\")\n",
        "print(f\"    → Higher score = more likely user will like item\")\n",
        "\n",
        "# Calculate and print parameter breakdown\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARAMETER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_params = 0\n",
        "if model_name != 'MLP':\n",
        "    gmf_user_params = user_num * factor_num\n",
        "    gmf_item_params = item_num * factor_num\n",
        "    gmf_total = gmf_user_params + gmf_item_params\n",
        "    total_params += gmf_total\n",
        "    print(f\"\\nGMF Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {factor_num} = {gmf_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {factor_num} = {gmf_item_params:,} parameters\")\n",
        "    print(f\"  GMF Total: {gmf_total:,} parameters\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    mlp_user_params = user_num * mlp_embed_dim\n",
        "    mlp_item_params = item_num * mlp_embed_dim\n",
        "    mlp_embed_total = mlp_user_params + mlp_item_params\n",
        "    total_params += mlp_embed_total\n",
        "    print(f\"\\nMLP Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {mlp_embed_dim} = {mlp_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {mlp_embed_dim} = {mlp_item_params:,} parameters\")\n",
        "    print(f\"  MLP Embeddings Total: {mlp_embed_total:,} parameters\")\n",
        "    \n",
        "    # MLP layers parameters\n",
        "    mlp_layer_params = 0\n",
        "    print(f\"\\nMLP Layers:\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        layer_params = (input_size * output_size) + output_size  # weights + bias\n",
        "        mlp_layer_params += layer_params\n",
        "        print(f\"  Layer {i+1} (Linear({input_size}, {output_size})): {layer_params:,} parameters\")\n",
        "    total_params += mlp_layer_params\n",
        "    print(f\"  MLP Layers Total: {mlp_layer_params:,} parameters\")\n",
        "\n",
        "# Prediction layer\n",
        "if model_name in ['MLP', 'GMF']:\n",
        "    predict_input = factor_num\n",
        "else:\n",
        "    predict_input = factor_num * 2\n",
        "predict_params = (predict_input * 1) + 1  # weights + bias\n",
        "total_params += predict_params\n",
        "print(f\"\\nPrediction Layer:\")\n",
        "print(f\"  Linear({predict_input}, 1): {predict_params:,} parameters\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"TOTAL MODEL PARAMETERS: {total_params:,}\")\n",
        "print(f\"Model Size (float32): ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print architecture diagram\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"ARCHITECTURE DIAGRAM:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_name == 'NeuMF-end' or model_name == 'NeuMF-pre':\n",
        "    print(\"\"\"\n",
        "    User ID ──┐\n",
        "              ├─→ Embedding (GMF) ──→ [32] ─┐\n",
        "              │                                ├─→ Element-wise Product ─→ [32] ─┐\n",
        "    Item ID ──┤                                │                                  │\n",
        "              ├─→ Embedding (GMF) ──→ [32] ─┘                                  │\n",
        "              │                                                                  │\n",
        "              ├─→ Embedding (MLP) ──→ [128] ─┐                                 │\n",
        "              │                                 ├─→ Concatenate ─→ [256]         │\n",
        "              └─→ Embedding (MLP) ──→ [128] ─┘                                 │\n",
        "                                                                                 │\n",
        "                                                                                 │\n",
        "    [256] ─→ Dropout ─→ Linear(256→128) ─→ ReLU ─→ [128]                       │\n",
        "    [128] ─→ Dropout ─→ Linear(128→64)  ─→ ReLU ─→ [64]                        │\n",
        "    [64]  ─→ Dropout ─→ Linear(64→32)   ─→ ReLU ─→ [32] ────────────────────────┤\n",
        "                                                                                 │\n",
        "                                                                                 ├─→ Concatenate ─→ [64] ─→ Linear(64→1) ─→ Score\n",
        "                                                                                 │\n",
        "    \"\"\")\n",
        "elif model_name == 'GMF':\n",
        "    print(\"\"\"\n",
        "    User ID ──→ Embedding ──→ [32] ─┐\n",
        "                                     ├─→ Element-wise Product ─→ [32] ─→ Linear(32→1) ─→ Score\n",
        "    Item ID ──→ Embedding ──→ [32] ─┘\n",
        "    \"\"\")\n",
        "else:  # MLP\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(f\"\"\"\n",
        "    User ID ──→ Embedding ──→ [{mlp_embed_dim}] ─┐\n",
        "                                                 ├─→ Concatenate ─→ [{mlp_embed_dim * 2}]\n",
        "    Item ID ──→ Embedding ──→ [{mlp_embed_dim}] ─┘\n",
        "    \n",
        "    [{mlp_embed_dim * 2}] ─→ Dropout ─→ Linear ─→ ReLU ─→ ... (MLP layers) ... ─→ [32] ─→ Linear(32→1) ─→ Score\n",
        "    \"\"\")\n",
        "\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 5:\n",
        "\n",
        "#### 5.1 Model Architecture Overview\n",
        "\n",
        "**What is NCF?**\n",
        "- Neural Collaborative Filtering combines traditional matrix factorization with deep learning\n",
        "- Learns user and item embeddings (dense vector representations)\n",
        "- Uses neural networks to model complex user-item interactions\n",
        "\n",
        "**Three Model Variants:**\n",
        "\n",
        "1. **GMF (Generalized Matrix Factorization)**:\n",
        "   - **Path**: Embedding → Element-wise product → Prediction\n",
        "   - **Interaction**: Linear (element-wise multiplication)\n",
        "   - **Similar to**: Traditional matrix factorization\n",
        "   - **Use case**: Simple, fast, good baseline\n",
        "\n",
        "2. **MLP (Multi-Layer Perceptron)**:\n",
        "   - **Path**: Embedding → Concatenate → MLP layers → Prediction\n",
        "   - **Interaction**: Non-linear (deep neural network)\n",
        "   - **Advantage**: Can learn complex patterns\n",
        "   - **Use case**: When you need non-linear interactions\n",
        "\n",
        "3. **NeuMF (Neural Matrix Factorization)**:\n",
        "   - **Path**: Both GMF and MLP paths → Concatenate → Prediction\n",
        "   - **Interaction**: Both linear and non-linear\n",
        "   - **Advantage**: Best of both worlds\n",
        "   - **Use case**: Best performance (recommended)\n",
        "\n",
        "---\n",
        "\n",
        "#### 5.2 Embedding Layers - Deep Dive\n",
        "\n",
        "**What are Embeddings?**\n",
        "- Convert discrete IDs (user/item IDs) to continuous vectors\n",
        "- Each user/item gets a dense vector representation\n",
        "- These vectors are learned during training\n",
        "\n",
        "**GMF Embeddings:**\n",
        "- Dimension: `factor_num` (e.g., 32)\n",
        "- Purpose: Linear interactions via element-wise product\n",
        "- Example: User embedding [0.1, 0.5, -0.3, ...] × Item embedding [0.2, 0.1, 0.4, ...]\n",
        "\n",
        "**MLP Embeddings:**\n",
        "- Dimension: `factor_num * 2^(num_layers-1)` (e.g., 32 * 2^2 = 128 for 3 layers)\n",
        "- Purpose: Non-linear interactions via deep network\n",
        "- Larger dimension allows more complex patterns\n",
        "\n",
        "**Why Different Embedding Dimensions?**\n",
        "- GMF: Simple element-wise product, smaller dimension is sufficient\n",
        "- MLP: Needs larger dimension to learn complex non-linear patterns\n",
        "- NeuMF: Uses both, so needs separate embeddings for each path\n",
        "\n",
        "---\n",
        "\n",
        "#### 5.3 MLP Layers Architecture\n",
        "\n",
        "**Layer Structure:**\n",
        "- Each layer: Dropout → Linear → ReLU\n",
        "- Dimensions decrease by half each layer\n",
        "- Example with factor_num=32, num_layers=3:\n",
        "  ```\n",
        "  Input: 256 (128 user + 128 item embeddings concatenated)\n",
        "  Layer 1: 256 → 128 (with dropout and ReLU)\n",
        "  Layer 2: 128 → 64 (with dropout and ReLU)\n",
        "  Layer 3: 64 → 32 (with dropout and ReLU)\n",
        "  Output: 32 dimensions\n",
        "  ```\n",
        "\n",
        "**Why Decreasing Dimensions?**\n",
        "- Compresses information progressively\n",
        "- Forces model to learn important features\n",
        "- Reduces overfitting\n",
        "\n",
        "**Dropout:**\n",
        "- Randomly sets some neurons to zero during training\n",
        "- Prevents overfitting\n",
        "- Only active during training (not during evaluation)\n",
        "\n",
        "**ReLU Activation:**\n",
        "- Introduces non-linearity\n",
        "- Allows model to learn complex patterns\n",
        "- Formula: ReLU(x) = max(0, x)\n",
        "\n",
        "---\n",
        "\n",
        "#### 5.4 Forward Pass - Step by Step\n",
        "\n",
        "**For NeuMF (combines GMF + MLP):**\n",
        "\n",
        "1. **Input**: User IDs and Item IDs (batch of integers)\n",
        "   ```\n",
        "   user = [1, 5, 10, ...]  # batch_size user IDs\n",
        "   item = [3, 7, 2, ...]   # batch_size item IDs\n",
        "   ```\n",
        "\n",
        "2. **GMF Path**:\n",
        "   ```\n",
        "   embed_user_GMF = Embedding(user)  # [batch_size, 32]\n",
        "   embed_item_GMF = Embedding(item)   # [batch_size, 32]\n",
        "   output_GMF = embed_user_GMF * embed_item_GMF  # Element-wise product\n",
        "   ```\n",
        "\n",
        "3. **MLP Path**:\n",
        "   ```\n",
        "   embed_user_MLP = Embedding(user)  # [batch_size, 128]\n",
        "   embed_item_MLP = Embedding(item)   # [batch_size, 128]\n",
        "   interaction = concat([embed_user_MLP, embed_item_MLP])  # [batch_size, 256]\n",
        "   output_MLP = MLP_layers(interaction)  # [batch_size, 32]\n",
        "   ```\n",
        "\n",
        "4. **Combine**:\n",
        "   ```\n",
        "   concat = concat([output_GMF, output_MLP])  # [batch_size, 64]\n",
        "   ```\n",
        "\n",
        "5. **Predict**:\n",
        "   ```\n",
        "   prediction = Linear(concat)  # [batch_size, 1]\n",
        "   ```\n",
        "\n",
        "6. **Output**: Interaction scores (higher = more likely user will like item)\n",
        "\n",
        "---\n",
        "\n",
        "#### 5.5 Weight Initialization\n",
        "\n",
        "**Why Initialize Weights?**\n",
        "- Random initialization helps training start properly\n",
        "- Prevents vanishing/exploding gradients\n",
        "- Different strategies for different layer types\n",
        "\n",
        "**Initialization Strategies:**\n",
        "\n",
        "1. **Embeddings**: Normal distribution (std=0.01)\n",
        "   - Small values prevent large initial gradients\n",
        "   - Allows gradual learning\n",
        "\n",
        "2. **MLP Layers**: Xavier uniform\n",
        "   - Good for ReLU activations\n",
        "   - Maintains variance across layers\n",
        "\n",
        "3. **Prediction Layer**: Kaiming uniform\n",
        "   - Good before sigmoid activation\n",
        "   - Prevents saturation\n",
        "\n",
        "4. **Biases**: Zero\n",
        "   - Common practice\n",
        "   - Model learns bias values during training\n",
        "\n",
        "**Pre-trained Initialization (NeuMF-pre):**\n",
        "- Copies weights from separately trained GMF and MLP models\n",
        "- Combines their prediction layers\n",
        "- Usually gives best performance but requires more training time\n",
        "\n",
        "---\n",
        "\n",
        "#### 5.6 Model Parameters\n",
        "\n",
        "**Parameter Count Example:**\n",
        "- Users: 6,000, Items: 4,000, factor_num=32, num_layers=3\n",
        "- GMF embeddings: 6,000×32 + 4,000×32 = 320,000\n",
        "- MLP embeddings: 6,000×128 + 4,000×128 = 1,280,000\n",
        "- MLP layers: ~50,000\n",
        "- Prediction: 64×1 = 64\n",
        "- **Total**: ~1.65 million parameters\n",
        "\n",
        "**Memory Usage:**\n",
        "- Each parameter: 4 bytes (float32)\n",
        "- Model size: ~6.6 MB\n",
        "- Very efficient for recommendation systems!\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 5 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Complete NCF model architecture\n",
        "- Model initialized and ready for training\n",
        "- Understanding of how embeddings and layers work\n",
        "\n",
        "---\n",
        "\n",
        "## Step 6: Evaluation Metrics\n",
        "\n",
        "In this step, we'll implement evaluation metrics to measure how well our model performs:\n",
        "1. **Hit Rate (HR@K)**: Percentage of test cases where the true item is in top-K recommendations\n",
        "2. **NDCG (Normalized Discounted Cumulative Gain@K)**: Measures ranking quality, giving more weight to items ranked higher\n",
        "\n",
        "These metrics are standard in recommendation systems and help us understand model performance.\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.1: Hit Rate Metric\n",
            "======================================================================\n",
            "✓ Hit Rate function defined\n",
            "  - Returns 1 if true item is in top-K recommendations\n",
            "  - Returns 0 otherwise\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements evaluation metrics for recommendation systems:\n",
        "- Hit Rate (HR@K): Binary metric - is the true item in top K?\n",
        "- NDCG (Normalized Discounted Cumulative Gain@K): Ranking quality metric\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 6.1 HIT RATE METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def hit(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Hit Rate for a single test case.\n",
        "    \n",
        "    Hit Rate is 1 if the ground truth item is in the predicted top-K items,\n",
        "    otherwise 0.\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - 1 if gt_item is in pred_items, 0 otherwise\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.1: Hit Rate Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Hit Rate function defined\")\n",
        "print(\"  - Returns 1 if true item is in top-K recommendations\")\n",
        "print(\"  - Returns 0 otherwise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.2: NDCG Metric\n",
            "======================================================================\n",
            "✓ NDCG function defined\n",
            "  - Measures ranking quality\n",
            "  - Higher score for items ranked higher\n",
            "  - Returns 0 if true item not in recommendations\n",
            "\n",
            "NDCG Examples:\n",
            "  Top-5 recommendations: [10, 20, 30, 40, 50]\n",
            "  If true item is at position 0: NDCG = 1.000\n",
            "  If true item is at position 2: NDCG = 0.500\n",
            "  If true item is at position 4: NDCG = 0.387\n",
            "  If true item not in list: NDCG = 0.000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.2 NDCG METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single test case.\n",
        "    \n",
        "    NDCG measures ranking quality by:\n",
        "    1. Giving more weight to items ranked higher (position matters)\n",
        "    2. Using logarithmic discounting (relevance decreases with position)\n",
        "    \n",
        "    Formula: NDCG = 1 / log2(position + 2)\n",
        "    - Position 0 (top): 1 / log2(2) = 1.0\n",
        "    - Position 1: 1 / log2(3) ≈ 0.63\n",
        "    - Position 2: 1 / log2(4) = 0.5\n",
        "    - Position 9: 1 / log2(11) ≈ 0.29\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - NDCG score (0.0 to 1.0) if gt_item is in pred_items\n",
        "    - 0.0 if gt_item is not in pred_items\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        # Find the position (index) of the ground truth item\n",
        "        index = pred_items.index(gt_item)\n",
        "        # Calculate NDCG: 1 / log2(position + 2)\n",
        "        # +2 because: position 0 should give 1/log2(2) = 1.0\n",
        "        return np.reciprocal(np.log2(index + 2))\n",
        "    return 0.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.2: NDCG Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NDCG function defined\")\n",
        "print(\"  - Measures ranking quality\")\n",
        "print(\"  - Higher score for items ranked higher\")\n",
        "print(\"  - Returns 0 if true item not in recommendations\")\n",
        "\n",
        "# Example to demonstrate NDCG\n",
        "print(\"\\nNDCG Examples:\")\n",
        "example_items = [10, 20, 30, 40, 50]\n",
        "print(f\"  Top-5 recommendations: {example_items}\")\n",
        "print(f\"  If true item is at position 0: NDCG = {ndcg(10, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 2: NDCG = {ndcg(30, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 4: NDCG = {ndcg(50, example_items):.3f}\")\n",
        "print(f\"  If true item not in list: NDCG = {ndcg(99, example_items):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.3: Evaluation Function\n",
            "======================================================================\n",
            "✓ Evaluation function defined\n",
            "  - Evaluates model on test data\n",
            "  - Calculates average Hit Rate and NDCG\n",
            "  - Works with GPU or CPU\n",
            "\n",
            "✓ Evaluation ready (device: cpu)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.3 EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_metrics(model, test_loader, top_k, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "    \n",
        "    This function:\n",
        "    1. For each test case (1 positive + 99 negatives):\n",
        "       - Gets model predictions for all 100 items\n",
        "       - Selects top-K items with highest scores\n",
        "       - Checks if the true item is in top-K (Hit Rate)\n",
        "       - Calculates NDCG based on true item's position\n",
        "    2. Averages metrics across all test cases\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - test_loader: DataLoader with test data\n",
        "    - top_k: Number of top items to consider (e.g., 10)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - mean_HR: Average Hit Rate across all test cases\n",
        "    - mean_NDCG: Average NDCG across all test cases\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout)\n",
        "    \n",
        "    HR_list = []  # List to store Hit Rate for each test case\n",
        "    NDCG_list = []  # List to store NDCG for each test case\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation (faster, saves memory)\n",
        "        for user, item, label in test_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                user = user.cuda()\n",
        "                item = item.cuda()\n",
        "            else:\n",
        "                device = 'cpu'\n",
        "            \n",
        "            # Get model predictions for all items in this batch\n",
        "            # Batch size = test_num_ng + 1 = 100 (1 positive + 99 negatives)\n",
        "            predictions = model(user, item)  # [100] tensor of scores\n",
        "            \n",
        "            # Get top-K items with highest prediction scores\n",
        "            # torch.topk returns (values, indices)\n",
        "            _, indices = torch.topk(predictions, top_k)\n",
        "            \n",
        "            # Get the actual item IDs for top-K recommendations\n",
        "            # torch.take extracts items at given indices\n",
        "            recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "            \n",
        "            # The first item in the batch is always the positive (true) item\n",
        "            gt_item = item[0].item()  # Ground truth item ID\n",
        "            \n",
        "            # Calculate metrics for this test case\n",
        "            HR_list.append(hit(gt_item, recommends))\n",
        "            NDCG_list.append(ndcg(gt_item, recommends))\n",
        "    \n",
        "    # Calculate average metrics across all test cases\n",
        "    mean_HR = np.mean(HR_list)\n",
        "    mean_NDCG = np.mean(NDCG_list)\n",
        "    \n",
        "    return mean_HR, mean_NDCG\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.3: Evaluation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Evaluation function defined\")\n",
        "print(\"  - Evaluates model on test data\")\n",
        "print(\"  - Calculates average Hit Rate and NDCG\")\n",
        "print(\"  - Works with GPU or CPU\")\n",
        "\n",
        "# Determine device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n✓ Evaluation ready (device: {device})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 6:\n",
        "\n",
        "#### 6.1 Hit Rate (HR@K) - Binary Metric\n",
        "\n",
        "**What is Hit Rate?**\n",
        "- Measures whether the true item appears in the top-K recommendations\n",
        "- Binary metric: 1 if found, 0 if not found\n",
        "- Simple and intuitive: \"Did we recommend the right item?\"\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "Hit Rate = 1 (item is in top-10)\n",
        "```\n",
        "\n",
        "**Why Hit Rate?**\n",
        "- Users typically only see top-K recommendations\n",
        "- If true item is in top-K, recommendation is successful\n",
        "- Easy to interpret: \"X% of test cases had correct item in top-K\"\n",
        "\n",
        "**Limitations:**\n",
        "- Doesn't consider position (item at position 1 vs position 10 both get 1)\n",
        "- Binary: doesn't measure how good the ranking is\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.2 NDCG (Normalized Discounted Cumulative Gain) - Ranking Metric\n",
        "\n",
        "**What is NDCG?**\n",
        "- Measures ranking quality, not just presence\n",
        "- Gives more weight to items ranked higher\n",
        "- Uses logarithmic discounting (relevance decreases with position)\n",
        "\n",
        "**NDCG Formula:**\n",
        "```\n",
        "NDCG = 1 / log2(position + 2)\n",
        "```\n",
        "\n",
        "**Position vs NDCG Score:**\n",
        "| Position | NDCG Score | Meaning |\n",
        "|----------|-----------|---------|\n",
        "| 0 (top)  | 1.000     | Perfect! Item ranked #1 |\n",
        "| 1        | 0.631     | Item ranked #2 |\n",
        "| 2        | 0.500     | Item ranked #3 |\n",
        "| 3        | 0.431     | Item ranked #4 |\n",
        "| 4        | 0.387     | Item ranked #5 |\n",
        "| 9        | 0.289     | Item ranked #10 |\n",
        "\n",
        "**Why Logarithmic Discounting?**\n",
        "- Position 0 → 1: Big difference (user sees it first)\n",
        "- Position 9 → 10: Small difference (both far down)\n",
        "- Logarithmic function captures this diminishing importance\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "NDCG = 1 / log2(2 + 2) = 1 / log2(4) = 1 / 2 = 0.5\n",
        "```\n",
        "\n",
        "**Why NDCG?**\n",
        "- Considers position: Better ranking = Higher score\n",
        "- More informative than Hit Rate\n",
        "- Standard metric in information retrieval and recommendation systems\n",
        "\n",
        "**Limitations:**\n",
        "- More complex than Hit Rate\n",
        "- Requires understanding of logarithmic discounting\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.3 Evaluation Process\n",
        "\n",
        "**How Evaluation Works:**\n",
        "\n",
        "1. **For each test case** (1 positive + 99 negatives):\n",
        "   ```\n",
        "   User: 123\n",
        "   Items: [42 (positive), 1, 5, 7, 9, ... (99 negatives)]\n",
        "   ```\n",
        "\n",
        "2. **Get predictions**:\n",
        "   ```\n",
        "   Model scores: [0.8, 0.3, 0.2, 0.1, 0.05, ...]\n",
        "   Item 42 gets score 0.8 (highest!)\n",
        "   ```\n",
        "\n",
        "3. **Select top-K** (e.g., K=10):\n",
        "   ```\n",
        "   Top-10 items: [42, 1, 5, 7, 9, 12, 15, 18, 20, 23]\n",
        "   ```\n",
        "\n",
        "4. **Calculate metrics**:\n",
        "   - Hit Rate: Is 42 in top-10? Yes → HR = 1\n",
        "   - NDCG: Position of 42? Position 0 → NDCG = 1.0\n",
        "\n",
        "5. **Average across all test cases**:\n",
        "   ```\n",
        "   Mean HR = (1 + 0 + 1 + 1 + ...) / N\n",
        "   Mean NDCG = (1.0 + 0.0 + 0.5 + 0.63 + ...) / N\n",
        "   ```\n",
        "\n",
        "**Test Data Structure:**\n",
        "- Each batch: 100 items (1 positive + 99 negatives)\n",
        "- Model should rank the positive item higher than negatives\n",
        "- We measure if positive is in top-K\n",
        "\n",
        "**Why 99 Negatives?**\n",
        "- Simulates real-world scenario: recommend 1 from 100 candidates\n",
        "- Standard evaluation protocol (used in research papers)\n",
        "- Makes evaluation realistic and challenging\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.4 Understanding the Results\n",
        "\n",
        "**Good Performance:**\n",
        "- HR@10 > 0.6: 60% of test cases have true item in top-10\n",
        "- NDCG@10 > 0.4: Good ranking quality on average\n",
        "\n",
        "**Excellent Performance:**\n",
        "- HR@10 > 0.7: 70% success rate\n",
        "- NDCG@10 > 0.5: Very good ranking quality\n",
        "\n",
        "**What to Expect:**\n",
        "- Random baseline: HR@10 ≈ 0.1 (10% chance)\n",
        "- Good model: HR@10 ≈ 0.6-0.7\n",
        "- State-of-the-art: HR@10 > 0.7\n",
        "\n",
        "**Interpreting Results:**\n",
        "- **HR higher than NDCG**: Model finds items but doesn't rank them well\n",
        "- **NDCG close to HR**: Model ranks items well (good positions)\n",
        "- **Both low**: Model needs more training or better architecture\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 6 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Hit Rate metric for binary evaluation\n",
        "- NDCG metric for ranking quality\n",
        "- Complete evaluation function ready to use\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: Training Loop\n",
        "\n",
        "In this step, we'll implement the complete training process:\n",
        "1. **Loss Function**: Binary Cross-Entropy with Logits (for binary classification)\n",
        "2. **Optimizer**: Adam optimizer (adaptive learning rate)\n",
        "3. **Training Loop**: Iterate through epochs, train on batches, evaluate periodically\n",
        "4. **Model Saving**: Save the best model based on validation performance\n",
        "\n",
        "This is where the model learns to make good recommendations!\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.1: Setting Up Loss Function and Optimizer\n",
            "======================================================================\n",
            "✓ Loss function: BCEWithLogitsLoss\n",
            "  - For binary classification (like/dislike)\n",
            "  - Combines sigmoid + cross-entropy for stability\n",
            "\n",
            "✓ Optimizer: Adam\n",
            "  - Learning rate: 0.001\n",
            "  - Adaptive: adjusts learning rate automatically\n",
            "\n",
            "✓ Model ready for training\n",
            "  - Trainable parameters: 1,574,657\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements the complete training process for the NCF model.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 7.1 SETUP LOSS FUNCTION AND OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.1: Setting Up Loss Function and Optimizer\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy with Logits\n",
        "# This combines sigmoid activation + binary cross-entropy loss\n",
        "# More numerically stable than applying sigmoid separately\n",
        "# \n",
        "# Why BCEWithLogitsLoss?\n",
        "# - Our task: Predict if user will like item (binary: 1 or 0)\n",
        "# - Model outputs raw scores (logits), not probabilities\n",
        "# - BCEWithLogitsLoss applies sigmoid internally and computes loss\n",
        "# - More stable than: sigmoid(output) then BCE(sigmoid_output, label)\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "print(\"✓ Loss function: BCEWithLogitsLoss\")\n",
        "print(\"  - For binary classification (like/dislike)\")\n",
        "print(\"  - Combines sigmoid + cross-entropy for stability\")\n",
        "\n",
        "# Optimizer: Adam (Adaptive Moment Estimation)\n",
        "# Adam is an adaptive learning rate optimizer that:\n",
        "# - Adjusts learning rate per parameter\n",
        "# - Uses momentum (moving average of gradients)\n",
        "# - Works well for most deep learning tasks\n",
        "# - Better than SGD for this problem\n",
        "\n",
        "optimizer = optim.Adam(ncf_model.parameters(), lr=learning_rate)\n",
        "print(f\"\\n✓ Optimizer: Adam\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Adaptive: adjusts learning rate automatically\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "print(f\"\\n✓ Model ready for training\")\n",
        "print(f\"  - Trainable parameters: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.2: Starting Training\n",
            "======================================================================\n",
            "Training for 20 epochs...\n",
            "Model: NeuMF-end\n",
            "Device: cpu\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.4856\n",
            "  Batch 200/8989 - Loss: 0.4368\n",
            "  Batch 300/8989 - Loss: 0.4145\n",
            "  Batch 400/8989 - Loss: 0.4014\n",
            "  Batch 500/8989 - Loss: 0.3922\n",
            "  Batch 600/8989 - Loss: 0.3861\n",
            "  Batch 700/8989 - Loss: 0.3830\n",
            "  Batch 800/8989 - Loss: 0.3795\n",
            "  Batch 900/8989 - Loss: 0.3767\n",
            "  Batch 1000/8989 - Loss: 0.3740\n",
            "  Batch 1100/8989 - Loss: 0.3719\n",
            "  Batch 1200/8989 - Loss: 0.3707\n",
            "  Batch 1300/8989 - Loss: 0.3699\n",
            "  Batch 1400/8989 - Loss: 0.3687\n",
            "  Batch 1500/8989 - Loss: 0.3673\n",
            "  Batch 1600/8989 - Loss: 0.3664\n",
            "  Batch 1700/8989 - Loss: 0.3652\n",
            "  Batch 1800/8989 - Loss: 0.3641\n",
            "  Batch 1900/8989 - Loss: 0.3631\n",
            "  Batch 2000/8989 - Loss: 0.3626\n",
            "  Batch 2100/8989 - Loss: 0.3622\n",
            "  Batch 2200/8989 - Loss: 0.3616\n",
            "  Batch 2300/8989 - Loss: 0.3611\n",
            "  Batch 2400/8989 - Loss: 0.3603\n",
            "  Batch 2500/8989 - Loss: 0.3597\n",
            "  Batch 2600/8989 - Loss: 0.3591\n",
            "  Batch 2700/8989 - Loss: 0.3585\n",
            "  Batch 2800/8989 - Loss: 0.3577\n",
            "  Batch 2900/8989 - Loss: 0.3571\n",
            "  Batch 3000/8989 - Loss: 0.3563\n",
            "  Batch 3100/8989 - Loss: 0.3557\n",
            "  Batch 3200/8989 - Loss: 0.3551\n",
            "  Batch 3300/8989 - Loss: 0.3543\n",
            "  Batch 3400/8989 - Loss: 0.3537\n",
            "  Batch 3500/8989 - Loss: 0.3532\n",
            "  Batch 3600/8989 - Loss: 0.3524\n",
            "  Batch 3700/8989 - Loss: 0.3517\n",
            "  Batch 3800/8989 - Loss: 0.3510\n",
            "  Batch 3900/8989 - Loss: 0.3504\n",
            "  Batch 4000/8989 - Loss: 0.3497\n",
            "  Batch 4100/8989 - Loss: 0.3492\n",
            "  Batch 4200/8989 - Loss: 0.3486\n",
            "  Batch 4300/8989 - Loss: 0.3479\n",
            "  Batch 4400/8989 - Loss: 0.3473\n",
            "  Batch 4500/8989 - Loss: 0.3467\n",
            "  Batch 4600/8989 - Loss: 0.3462\n",
            "  Batch 4700/8989 - Loss: 0.3455\n",
            "  Batch 4800/8989 - Loss: 0.3450\n",
            "  Batch 4900/8989 - Loss: 0.3444\n",
            "  Batch 5000/8989 - Loss: 0.3438\n",
            "  Batch 5100/8989 - Loss: 0.3433\n",
            "  Batch 5200/8989 - Loss: 0.3426\n",
            "  Batch 5300/8989 - Loss: 0.3420\n",
            "  Batch 5400/8989 - Loss: 0.3414\n",
            "  Batch 5500/8989 - Loss: 0.3409\n",
            "  Batch 5600/8989 - Loss: 0.3405\n",
            "  Batch 5700/8989 - Loss: 0.3399\n",
            "  Batch 5800/8989 - Loss: 0.3395\n",
            "  Batch 5900/8989 - Loss: 0.3389\n",
            "  Batch 6000/8989 - Loss: 0.3385\n",
            "  Batch 6100/8989 - Loss: 0.3380\n",
            "  Batch 6200/8989 - Loss: 0.3375\n",
            "  Batch 6300/8989 - Loss: 0.3371\n",
            "  Batch 6400/8989 - Loss: 0.3366\n",
            "  Batch 6500/8989 - Loss: 0.3361\n",
            "  Batch 6600/8989 - Loss: 0.3356\n",
            "  Batch 6700/8989 - Loss: 0.3350\n",
            "  Batch 6800/8989 - Loss: 0.3346\n",
            "  Batch 6900/8989 - Loss: 0.3342\n",
            "  Batch 7000/8989 - Loss: 0.3338\n",
            "  Batch 7100/8989 - Loss: 0.3332\n",
            "  Batch 7200/8989 - Loss: 0.3328\n",
            "  Batch 7300/8989 - Loss: 0.3324\n",
            "  Batch 7400/8989 - Loss: 0.3318\n",
            "  Batch 7500/8989 - Loss: 0.3313\n",
            "  Batch 7600/8989 - Loss: 0.3309\n",
            "  Batch 7700/8989 - Loss: 0.3304\n",
            "  Batch 7800/8989 - Loss: 0.3301\n",
            "  Batch 7900/8989 - Loss: 0.3297\n",
            "  Batch 8000/8989 - Loss: 0.3293\n",
            "  Batch 8100/8989 - Loss: 0.3289\n",
            "  Batch 8200/8989 - Loss: 0.3285\n",
            "  Batch 8300/8989 - Loss: 0.3281\n",
            "  Batch 8400/8989 - Loss: 0.3277\n",
            "  Batch 8500/8989 - Loss: 0.3273\n",
            "  Batch 8600/8989 - Loss: 0.3269\n",
            "  Batch 8700/8989 - Loss: 0.3265\n",
            "  Batch 8800/8989 - Loss: 0.3261\n",
            "  Batch 8900/8989 - Loss: 0.3257\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.3254\n",
            "  HR@10: 0.7023\n",
            "  NDCG@10: 0.4255\n",
            "  ✓ New best model! (HR@10: 0.7023)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 2/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2771\n",
            "  Batch 200/8989 - Loss: 0.2769\n",
            "  Batch 300/8989 - Loss: 0.2743\n",
            "  Batch 400/8989 - Loss: 0.2734\n",
            "  Batch 500/8989 - Loss: 0.2743\n",
            "  Batch 600/8989 - Loss: 0.2739\n",
            "  Batch 700/8989 - Loss: 0.2731\n",
            "  Batch 800/8989 - Loss: 0.2735\n",
            "  Batch 900/8989 - Loss: 0.2737\n",
            "  Batch 1000/8989 - Loss: 0.2741\n",
            "  Batch 1100/8989 - Loss: 0.2745\n",
            "  Batch 1200/8989 - Loss: 0.2747\n",
            "  Batch 1300/8989 - Loss: 0.2743\n",
            "  Batch 1400/8989 - Loss: 0.2742\n",
            "  Batch 1500/8989 - Loss: 0.2743\n",
            "  Batch 1600/8989 - Loss: 0.2746\n",
            "  Batch 1700/8989 - Loss: 0.2745\n",
            "  Batch 1800/8989 - Loss: 0.2745\n",
            "  Batch 1900/8989 - Loss: 0.2750\n",
            "  Batch 2000/8989 - Loss: 0.2751\n",
            "  Batch 2100/8989 - Loss: 0.2748\n",
            "  Batch 2200/8989 - Loss: 0.2749\n",
            "  Batch 2300/8989 - Loss: 0.2746\n",
            "  Batch 2400/8989 - Loss: 0.2747\n",
            "  Batch 2500/8989 - Loss: 0.2746\n",
            "  Batch 2600/8989 - Loss: 0.2742\n",
            "  Batch 2700/8989 - Loss: 0.2743\n",
            "  Batch 2800/8989 - Loss: 0.2743\n",
            "  Batch 2900/8989 - Loss: 0.2744\n",
            "  Batch 3000/8989 - Loss: 0.2743\n",
            "  Batch 3100/8989 - Loss: 0.2742\n",
            "  Batch 3200/8989 - Loss: 0.2740\n",
            "  Batch 3300/8989 - Loss: 0.2739\n",
            "  Batch 3400/8989 - Loss: 0.2737\n",
            "  Batch 3500/8989 - Loss: 0.2738\n",
            "  Batch 3600/8989 - Loss: 0.2737\n",
            "  Batch 3700/8989 - Loss: 0.2737\n",
            "  Batch 3800/8989 - Loss: 0.2737\n",
            "  Batch 3900/8989 - Loss: 0.2736\n",
            "  Batch 4000/8989 - Loss: 0.2735\n",
            "  Batch 4100/8989 - Loss: 0.2734\n",
            "  Batch 4200/8989 - Loss: 0.2733\n",
            "  Batch 4300/8989 - Loss: 0.2732\n",
            "  Batch 4400/8989 - Loss: 0.2732\n",
            "  Batch 4500/8989 - Loss: 0.2732\n",
            "  Batch 4600/8989 - Loss: 0.2732\n",
            "  Batch 4700/8989 - Loss: 0.2732\n",
            "  Batch 4800/8989 - Loss: 0.2731\n",
            "  Batch 4900/8989 - Loss: 0.2731\n",
            "  Batch 5000/8989 - Loss: 0.2730\n",
            "  Batch 5100/8989 - Loss: 0.2730\n",
            "  Batch 5200/8989 - Loss: 0.2730\n",
            "  Batch 5300/8989 - Loss: 0.2730\n",
            "  Batch 5400/8989 - Loss: 0.2729\n",
            "  Batch 5500/8989 - Loss: 0.2728\n",
            "  Batch 5600/8989 - Loss: 0.2727\n",
            "  Batch 5700/8989 - Loss: 0.2726\n",
            "  Batch 5800/8989 - Loss: 0.2725\n",
            "  Batch 5900/8989 - Loss: 0.2723\n",
            "  Batch 6000/8989 - Loss: 0.2723\n",
            "  Batch 6100/8989 - Loss: 0.2723\n",
            "  Batch 6200/8989 - Loss: 0.2722\n",
            "  Batch 6300/8989 - Loss: 0.2721\n",
            "  Batch 6400/8989 - Loss: 0.2721\n",
            "  Batch 6500/8989 - Loss: 0.2719\n",
            "  Batch 6600/8989 - Loss: 0.2719\n",
            "  Batch 6700/8989 - Loss: 0.2718\n",
            "  Batch 6800/8989 - Loss: 0.2717\n",
            "  Batch 6900/8989 - Loss: 0.2716\n",
            "  Batch 7000/8989 - Loss: 0.2716\n",
            "  Batch 7100/8989 - Loss: 0.2715\n",
            "  Batch 7200/8989 - Loss: 0.2714\n",
            "  Batch 7300/8989 - Loss: 0.2713\n",
            "  Batch 7400/8989 - Loss: 0.2712\n",
            "  Batch 7500/8989 - Loss: 0.2712\n",
            "  Batch 7600/8989 - Loss: 0.2711\n",
            "  Batch 7700/8989 - Loss: 0.2710\n",
            "  Batch 7800/8989 - Loss: 0.2710\n",
            "  Batch 7900/8989 - Loss: 0.2709\n",
            "  Batch 8000/8989 - Loss: 0.2709\n",
            "  Batch 8100/8989 - Loss: 0.2708\n",
            "  Batch 8200/8989 - Loss: 0.2706\n",
            "  Batch 8300/8989 - Loss: 0.2706\n",
            "  Batch 8400/8989 - Loss: 0.2704\n",
            "  Batch 8500/8989 - Loss: 0.2704\n",
            "  Batch 8600/8989 - Loss: 0.2703\n",
            "  Batch 8700/8989 - Loss: 0.2703\n",
            "  Batch 8800/8989 - Loss: 0.2702\n",
            "  Batch 8900/8989 - Loss: 0.2702\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2701\n",
            "  HR@10: 0.7352\n",
            "  NDCG@10: 0.4519\n",
            "  ✓ New best model! (HR@10: 0.7352)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 3/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2444\n",
            "  Batch 200/8989 - Loss: 0.2429\n",
            "  Batch 300/8989 - Loss: 0.2442\n",
            "  Batch 400/8989 - Loss: 0.2455\n",
            "  Batch 500/8989 - Loss: 0.2456\n",
            "  Batch 600/8989 - Loss: 0.2453\n",
            "  Batch 700/8989 - Loss: 0.2455\n",
            "  Batch 800/8989 - Loss: 0.2456\n",
            "  Batch 900/8989 - Loss: 0.2454\n",
            "  Batch 1000/8989 - Loss: 0.2456\n",
            "  Batch 1100/8989 - Loss: 0.2458\n",
            "  Batch 1200/8989 - Loss: 0.2463\n",
            "  Batch 1300/8989 - Loss: 0.2461\n",
            "  Batch 1400/8989 - Loss: 0.2462\n",
            "  Batch 1500/8989 - Loss: 0.2462\n",
            "  Batch 1600/8989 - Loss: 0.2461\n",
            "  Batch 1700/8989 - Loss: 0.2458\n",
            "  Batch 1800/8989 - Loss: 0.2459\n",
            "  Batch 1900/8989 - Loss: 0.2461\n",
            "  Batch 2000/8989 - Loss: 0.2462\n",
            "  Batch 2100/8989 - Loss: 0.2465\n",
            "  Batch 2200/8989 - Loss: 0.2467\n",
            "  Batch 2300/8989 - Loss: 0.2468\n",
            "  Batch 2400/8989 - Loss: 0.2469\n",
            "  Batch 2500/8989 - Loss: 0.2469\n",
            "  Batch 2600/8989 - Loss: 0.2469\n",
            "  Batch 2700/8989 - Loss: 0.2470\n",
            "  Batch 2800/8989 - Loss: 0.2471\n",
            "  Batch 2900/8989 - Loss: 0.2472\n",
            "  Batch 3000/8989 - Loss: 0.2472\n",
            "  Batch 3100/8989 - Loss: 0.2474\n",
            "  Batch 3200/8989 - Loss: 0.2475\n",
            "  Batch 3300/8989 - Loss: 0.2476\n",
            "  Batch 3400/8989 - Loss: 0.2477\n",
            "  Batch 3500/8989 - Loss: 0.2478\n",
            "  Batch 3600/8989 - Loss: 0.2479\n",
            "  Batch 3700/8989 - Loss: 0.2479\n",
            "  Batch 3800/8989 - Loss: 0.2478\n",
            "  Batch 3900/8989 - Loss: 0.2477\n",
            "  Batch 4000/8989 - Loss: 0.2478\n",
            "  Batch 4100/8989 - Loss: 0.2478\n",
            "  Batch 4200/8989 - Loss: 0.2478\n",
            "  Batch 4300/8989 - Loss: 0.2478\n",
            "  Batch 4400/8989 - Loss: 0.2479\n",
            "  Batch 4500/8989 - Loss: 0.2480\n",
            "  Batch 4600/8989 - Loss: 0.2480\n",
            "  Batch 4700/8989 - Loss: 0.2479\n",
            "  Batch 4800/8989 - Loss: 0.2479\n",
            "  Batch 4900/8989 - Loss: 0.2479\n",
            "  Batch 5000/8989 - Loss: 0.2480\n",
            "  Batch 5100/8989 - Loss: 0.2480\n",
            "  Batch 5200/8989 - Loss: 0.2481\n",
            "  Batch 5300/8989 - Loss: 0.2482\n",
            "  Batch 5400/8989 - Loss: 0.2483\n",
            "  Batch 5500/8989 - Loss: 0.2482\n",
            "  Batch 5600/8989 - Loss: 0.2482\n",
            "  Batch 5700/8989 - Loss: 0.2482\n",
            "  Batch 5800/8989 - Loss: 0.2483\n",
            "  Batch 5900/8989 - Loss: 0.2484\n",
            "  Batch 6000/8989 - Loss: 0.2483\n",
            "  Batch 6100/8989 - Loss: 0.2484\n",
            "  Batch 6200/8989 - Loss: 0.2484\n",
            "  Batch 6300/8989 - Loss: 0.2484\n",
            "  Batch 6400/8989 - Loss: 0.2483\n",
            "  Batch 6500/8989 - Loss: 0.2484\n",
            "  Batch 6600/8989 - Loss: 0.2484\n",
            "  Batch 6700/8989 - Loss: 0.2483\n",
            "  Batch 6800/8989 - Loss: 0.2483\n",
            "  Batch 6900/8989 - Loss: 0.2483\n",
            "  Batch 7000/8989 - Loss: 0.2484\n",
            "  Batch 7100/8989 - Loss: 0.2483\n",
            "  Batch 7200/8989 - Loss: 0.2483\n",
            "  Batch 7300/8989 - Loss: 0.2484\n",
            "  Batch 7400/8989 - Loss: 0.2484\n",
            "  Batch 7500/8989 - Loss: 0.2484\n",
            "  Batch 7600/8989 - Loss: 0.2484\n",
            "  Batch 7700/8989 - Loss: 0.2484\n",
            "  Batch 7800/8989 - Loss: 0.2485\n",
            "  Batch 7900/8989 - Loss: 0.2484\n",
            "  Batch 8000/8989 - Loss: 0.2485\n",
            "  Batch 8100/8989 - Loss: 0.2485\n",
            "  Batch 8200/8989 - Loss: 0.2484\n",
            "  Batch 8300/8989 - Loss: 0.2484\n",
            "  Batch 8400/8989 - Loss: 0.2485\n",
            "  Batch 8500/8989 - Loss: 0.2485\n",
            "  Batch 8600/8989 - Loss: 0.2486\n",
            "  Batch 8700/8989 - Loss: 0.2486\n",
            "  Batch 8800/8989 - Loss: 0.2486\n",
            "  Batch 8900/8989 - Loss: 0.2486\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2486\n",
            "  HR@10: 0.7404\n",
            "  NDCG@10: 0.4565\n",
            "  ✓ New best model! (HR@10: 0.7404)\n",
            "  ✓ Model saved to ./models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 4/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2311\n",
            "  Batch 200/8989 - Loss: 0.2298\n",
            "  Batch 300/8989 - Loss: 0.2286\n",
            "  Batch 400/8989 - Loss: 0.2289\n",
            "  Batch 500/8989 - Loss: 0.2285\n",
            "  Batch 600/8989 - Loss: 0.2278\n",
            "  Batch 700/8989 - Loss: 0.2289\n",
            "  Batch 800/8989 - Loss: 0.2291\n",
            "  Batch 900/8989 - Loss: 0.2291\n",
            "  Batch 1000/8989 - Loss: 0.2292\n",
            "  Batch 1100/8989 - Loss: 0.2296\n",
            "  Batch 1200/8989 - Loss: 0.2296\n",
            "  Batch 1300/8989 - Loss: 0.2303\n",
            "  Batch 1400/8989 - Loss: 0.2303\n",
            "  Batch 1500/8989 - Loss: 0.2307\n",
            "  Batch 1600/8989 - Loss: 0.2307\n",
            "  Batch 1700/8989 - Loss: 0.2307\n",
            "  Batch 1800/8989 - Loss: 0.2307\n",
            "  Batch 1900/8989 - Loss: 0.2308\n",
            "  Batch 2000/8989 - Loss: 0.2310\n",
            "  Batch 2100/8989 - Loss: 0.2311\n",
            "  Batch 2200/8989 - Loss: 0.2312\n",
            "  Batch 2300/8989 - Loss: 0.2313\n",
            "  Batch 2400/8989 - Loss: 0.2315\n",
            "  Batch 2500/8989 - Loss: 0.2317\n",
            "  Batch 2600/8989 - Loss: 0.2320\n",
            "  Batch 2700/8989 - Loss: 0.2320\n",
            "  Batch 2800/8989 - Loss: 0.2321\n",
            "  Batch 2900/8989 - Loss: 0.2322\n",
            "  Batch 3000/8989 - Loss: 0.2324\n",
            "  Batch 3100/8989 - Loss: 0.2325\n",
            "  Batch 3200/8989 - Loss: 0.2323\n",
            "  Batch 3300/8989 - Loss: 0.2323\n",
            "  Batch 3400/8989 - Loss: 0.2325\n",
            "  Batch 3500/8989 - Loss: 0.2325\n",
            "  Batch 3600/8989 - Loss: 0.2325\n",
            "  Batch 3700/8989 - Loss: 0.2326\n",
            "  Batch 3800/8989 - Loss: 0.2327\n",
            "  Batch 3900/8989 - Loss: 0.2327\n",
            "  Batch 4000/8989 - Loss: 0.2328\n",
            "  Batch 4100/8989 - Loss: 0.2329\n",
            "  Batch 4200/8989 - Loss: 0.2331\n",
            "  Batch 4300/8989 - Loss: 0.2332\n",
            "  Batch 4400/8989 - Loss: 0.2334\n",
            "  Batch 4500/8989 - Loss: 0.2335\n",
            "  Batch 4600/8989 - Loss: 0.2336\n",
            "  Batch 4700/8989 - Loss: 0.2338\n",
            "  Batch 4800/8989 - Loss: 0.2337\n",
            "  Batch 4900/8989 - Loss: 0.2338\n",
            "  Batch 5000/8989 - Loss: 0.2339\n",
            "  Batch 5100/8989 - Loss: 0.2339\n",
            "  Batch 5200/8989 - Loss: 0.2340\n",
            "  Batch 5300/8989 - Loss: 0.2340\n",
            "  Batch 5400/8989 - Loss: 0.2341\n",
            "  Batch 5500/8989 - Loss: 0.2341\n",
            "  Batch 5600/8989 - Loss: 0.2341\n",
            "  Batch 5700/8989 - Loss: 0.2342\n",
            "  Batch 5800/8989 - Loss: 0.2341\n",
            "  Batch 5900/8989 - Loss: 0.2342\n",
            "  Batch 6000/8989 - Loss: 0.2343\n",
            "  Batch 6100/8989 - Loss: 0.2345\n",
            "  Batch 6200/8989 - Loss: 0.2345\n",
            "  Batch 6300/8989 - Loss: 0.2346\n",
            "  Batch 6400/8989 - Loss: 0.2347\n",
            "  Batch 6500/8989 - Loss: 0.2347\n",
            "  Batch 6600/8989 - Loss: 0.2348\n",
            "  Batch 6700/8989 - Loss: 0.2348\n",
            "  Batch 6800/8989 - Loss: 0.2348\n",
            "  Batch 6900/8989 - Loss: 0.2349\n",
            "  Batch 7000/8989 - Loss: 0.2350\n",
            "  Batch 7100/8989 - Loss: 0.2350\n",
            "  Batch 7200/8989 - Loss: 0.2351\n",
            "  Batch 7300/8989 - Loss: 0.2351\n",
            "  Batch 7400/8989 - Loss: 0.2352\n",
            "  Batch 7500/8989 - Loss: 0.2352\n",
            "  Batch 7600/8989 - Loss: 0.2353\n",
            "  Batch 7700/8989 - Loss: 0.2354\n",
            "  Batch 7800/8989 - Loss: 0.2354\n",
            "  Batch 7900/8989 - Loss: 0.2355\n",
            "  Batch 8000/8989 - Loss: 0.2355\n",
            "  Batch 8100/8989 - Loss: 0.2356\n",
            "  Batch 8200/8989 - Loss: 0.2356\n",
            "  Batch 8300/8989 - Loss: 0.2356\n",
            "  Batch 8400/8989 - Loss: 0.2357\n",
            "  Batch 8500/8989 - Loss: 0.2357\n",
            "  Batch 8600/8989 - Loss: 0.2357\n",
            "  Batch 8700/8989 - Loss: 0.2358\n",
            "  Batch 8800/8989 - Loss: 0.2359\n",
            "  Batch 8900/8989 - Loss: 0.2360\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:42\n",
            "  Loss: 0.2361\n",
            "  HR@10: 0.7360\n",
            "  NDCG@10: 0.4530\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 5/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2147\n",
            "  Batch 200/8989 - Loss: 0.2130\n",
            "  Batch 300/8989 - Loss: 0.2138\n",
            "  Batch 400/8989 - Loss: 0.2132\n",
            "  Batch 500/8989 - Loss: 0.2147\n",
            "  Batch 600/8989 - Loss: 0.2154\n",
            "  Batch 700/8989 - Loss: 0.2154\n",
            "  Batch 800/8989 - Loss: 0.2164\n",
            "  Batch 900/8989 - Loss: 0.2166\n",
            "  Batch 1000/8989 - Loss: 0.2165\n",
            "  Batch 1100/8989 - Loss: 0.2169\n",
            "  Batch 1200/8989 - Loss: 0.2172\n",
            "  Batch 1300/8989 - Loss: 0.2173\n",
            "  Batch 1400/8989 - Loss: 0.2176\n",
            "  Batch 1500/8989 - Loss: 0.2179\n",
            "  Batch 1600/8989 - Loss: 0.2184\n",
            "  Batch 1700/8989 - Loss: 0.2187\n",
            "  Batch 1800/8989 - Loss: 0.2185\n",
            "  Batch 1900/8989 - Loss: 0.2190\n",
            "  Batch 2000/8989 - Loss: 0.2190\n",
            "  Batch 2100/8989 - Loss: 0.2193\n",
            "  Batch 2200/8989 - Loss: 0.2193\n",
            "  Batch 2300/8989 - Loss: 0.2197\n",
            "  Batch 2400/8989 - Loss: 0.2201\n",
            "  Batch 2500/8989 - Loss: 0.2203\n",
            "  Batch 2600/8989 - Loss: 0.2206\n",
            "  Batch 2700/8989 - Loss: 0.2208\n",
            "  Batch 2800/8989 - Loss: 0.2210\n",
            "  Batch 2900/8989 - Loss: 0.2213\n",
            "  Batch 3000/8989 - Loss: 0.2216\n",
            "  Batch 3100/8989 - Loss: 0.2216\n",
            "  Batch 3200/8989 - Loss: 0.2217\n",
            "  Batch 3300/8989 - Loss: 0.2218\n",
            "  Batch 3400/8989 - Loss: 0.2219\n",
            "  Batch 3500/8989 - Loss: 0.2219\n",
            "  Batch 3600/8989 - Loss: 0.2222\n",
            "  Batch 3700/8989 - Loss: 0.2224\n",
            "  Batch 3800/8989 - Loss: 0.2225\n",
            "  Batch 3900/8989 - Loss: 0.2225\n",
            "  Batch 4000/8989 - Loss: 0.2226\n",
            "  Batch 4100/8989 - Loss: 0.2227\n",
            "  Batch 4200/8989 - Loss: 0.2227\n",
            "  Batch 4300/8989 - Loss: 0.2229\n",
            "  Batch 4400/8989 - Loss: 0.2230\n",
            "  Batch 4500/8989 - Loss: 0.2231\n",
            "  Batch 4600/8989 - Loss: 0.2232\n",
            "  Batch 4700/8989 - Loss: 0.2232\n",
            "  Batch 4800/8989 - Loss: 0.2232\n",
            "  Batch 4900/8989 - Loss: 0.2234\n",
            "  Batch 5000/8989 - Loss: 0.2235\n",
            "  Batch 5100/8989 - Loss: 0.2235\n",
            "  Batch 5200/8989 - Loss: 0.2236\n",
            "  Batch 5300/8989 - Loss: 0.2237\n",
            "  Batch 5400/8989 - Loss: 0.2237\n",
            "  Batch 5500/8989 - Loss: 0.2239\n",
            "  Batch 5600/8989 - Loss: 0.2240\n",
            "  Batch 5700/8989 - Loss: 0.2240\n",
            "  Batch 5800/8989 - Loss: 0.2241\n",
            "  Batch 5900/8989 - Loss: 0.2242\n",
            "  Batch 6000/8989 - Loss: 0.2244\n",
            "  Batch 6100/8989 - Loss: 0.2244\n",
            "  Batch 6200/8989 - Loss: 0.2245\n",
            "  Batch 6300/8989 - Loss: 0.2246\n",
            "  Batch 6400/8989 - Loss: 0.2247\n",
            "  Batch 6500/8989 - Loss: 0.2247\n",
            "  Batch 6600/8989 - Loss: 0.2249\n",
            "  Batch 6700/8989 - Loss: 0.2249\n",
            "  Batch 6800/8989 - Loss: 0.2251\n",
            "  Batch 6900/8989 - Loss: 0.2252\n",
            "  Batch 7000/8989 - Loss: 0.2253\n",
            "  Batch 7100/8989 - Loss: 0.2254\n",
            "  Batch 7200/8989 - Loss: 0.2254\n",
            "  Batch 7300/8989 - Loss: 0.2255\n",
            "  Batch 7400/8989 - Loss: 0.2256\n",
            "  Batch 7500/8989 - Loss: 0.2257\n",
            "  Batch 7600/8989 - Loss: 0.2258\n",
            "  Batch 7700/8989 - Loss: 0.2258\n",
            "  Batch 7800/8989 - Loss: 0.2258\n",
            "  Batch 7900/8989 - Loss: 0.2259\n",
            "  Batch 8000/8989 - Loss: 0.2260\n",
            "  Batch 8100/8989 - Loss: 0.2261\n",
            "  Batch 8200/8989 - Loss: 0.2263\n",
            "  Batch 8300/8989 - Loss: 0.2264\n",
            "  Batch 8400/8989 - Loss: 0.2264\n",
            "  Batch 8500/8989 - Loss: 0.2264\n",
            "  Batch 8600/8989 - Loss: 0.2265\n",
            "  Batch 8700/8989 - Loss: 0.2265\n",
            "  Batch 8800/8989 - Loss: 0.2266\n",
            "  Batch 8900/8989 - Loss: 0.2267\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:41\n",
            "  Loss: 0.2267\n",
            "  HR@10: 0.7368\n",
            "  NDCG@10: 0.4552\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 6/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2047\n",
            "  Batch 200/8989 - Loss: 0.2034\n",
            "  Batch 300/8989 - Loss: 0.2063\n",
            "  Batch 400/8989 - Loss: 0.2079\n",
            "  Batch 500/8989 - Loss: 0.2085\n",
            "  Batch 600/8989 - Loss: 0.2084\n",
            "  Batch 700/8989 - Loss: 0.2090\n",
            "  Batch 800/8989 - Loss: 0.2090\n",
            "  Batch 900/8989 - Loss: 0.2089\n",
            "  Batch 1000/8989 - Loss: 0.2087\n",
            "  Batch 1100/8989 - Loss: 0.2085\n",
            "  Batch 1200/8989 - Loss: 0.2089\n",
            "  Batch 1300/8989 - Loss: 0.2090\n",
            "  Batch 1400/8989 - Loss: 0.2095\n",
            "  Batch 1500/8989 - Loss: 0.2099\n",
            "  Batch 1600/8989 - Loss: 0.2101\n",
            "  Batch 1700/8989 - Loss: 0.2104\n",
            "  Batch 1800/8989 - Loss: 0.2105\n",
            "  Batch 1900/8989 - Loss: 0.2107\n",
            "  Batch 2000/8989 - Loss: 0.2108\n",
            "  Batch 2100/8989 - Loss: 0.2110\n",
            "  Batch 2200/8989 - Loss: 0.2110\n",
            "  Batch 2300/8989 - Loss: 0.2113\n",
            "  Batch 2400/8989 - Loss: 0.2115\n",
            "  Batch 2500/8989 - Loss: 0.2116\n",
            "  Batch 2600/8989 - Loss: 0.2119\n",
            "  Batch 2700/8989 - Loss: 0.2119\n",
            "  Batch 2800/8989 - Loss: 0.2121\n",
            "  Batch 2900/8989 - Loss: 0.2122\n",
            "  Batch 3000/8989 - Loss: 0.2124\n",
            "  Batch 3100/8989 - Loss: 0.2126\n",
            "  Batch 3200/8989 - Loss: 0.2128\n",
            "  Batch 3300/8989 - Loss: 0.2131\n",
            "  Batch 3400/8989 - Loss: 0.2132\n",
            "  Batch 3500/8989 - Loss: 0.2132\n",
            "  Batch 3600/8989 - Loss: 0.2135\n",
            "  Batch 3700/8989 - Loss: 0.2138\n",
            "  Batch 3800/8989 - Loss: 0.2140\n",
            "  Batch 3900/8989 - Loss: 0.2140\n",
            "  Batch 4000/8989 - Loss: 0.2140\n",
            "  Batch 4100/8989 - Loss: 0.2142\n",
            "  Batch 4200/8989 - Loss: 0.2144\n",
            "  Batch 4300/8989 - Loss: 0.2145\n",
            "  Batch 4400/8989 - Loss: 0.2147\n",
            "  Batch 4500/8989 - Loss: 0.2149\n",
            "  Batch 4600/8989 - Loss: 0.2150\n",
            "  Batch 4700/8989 - Loss: 0.2151\n",
            "  Batch 4800/8989 - Loss: 0.2152\n",
            "  Batch 4900/8989 - Loss: 0.2153\n",
            "  Batch 5000/8989 - Loss: 0.2154\n",
            "  Batch 5100/8989 - Loss: 0.2155\n",
            "  Batch 5200/8989 - Loss: 0.2157\n",
            "  Batch 5300/8989 - Loss: 0.2157\n",
            "  Batch 5400/8989 - Loss: 0.2158\n",
            "  Batch 5500/8989 - Loss: 0.2159\n",
            "  Batch 5600/8989 - Loss: 0.2160\n",
            "  Batch 5700/8989 - Loss: 0.2162\n",
            "  Batch 5800/8989 - Loss: 0.2163\n",
            "  Batch 5900/8989 - Loss: 0.2165\n",
            "  Batch 6000/8989 - Loss: 0.2165\n",
            "  Batch 6100/8989 - Loss: 0.2167\n",
            "  Batch 6200/8989 - Loss: 0.2168\n",
            "  Batch 6300/8989 - Loss: 0.2170\n",
            "  Batch 6400/8989 - Loss: 0.2171\n",
            "  Batch 6500/8989 - Loss: 0.2173\n",
            "  Batch 6600/8989 - Loss: 0.2173\n",
            "  Batch 6700/8989 - Loss: 0.2175\n",
            "  Batch 6800/8989 - Loss: 0.2177\n",
            "  Batch 6900/8989 - Loss: 0.2178\n",
            "  Batch 7000/8989 - Loss: 0.2178\n",
            "  Batch 7100/8989 - Loss: 0.2179\n",
            "  Batch 7200/8989 - Loss: 0.2180\n",
            "  Batch 7300/8989 - Loss: 0.2181\n",
            "  Batch 7400/8989 - Loss: 0.2182\n",
            "  Batch 7500/8989 - Loss: 0.2183\n",
            "  Batch 7600/8989 - Loss: 0.2184\n",
            "  Batch 7700/8989 - Loss: 0.2185\n",
            "  Batch 7800/8989 - Loss: 0.2186\n",
            "  Batch 7900/8989 - Loss: 0.2187\n",
            "  Batch 8000/8989 - Loss: 0.2188\n",
            "  Batch 8100/8989 - Loss: 0.2189\n",
            "  Batch 8200/8989 - Loss: 0.2190\n",
            "  Batch 8300/8989 - Loss: 0.2191\n",
            "  Batch 8400/8989 - Loss: 0.2192\n",
            "  Batch 8500/8989 - Loss: 0.2193\n",
            "  Batch 8600/8989 - Loss: 0.2194\n",
            "  Batch 8700/8989 - Loss: 0.2195\n",
            "  Batch 8800/8989 - Loss: 0.2195\n",
            "  Batch 8900/8989 - Loss: 0.2196\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:40\n",
            "  Loss: 0.2197\n",
            "  HR@10: 0.7336\n",
            "  NDCG@10: 0.4491\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 7/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2048\n",
            "  Batch 200/8989 - Loss: 0.2033\n",
            "  Batch 300/8989 - Loss: 0.2040\n",
            "  Batch 400/8989 - Loss: 0.2027\n",
            "  Batch 500/8989 - Loss: 0.2026\n",
            "  Batch 600/8989 - Loss: 0.2024\n",
            "  Batch 700/8989 - Loss: 0.2020\n",
            "  Batch 800/8989 - Loss: 0.2024\n",
            "  Batch 900/8989 - Loss: 0.2028\n",
            "  Batch 1000/8989 - Loss: 0.2032\n",
            "  Batch 1100/8989 - Loss: 0.2036\n",
            "  Batch 1200/8989 - Loss: 0.2037\n",
            "  Batch 1300/8989 - Loss: 0.2037\n",
            "  Batch 1400/8989 - Loss: 0.2040\n",
            "  Batch 1500/8989 - Loss: 0.2039\n",
            "  Batch 1600/8989 - Loss: 0.2041\n",
            "  Batch 1700/8989 - Loss: 0.2043\n",
            "  Batch 1800/8989 - Loss: 0.2043\n",
            "  Batch 1900/8989 - Loss: 0.2044\n",
            "  Batch 2000/8989 - Loss: 0.2048\n",
            "  Batch 2100/8989 - Loss: 0.2049\n",
            "  Batch 2200/8989 - Loss: 0.2051\n",
            "  Batch 2300/8989 - Loss: 0.2053\n",
            "  Batch 2400/8989 - Loss: 0.2056\n",
            "  Batch 2500/8989 - Loss: 0.2058\n",
            "  Batch 2600/8989 - Loss: 0.2058\n",
            "  Batch 2700/8989 - Loss: 0.2060\n",
            "  Batch 2800/8989 - Loss: 0.2061\n",
            "  Batch 2900/8989 - Loss: 0.2064\n",
            "  Batch 3000/8989 - Loss: 0.2067\n",
            "  Batch 3100/8989 - Loss: 0.2070\n",
            "  Batch 3200/8989 - Loss: 0.2071\n",
            "  Batch 3300/8989 - Loss: 0.2073\n",
            "  Batch 3400/8989 - Loss: 0.2075\n",
            "  Batch 3500/8989 - Loss: 0.2076\n",
            "  Batch 3600/8989 - Loss: 0.2077\n",
            "  Batch 3700/8989 - Loss: 0.2078\n",
            "  Batch 3800/8989 - Loss: 0.2079\n",
            "  Batch 3900/8989 - Loss: 0.2081\n",
            "  Batch 4000/8989 - Loss: 0.2083\n",
            "  Batch 4100/8989 - Loss: 0.2084\n",
            "  Batch 4200/8989 - Loss: 0.2085\n",
            "  Batch 4300/8989 - Loss: 0.2086\n",
            "  Batch 4400/8989 - Loss: 0.2088\n",
            "  Batch 4500/8989 - Loss: 0.2089\n",
            "  Batch 4600/8989 - Loss: 0.2092\n",
            "  Batch 4700/8989 - Loss: 0.2093\n",
            "  Batch 4800/8989 - Loss: 0.2094\n",
            "  Batch 4900/8989 - Loss: 0.2096\n",
            "  Batch 5000/8989 - Loss: 0.2098\n",
            "  Batch 5100/8989 - Loss: 0.2098\n",
            "  Batch 5200/8989 - Loss: 0.2099\n",
            "  Batch 5300/8989 - Loss: 0.2100\n",
            "  Batch 5400/8989 - Loss: 0.2101\n",
            "  Batch 5500/8989 - Loss: 0.2103\n",
            "  Batch 5600/8989 - Loss: 0.2105\n",
            "  Batch 5700/8989 - Loss: 0.2107\n",
            "  Batch 5800/8989 - Loss: 0.2108\n",
            "  Batch 5900/8989 - Loss: 0.2109\n",
            "  Batch 6000/8989 - Loss: 0.2110\n",
            "  Batch 6100/8989 - Loss: 0.2111\n",
            "  Batch 6200/8989 - Loss: 0.2112\n",
            "  Batch 6300/8989 - Loss: 0.2113\n",
            "  Batch 6400/8989 - Loss: 0.2115\n",
            "  Batch 6500/8989 - Loss: 0.2116\n",
            "  Batch 6600/8989 - Loss: 0.2117\n",
            "  Batch 6700/8989 - Loss: 0.2118\n",
            "  Batch 6800/8989 - Loss: 0.2119\n",
            "  Batch 6900/8989 - Loss: 0.2120\n",
            "  Batch 7000/8989 - Loss: 0.2121\n",
            "  Batch 7100/8989 - Loss: 0.2122\n",
            "  Batch 7200/8989 - Loss: 0.2123\n",
            "  Batch 7300/8989 - Loss: 0.2123\n",
            "  Batch 7400/8989 - Loss: 0.2125\n",
            "  Batch 7500/8989 - Loss: 0.2125\n",
            "  Batch 7600/8989 - Loss: 0.2126\n",
            "  Batch 7700/8989 - Loss: 0.2127\n",
            "  Batch 7800/8989 - Loss: 0.2127\n",
            "  Batch 7900/8989 - Loss: 0.2128\n",
            "  Batch 8000/8989 - Loss: 0.2129\n",
            "  Batch 8100/8989 - Loss: 0.2130\n",
            "  Batch 8200/8989 - Loss: 0.2130\n",
            "  Batch 8300/8989 - Loss: 0.2131\n",
            "  Batch 8400/8989 - Loss: 0.2132\n",
            "  Batch 8500/8989 - Loss: 0.2133\n",
            "  Batch 8600/8989 - Loss: 0.2133\n",
            "  Batch 8700/8989 - Loss: 0.2134\n",
            "  Batch 8800/8989 - Loss: 0.2135\n",
            "  Batch 8900/8989 - Loss: 0.2135\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.2135\n",
            "  HR@10: 0.7371\n",
            "  NDCG@10: 0.4524\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 8/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2013\n",
            "  Batch 200/8989 - Loss: 0.1996\n",
            "  Batch 300/8989 - Loss: 0.1989\n",
            "  Batch 400/8989 - Loss: 0.1981\n",
            "  Batch 500/8989 - Loss: 0.1975\n",
            "  Batch 600/8989 - Loss: 0.1976\n",
            "  Batch 700/8989 - Loss: 0.1983\n",
            "  Batch 800/8989 - Loss: 0.1986\n",
            "  Batch 900/8989 - Loss: 0.1992\n",
            "  Batch 1000/8989 - Loss: 0.1990\n",
            "  Batch 1100/8989 - Loss: 0.1993\n",
            "  Batch 1200/8989 - Loss: 0.1989\n",
            "  Batch 1300/8989 - Loss: 0.1991\n",
            "  Batch 1400/8989 - Loss: 0.1989\n",
            "  Batch 1500/8989 - Loss: 0.1987\n",
            "  Batch 1600/8989 - Loss: 0.1986\n",
            "  Batch 1700/8989 - Loss: 0.1988\n",
            "  Batch 1800/8989 - Loss: 0.1989\n",
            "  Batch 1900/8989 - Loss: 0.1991\n",
            "  Batch 2000/8989 - Loss: 0.1994\n",
            "  Batch 2100/8989 - Loss: 0.1996\n",
            "  Batch 2200/8989 - Loss: 0.1999\n",
            "  Batch 2300/8989 - Loss: 0.1998\n",
            "  Batch 2400/8989 - Loss: 0.2002\n",
            "  Batch 2500/8989 - Loss: 0.2002\n",
            "  Batch 2600/8989 - Loss: 0.2003\n",
            "  Batch 2700/8989 - Loss: 0.2005\n",
            "  Batch 2800/8989 - Loss: 0.2008\n",
            "  Batch 2900/8989 - Loss: 0.2008\n",
            "  Batch 3000/8989 - Loss: 0.2010\n",
            "  Batch 3100/8989 - Loss: 0.2012\n",
            "  Batch 3200/8989 - Loss: 0.2013\n",
            "  Batch 3300/8989 - Loss: 0.2013\n",
            "  Batch 3400/8989 - Loss: 0.2015\n",
            "  Batch 3500/8989 - Loss: 0.2016\n",
            "  Batch 3600/8989 - Loss: 0.2018\n",
            "  Batch 3700/8989 - Loss: 0.2019\n",
            "  Batch 3800/8989 - Loss: 0.2022\n",
            "  Batch 3900/8989 - Loss: 0.2024\n",
            "  Batch 4000/8989 - Loss: 0.2026\n",
            "  Batch 4100/8989 - Loss: 0.2028\n",
            "  Batch 4200/8989 - Loss: 0.2030\n",
            "  Batch 4300/8989 - Loss: 0.2031\n",
            "  Batch 4400/8989 - Loss: 0.2032\n",
            "  Batch 4500/8989 - Loss: 0.2033\n",
            "  Batch 4600/8989 - Loss: 0.2035\n",
            "  Batch 4700/8989 - Loss: 0.2038\n",
            "  Batch 4800/8989 - Loss: 0.2039\n",
            "  Batch 4900/8989 - Loss: 0.2040\n",
            "  Batch 5000/8989 - Loss: 0.2042\n",
            "  Batch 5100/8989 - Loss: 0.2043\n",
            "  Batch 5200/8989 - Loss: 0.2045\n",
            "  Batch 5300/8989 - Loss: 0.2046\n",
            "  Batch 5400/8989 - Loss: 0.2047\n",
            "  Batch 5500/8989 - Loss: 0.2048\n",
            "  Batch 5600/8989 - Loss: 0.2048\n",
            "  Batch 5700/8989 - Loss: 0.2050\n",
            "  Batch 5800/8989 - Loss: 0.2052\n",
            "  Batch 5900/8989 - Loss: 0.2053\n",
            "  Batch 6000/8989 - Loss: 0.2054\n",
            "  Batch 6100/8989 - Loss: 0.2055\n",
            "  Batch 6200/8989 - Loss: 0.2055\n",
            "  Batch 6300/8989 - Loss: 0.2057\n",
            "  Batch 6400/8989 - Loss: 0.2058\n",
            "  Batch 6500/8989 - Loss: 0.2060\n",
            "  Batch 6600/8989 - Loss: 0.2061\n",
            "  Batch 6700/8989 - Loss: 0.2062\n",
            "  Batch 6800/8989 - Loss: 0.2064\n",
            "  Batch 6900/8989 - Loss: 0.2065\n",
            "  Batch 7000/8989 - Loss: 0.2066\n",
            "  Batch 7100/8989 - Loss: 0.2068\n",
            "  Batch 7200/8989 - Loss: 0.2069\n",
            "  Batch 7300/8989 - Loss: 0.2071\n",
            "  Batch 7400/8989 - Loss: 0.2072\n",
            "  Batch 7500/8989 - Loss: 0.2072\n",
            "  Batch 7600/8989 - Loss: 0.2074\n",
            "  Batch 7700/8989 - Loss: 0.2075\n",
            "  Batch 7800/8989 - Loss: 0.2076\n",
            "  Batch 7900/8989 - Loss: 0.2077\n",
            "  Batch 8000/8989 - Loss: 0.2077\n",
            "  Batch 8100/8989 - Loss: 0.2078\n",
            "  Batch 8200/8989 - Loss: 0.2079\n",
            "  Batch 8300/8989 - Loss: 0.2079\n",
            "  Batch 8400/8989 - Loss: 0.2081\n",
            "  Batch 8500/8989 - Loss: 0.2081\n",
            "  Batch 8600/8989 - Loss: 0.2082\n",
            "  Batch 8700/8989 - Loss: 0.2083\n",
            "  Batch 8800/8989 - Loss: 0.2084\n",
            "  Batch 8900/8989 - Loss: 0.2085\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.2087\n",
            "  HR@10: 0.7341\n",
            "  NDCG@10: 0.4508\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 9/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1860\n",
            "  Batch 200/8989 - Loss: 0.1872\n",
            "  Batch 300/8989 - Loss: 0.1912\n",
            "  Batch 400/8989 - Loss: 0.1919\n",
            "  Batch 500/8989 - Loss: 0.1912\n",
            "  Batch 600/8989 - Loss: 0.1914\n",
            "  Batch 700/8989 - Loss: 0.1916\n",
            "  Batch 800/8989 - Loss: 0.1920\n",
            "  Batch 900/8989 - Loss: 0.1922\n",
            "  Batch 1000/8989 - Loss: 0.1928\n",
            "  Batch 1100/8989 - Loss: 0.1929\n",
            "  Batch 1200/8989 - Loss: 0.1932\n",
            "  Batch 1300/8989 - Loss: 0.1935\n",
            "  Batch 1400/8989 - Loss: 0.1941\n",
            "  Batch 1500/8989 - Loss: 0.1942\n",
            "  Batch 1600/8989 - Loss: 0.1944\n",
            "  Batch 1700/8989 - Loss: 0.1945\n",
            "  Batch 1800/8989 - Loss: 0.1949\n",
            "  Batch 1900/8989 - Loss: 0.1951\n",
            "  Batch 2000/8989 - Loss: 0.1954\n",
            "  Batch 2100/8989 - Loss: 0.1956\n",
            "  Batch 2200/8989 - Loss: 0.1956\n",
            "  Batch 2300/8989 - Loss: 0.1957\n",
            "  Batch 2400/8989 - Loss: 0.1958\n",
            "  Batch 2500/8989 - Loss: 0.1960\n",
            "  Batch 2600/8989 - Loss: 0.1961\n",
            "  Batch 2700/8989 - Loss: 0.1961\n",
            "  Batch 2800/8989 - Loss: 0.1963\n",
            "  Batch 2900/8989 - Loss: 0.1964\n",
            "  Batch 3000/8989 - Loss: 0.1965\n",
            "  Batch 3100/8989 - Loss: 0.1966\n",
            "  Batch 3200/8989 - Loss: 0.1967\n",
            "  Batch 3300/8989 - Loss: 0.1969\n",
            "  Batch 3400/8989 - Loss: 0.1970\n",
            "  Batch 3500/8989 - Loss: 0.1970\n",
            "  Batch 3600/8989 - Loss: 0.1971\n",
            "  Batch 3700/8989 - Loss: 0.1971\n",
            "  Batch 3800/8989 - Loss: 0.1973\n",
            "  Batch 3900/8989 - Loss: 0.1973\n",
            "  Batch 4000/8989 - Loss: 0.1974\n",
            "  Batch 4100/8989 - Loss: 0.1976\n",
            "  Batch 4200/8989 - Loss: 0.1977\n",
            "  Batch 4300/8989 - Loss: 0.1978\n",
            "  Batch 4400/8989 - Loss: 0.1981\n",
            "  Batch 4500/8989 - Loss: 0.1982\n",
            "  Batch 4600/8989 - Loss: 0.1985\n",
            "  Batch 4700/8989 - Loss: 0.1986\n",
            "  Batch 4800/8989 - Loss: 0.1987\n",
            "  Batch 4900/8989 - Loss: 0.1989\n",
            "  Batch 5000/8989 - Loss: 0.1991\n",
            "  Batch 5100/8989 - Loss: 0.1992\n",
            "  Batch 5200/8989 - Loss: 0.1993\n",
            "  Batch 5300/8989 - Loss: 0.1995\n",
            "  Batch 5400/8989 - Loss: 0.1997\n",
            "  Batch 5500/8989 - Loss: 0.1997\n",
            "  Batch 5600/8989 - Loss: 0.1999\n",
            "  Batch 5700/8989 - Loss: 0.2000\n",
            "  Batch 5800/8989 - Loss: 0.2002\n",
            "  Batch 5900/8989 - Loss: 0.2002\n",
            "  Batch 6000/8989 - Loss: 0.2004\n",
            "  Batch 6100/8989 - Loss: 0.2005\n",
            "  Batch 6200/8989 - Loss: 0.2006\n",
            "  Batch 6300/8989 - Loss: 0.2007\n",
            "  Batch 6400/8989 - Loss: 0.2009\n",
            "  Batch 6500/8989 - Loss: 0.2010\n",
            "  Batch 6600/8989 - Loss: 0.2010\n",
            "  Batch 6700/8989 - Loss: 0.2012\n",
            "  Batch 6800/8989 - Loss: 0.2013\n",
            "  Batch 6900/8989 - Loss: 0.2014\n",
            "  Batch 7000/8989 - Loss: 0.2015\n",
            "  Batch 7100/8989 - Loss: 0.2017\n",
            "  Batch 7200/8989 - Loss: 0.2018\n",
            "  Batch 7300/8989 - Loss: 0.2019\n",
            "  Batch 7400/8989 - Loss: 0.2020\n",
            "  Batch 7500/8989 - Loss: 0.2021\n",
            "  Batch 7600/8989 - Loss: 0.2022\n",
            "  Batch 7700/8989 - Loss: 0.2024\n",
            "  Batch 7800/8989 - Loss: 0.2025\n",
            "  Batch 7900/8989 - Loss: 0.2026\n",
            "  Batch 8000/8989 - Loss: 0.2027\n",
            "  Batch 8100/8989 - Loss: 0.2028\n",
            "  Batch 8200/8989 - Loss: 0.2029\n",
            "  Batch 8300/8989 - Loss: 0.2029\n",
            "  Batch 8400/8989 - Loss: 0.2030\n",
            "  Batch 8500/8989 - Loss: 0.2031\n",
            "  Batch 8600/8989 - Loss: 0.2032\n",
            "  Batch 8700/8989 - Loss: 0.2033\n",
            "  Batch 8800/8989 - Loss: 0.2034\n",
            "  Batch 8900/8989 - Loss: 0.2034\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.2036\n",
            "  HR@10: 0.7296\n",
            "  NDCG@10: 0.4493\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 10/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1828\n",
            "  Batch 200/8989 - Loss: 0.1842\n",
            "  Batch 300/8989 - Loss: 0.1859\n",
            "  Batch 400/8989 - Loss: 0.1876\n",
            "  Batch 500/8989 - Loss: 0.1881\n",
            "  Batch 600/8989 - Loss: 0.1883\n",
            "  Batch 700/8989 - Loss: 0.1885\n",
            "  Batch 800/8989 - Loss: 0.1879\n",
            "  Batch 900/8989 - Loss: 0.1883\n",
            "  Batch 1000/8989 - Loss: 0.1880\n",
            "  Batch 1100/8989 - Loss: 0.1884\n",
            "  Batch 1200/8989 - Loss: 0.1886\n",
            "  Batch 1300/8989 - Loss: 0.1888\n",
            "  Batch 1400/8989 - Loss: 0.1891\n",
            "  Batch 1500/8989 - Loss: 0.1893\n",
            "  Batch 1600/8989 - Loss: 0.1894\n",
            "  Batch 1700/8989 - Loss: 0.1896\n",
            "  Batch 1800/8989 - Loss: 0.1897\n",
            "  Batch 1900/8989 - Loss: 0.1898\n",
            "  Batch 2000/8989 - Loss: 0.1903\n",
            "  Batch 2100/8989 - Loss: 0.1903\n",
            "  Batch 2200/8989 - Loss: 0.1905\n",
            "  Batch 2300/8989 - Loss: 0.1908\n",
            "  Batch 2400/8989 - Loss: 0.1911\n",
            "  Batch 2500/8989 - Loss: 0.1912\n",
            "  Batch 2600/8989 - Loss: 0.1913\n",
            "  Batch 2700/8989 - Loss: 0.1913\n",
            "  Batch 2800/8989 - Loss: 0.1916\n",
            "  Batch 2900/8989 - Loss: 0.1917\n",
            "  Batch 3000/8989 - Loss: 0.1920\n",
            "  Batch 3100/8989 - Loss: 0.1922\n",
            "  Batch 3200/8989 - Loss: 0.1923\n",
            "  Batch 3300/8989 - Loss: 0.1924\n",
            "  Batch 3400/8989 - Loss: 0.1926\n",
            "  Batch 3500/8989 - Loss: 0.1927\n",
            "  Batch 3600/8989 - Loss: 0.1928\n",
            "  Batch 3700/8989 - Loss: 0.1930\n",
            "  Batch 3800/8989 - Loss: 0.1930\n",
            "  Batch 3900/8989 - Loss: 0.1930\n",
            "  Batch 4000/8989 - Loss: 0.1932\n",
            "  Batch 4100/8989 - Loss: 0.1933\n",
            "  Batch 4200/8989 - Loss: 0.1934\n",
            "  Batch 4300/8989 - Loss: 0.1935\n",
            "  Batch 4400/8989 - Loss: 0.1937\n",
            "  Batch 4500/8989 - Loss: 0.1938\n",
            "  Batch 4600/8989 - Loss: 0.1938\n",
            "  Batch 4700/8989 - Loss: 0.1940\n",
            "  Batch 4800/8989 - Loss: 0.1941\n",
            "  Batch 4900/8989 - Loss: 0.1944\n",
            "  Batch 5000/8989 - Loss: 0.1945\n",
            "  Batch 5100/8989 - Loss: 0.1946\n",
            "  Batch 5200/8989 - Loss: 0.1947\n",
            "  Batch 5300/8989 - Loss: 0.1949\n",
            "  Batch 5400/8989 - Loss: 0.1950\n",
            "  Batch 5500/8989 - Loss: 0.1950\n",
            "  Batch 5600/8989 - Loss: 0.1952\n",
            "  Batch 5700/8989 - Loss: 0.1953\n",
            "  Batch 5800/8989 - Loss: 0.1954\n",
            "  Batch 5900/8989 - Loss: 0.1955\n",
            "  Batch 6000/8989 - Loss: 0.1957\n",
            "  Batch 6100/8989 - Loss: 0.1958\n",
            "  Batch 6200/8989 - Loss: 0.1960\n",
            "  Batch 6300/8989 - Loss: 0.1961\n",
            "  Batch 6400/8989 - Loss: 0.1962\n",
            "  Batch 6500/8989 - Loss: 0.1964\n",
            "  Batch 6600/8989 - Loss: 0.1964\n",
            "  Batch 6700/8989 - Loss: 0.1965\n",
            "  Batch 6800/8989 - Loss: 0.1967\n",
            "  Batch 6900/8989 - Loss: 0.1968\n",
            "  Batch 7000/8989 - Loss: 0.1969\n",
            "  Batch 7100/8989 - Loss: 0.1969\n",
            "  Batch 7200/8989 - Loss: 0.1971\n",
            "  Batch 7300/8989 - Loss: 0.1972\n",
            "  Batch 7400/8989 - Loss: 0.1974\n",
            "  Batch 7500/8989 - Loss: 0.1975\n",
            "  Batch 7600/8989 - Loss: 0.1976\n",
            "  Batch 7700/8989 - Loss: 0.1977\n",
            "  Batch 7800/8989 - Loss: 0.1978\n",
            "  Batch 7900/8989 - Loss: 0.1980\n",
            "  Batch 8000/8989 - Loss: 0.1981\n",
            "  Batch 8100/8989 - Loss: 0.1982\n",
            "  Batch 8200/8989 - Loss: 0.1984\n",
            "  Batch 8300/8989 - Loss: 0.1985\n",
            "  Batch 8400/8989 - Loss: 0.1986\n",
            "  Batch 8500/8989 - Loss: 0.1988\n",
            "  Batch 8600/8989 - Loss: 0.1989\n",
            "  Batch 8700/8989 - Loss: 0.1989\n",
            "  Batch 8800/8989 - Loss: 0.1991\n",
            "  Batch 8900/8989 - Loss: 0.1992\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1993\n",
            "  HR@10: 0.7305\n",
            "  NDCG@10: 0.4487\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 11/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1805\n",
            "  Batch 200/8989 - Loss: 0.1817\n",
            "  Batch 300/8989 - Loss: 0.1816\n",
            "  Batch 400/8989 - Loss: 0.1821\n",
            "  Batch 500/8989 - Loss: 0.1823\n",
            "  Batch 600/8989 - Loss: 0.1833\n",
            "  Batch 700/8989 - Loss: 0.1838\n",
            "  Batch 800/8989 - Loss: 0.1840\n",
            "  Batch 900/8989 - Loss: 0.1835\n",
            "  Batch 1000/8989 - Loss: 0.1837\n",
            "  Batch 1100/8989 - Loss: 0.1837\n",
            "  Batch 1200/8989 - Loss: 0.1837\n",
            "  Batch 1300/8989 - Loss: 0.1836\n",
            "  Batch 1400/8989 - Loss: 0.1837\n",
            "  Batch 1500/8989 - Loss: 0.1837\n",
            "  Batch 1600/8989 - Loss: 0.1841\n",
            "  Batch 1700/8989 - Loss: 0.1842\n",
            "  Batch 1800/8989 - Loss: 0.1842\n",
            "  Batch 1900/8989 - Loss: 0.1845\n",
            "  Batch 2000/8989 - Loss: 0.1846\n",
            "  Batch 2100/8989 - Loss: 0.1850\n",
            "  Batch 2200/8989 - Loss: 0.1854\n",
            "  Batch 2300/8989 - Loss: 0.1856\n",
            "  Batch 2400/8989 - Loss: 0.1857\n",
            "  Batch 2500/8989 - Loss: 0.1857\n",
            "  Batch 2600/8989 - Loss: 0.1858\n",
            "  Batch 2700/8989 - Loss: 0.1858\n",
            "  Batch 2800/8989 - Loss: 0.1859\n",
            "  Batch 2900/8989 - Loss: 0.1862\n",
            "  Batch 3000/8989 - Loss: 0.1863\n",
            "  Batch 3100/8989 - Loss: 0.1864\n",
            "  Batch 3200/8989 - Loss: 0.1866\n",
            "  Batch 3300/8989 - Loss: 0.1869\n",
            "  Batch 3400/8989 - Loss: 0.1872\n",
            "  Batch 3500/8989 - Loss: 0.1875\n",
            "  Batch 3600/8989 - Loss: 0.1877\n",
            "  Batch 3700/8989 - Loss: 0.1879\n",
            "  Batch 3800/8989 - Loss: 0.1881\n",
            "  Batch 3900/8989 - Loss: 0.1882\n",
            "  Batch 4000/8989 - Loss: 0.1884\n",
            "  Batch 4100/8989 - Loss: 0.1885\n",
            "  Batch 4200/8989 - Loss: 0.1888\n",
            "  Batch 4300/8989 - Loss: 0.1889\n",
            "  Batch 4400/8989 - Loss: 0.1890\n",
            "  Batch 4500/8989 - Loss: 0.1891\n",
            "  Batch 4600/8989 - Loss: 0.1892\n",
            "  Batch 4700/8989 - Loss: 0.1894\n",
            "  Batch 4800/8989 - Loss: 0.1895\n",
            "  Batch 4900/8989 - Loss: 0.1897\n",
            "  Batch 5000/8989 - Loss: 0.1898\n",
            "  Batch 5100/8989 - Loss: 0.1898\n",
            "  Batch 5200/8989 - Loss: 0.1900\n",
            "  Batch 5300/8989 - Loss: 0.1902\n",
            "  Batch 5400/8989 - Loss: 0.1904\n",
            "  Batch 5500/8989 - Loss: 0.1904\n",
            "  Batch 5600/8989 - Loss: 0.1906\n",
            "  Batch 5700/8989 - Loss: 0.1907\n",
            "  Batch 5800/8989 - Loss: 0.1907\n",
            "  Batch 5900/8989 - Loss: 0.1908\n",
            "  Batch 6000/8989 - Loss: 0.1909\n",
            "  Batch 6100/8989 - Loss: 0.1909\n",
            "  Batch 6200/8989 - Loss: 0.1911\n",
            "  Batch 6300/8989 - Loss: 0.1913\n",
            "  Batch 6400/8989 - Loss: 0.1914\n",
            "  Batch 6500/8989 - Loss: 0.1915\n",
            "  Batch 6600/8989 - Loss: 0.1916\n",
            "  Batch 6700/8989 - Loss: 0.1918\n",
            "  Batch 6800/8989 - Loss: 0.1919\n",
            "  Batch 6900/8989 - Loss: 0.1920\n",
            "  Batch 7000/8989 - Loss: 0.1921\n",
            "  Batch 7100/8989 - Loss: 0.1922\n",
            "  Batch 7200/8989 - Loss: 0.1923\n",
            "  Batch 7300/8989 - Loss: 0.1925\n",
            "  Batch 7400/8989 - Loss: 0.1926\n",
            "  Batch 7500/8989 - Loss: 0.1927\n",
            "  Batch 7600/8989 - Loss: 0.1927\n",
            "  Batch 7700/8989 - Loss: 0.1928\n",
            "  Batch 7800/8989 - Loss: 0.1929\n",
            "  Batch 7900/8989 - Loss: 0.1929\n",
            "  Batch 8000/8989 - Loss: 0.1931\n",
            "  Batch 8100/8989 - Loss: 0.1932\n",
            "  Batch 8200/8989 - Loss: 0.1933\n",
            "  Batch 8300/8989 - Loss: 0.1934\n",
            "  Batch 8400/8989 - Loss: 0.1935\n",
            "  Batch 8500/8989 - Loss: 0.1936\n",
            "  Batch 8600/8989 - Loss: 0.1938\n",
            "  Batch 8700/8989 - Loss: 0.1939\n",
            "  Batch 8800/8989 - Loss: 0.1940\n",
            "  Batch 8900/8989 - Loss: 0.1941\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1942\n",
            "  HR@10: 0.7254\n",
            "  NDCG@10: 0.4458\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 12/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1758\n",
            "  Batch 200/8989 - Loss: 0.1744\n",
            "  Batch 300/8989 - Loss: 0.1762\n",
            "  Batch 400/8989 - Loss: 0.1768\n",
            "  Batch 500/8989 - Loss: 0.1762\n",
            "  Batch 600/8989 - Loss: 0.1752\n",
            "  Batch 700/8989 - Loss: 0.1752\n",
            "  Batch 800/8989 - Loss: 0.1757\n",
            "  Batch 900/8989 - Loss: 0.1758\n",
            "  Batch 1000/8989 - Loss: 0.1761\n",
            "  Batch 1100/8989 - Loss: 0.1769\n",
            "  Batch 1200/8989 - Loss: 0.1769\n",
            "  Batch 1300/8989 - Loss: 0.1774\n",
            "  Batch 1400/8989 - Loss: 0.1776\n",
            "  Batch 1500/8989 - Loss: 0.1781\n",
            "  Batch 1600/8989 - Loss: 0.1783\n",
            "  Batch 1700/8989 - Loss: 0.1786\n",
            "  Batch 1800/8989 - Loss: 0.1786\n",
            "  Batch 1900/8989 - Loss: 0.1788\n",
            "  Batch 2000/8989 - Loss: 0.1791\n",
            "  Batch 2100/8989 - Loss: 0.1793\n",
            "  Batch 2200/8989 - Loss: 0.1799\n",
            "  Batch 2300/8989 - Loss: 0.1799\n",
            "  Batch 2400/8989 - Loss: 0.1801\n",
            "  Batch 2500/8989 - Loss: 0.1805\n",
            "  Batch 2600/8989 - Loss: 0.1804\n",
            "  Batch 2700/8989 - Loss: 0.1808\n",
            "  Batch 2800/8989 - Loss: 0.1810\n",
            "  Batch 2900/8989 - Loss: 0.1812\n",
            "  Batch 3000/8989 - Loss: 0.1813\n",
            "  Batch 3100/8989 - Loss: 0.1814\n",
            "  Batch 3200/8989 - Loss: 0.1815\n",
            "  Batch 3300/8989 - Loss: 0.1817\n",
            "  Batch 3400/8989 - Loss: 0.1818\n",
            "  Batch 3500/8989 - Loss: 0.1821\n",
            "  Batch 3600/8989 - Loss: 0.1823\n",
            "  Batch 3700/8989 - Loss: 0.1824\n",
            "  Batch 3800/8989 - Loss: 0.1824\n",
            "  Batch 3900/8989 - Loss: 0.1826\n",
            "  Batch 4000/8989 - Loss: 0.1828\n",
            "  Batch 4100/8989 - Loss: 0.1830\n",
            "  Batch 4200/8989 - Loss: 0.1831\n",
            "  Batch 4300/8989 - Loss: 0.1833\n",
            "  Batch 4400/8989 - Loss: 0.1835\n",
            "  Batch 4500/8989 - Loss: 0.1837\n",
            "  Batch 4600/8989 - Loss: 0.1838\n",
            "  Batch 4700/8989 - Loss: 0.1839\n",
            "  Batch 4800/8989 - Loss: 0.1841\n",
            "  Batch 4900/8989 - Loss: 0.1842\n",
            "  Batch 5000/8989 - Loss: 0.1843\n",
            "  Batch 5100/8989 - Loss: 0.1844\n",
            "  Batch 5200/8989 - Loss: 0.1847\n",
            "  Batch 5300/8989 - Loss: 0.1847\n",
            "  Batch 5400/8989 - Loss: 0.1849\n",
            "  Batch 5500/8989 - Loss: 0.1851\n",
            "  Batch 5600/8989 - Loss: 0.1852\n",
            "  Batch 5700/8989 - Loss: 0.1853\n",
            "  Batch 5800/8989 - Loss: 0.1855\n",
            "  Batch 5900/8989 - Loss: 0.1856\n",
            "  Batch 6000/8989 - Loss: 0.1857\n",
            "  Batch 6100/8989 - Loss: 0.1859\n",
            "  Batch 6200/8989 - Loss: 0.1860\n",
            "  Batch 6300/8989 - Loss: 0.1861\n",
            "  Batch 6400/8989 - Loss: 0.1862\n",
            "  Batch 6500/8989 - Loss: 0.1864\n",
            "  Batch 6600/8989 - Loss: 0.1866\n",
            "  Batch 6700/8989 - Loss: 0.1867\n",
            "  Batch 6800/8989 - Loss: 0.1868\n",
            "  Batch 6900/8989 - Loss: 0.1868\n",
            "  Batch 7000/8989 - Loss: 0.1870\n",
            "  Batch 7100/8989 - Loss: 0.1871\n",
            "  Batch 7200/8989 - Loss: 0.1873\n",
            "  Batch 7300/8989 - Loss: 0.1875\n",
            "  Batch 7400/8989 - Loss: 0.1875\n",
            "  Batch 7500/8989 - Loss: 0.1878\n",
            "  Batch 7600/8989 - Loss: 0.1879\n",
            "  Batch 7700/8989 - Loss: 0.1881\n",
            "  Batch 7800/8989 - Loss: 0.1882\n",
            "  Batch 7900/8989 - Loss: 0.1883\n",
            "  Batch 8000/8989 - Loss: 0.1884\n",
            "  Batch 8100/8989 - Loss: 0.1886\n",
            "  Batch 8200/8989 - Loss: 0.1886\n",
            "  Batch 8300/8989 - Loss: 0.1887\n",
            "  Batch 8400/8989 - Loss: 0.1888\n",
            "  Batch 8500/8989 - Loss: 0.1890\n",
            "  Batch 8600/8989 - Loss: 0.1890\n",
            "  Batch 8700/8989 - Loss: 0.1891\n",
            "  Batch 8800/8989 - Loss: 0.1893\n",
            "  Batch 8900/8989 - Loss: 0.1894\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1895\n",
            "  HR@10: 0.7224\n",
            "  NDCG@10: 0.4440\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 13/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1736\n",
            "  Batch 200/8989 - Loss: 0.1743\n",
            "  Batch 300/8989 - Loss: 0.1737\n",
            "  Batch 400/8989 - Loss: 0.1739\n",
            "  Batch 500/8989 - Loss: 0.1738\n",
            "  Batch 600/8989 - Loss: 0.1740\n",
            "  Batch 700/8989 - Loss: 0.1742\n",
            "  Batch 800/8989 - Loss: 0.1745\n",
            "  Batch 900/8989 - Loss: 0.1744\n",
            "  Batch 1000/8989 - Loss: 0.1742\n",
            "  Batch 1100/8989 - Loss: 0.1741\n",
            "  Batch 1200/8989 - Loss: 0.1744\n",
            "  Batch 1300/8989 - Loss: 0.1746\n",
            "  Batch 1400/8989 - Loss: 0.1748\n",
            "  Batch 1500/8989 - Loss: 0.1746\n",
            "  Batch 1600/8989 - Loss: 0.1749\n",
            "  Batch 1700/8989 - Loss: 0.1753\n",
            "  Batch 1800/8989 - Loss: 0.1754\n",
            "  Batch 1900/8989 - Loss: 0.1755\n",
            "  Batch 2000/8989 - Loss: 0.1756\n",
            "  Batch 2100/8989 - Loss: 0.1759\n",
            "  Batch 2200/8989 - Loss: 0.1761\n",
            "  Batch 2300/8989 - Loss: 0.1763\n",
            "  Batch 2400/8989 - Loss: 0.1765\n",
            "  Batch 2500/8989 - Loss: 0.1767\n",
            "  Batch 2600/8989 - Loss: 0.1766\n",
            "  Batch 2700/8989 - Loss: 0.1768\n",
            "  Batch 2800/8989 - Loss: 0.1771\n",
            "  Batch 2900/8989 - Loss: 0.1772\n",
            "  Batch 3000/8989 - Loss: 0.1773\n",
            "  Batch 3100/8989 - Loss: 0.1776\n",
            "  Batch 3200/8989 - Loss: 0.1777\n",
            "  Batch 3300/8989 - Loss: 0.1780\n",
            "  Batch 3400/8989 - Loss: 0.1780\n",
            "  Batch 3500/8989 - Loss: 0.1782\n",
            "  Batch 3600/8989 - Loss: 0.1783\n",
            "  Batch 3700/8989 - Loss: 0.1786\n",
            "  Batch 3800/8989 - Loss: 0.1788\n",
            "  Batch 3900/8989 - Loss: 0.1789\n",
            "  Batch 4000/8989 - Loss: 0.1790\n",
            "  Batch 4100/8989 - Loss: 0.1792\n",
            "  Batch 4200/8989 - Loss: 0.1793\n",
            "  Batch 4300/8989 - Loss: 0.1793\n",
            "  Batch 4400/8989 - Loss: 0.1795\n",
            "  Batch 4500/8989 - Loss: 0.1796\n",
            "  Batch 4600/8989 - Loss: 0.1798\n",
            "  Batch 4700/8989 - Loss: 0.1800\n",
            "  Batch 4800/8989 - Loss: 0.1801\n",
            "  Batch 4900/8989 - Loss: 0.1803\n",
            "  Batch 5000/8989 - Loss: 0.1804\n",
            "  Batch 5100/8989 - Loss: 0.1806\n",
            "  Batch 5200/8989 - Loss: 0.1807\n",
            "  Batch 5300/8989 - Loss: 0.1808\n",
            "  Batch 5400/8989 - Loss: 0.1810\n",
            "  Batch 5500/8989 - Loss: 0.1811\n",
            "  Batch 5600/8989 - Loss: 0.1812\n",
            "  Batch 5700/8989 - Loss: 0.1814\n",
            "  Batch 5800/8989 - Loss: 0.1816\n",
            "  Batch 5900/8989 - Loss: 0.1818\n",
            "  Batch 6000/8989 - Loss: 0.1819\n",
            "  Batch 6100/8989 - Loss: 0.1820\n",
            "  Batch 6200/8989 - Loss: 0.1822\n",
            "  Batch 6300/8989 - Loss: 0.1823\n",
            "  Batch 6400/8989 - Loss: 0.1824\n",
            "  Batch 6500/8989 - Loss: 0.1825\n",
            "  Batch 6600/8989 - Loss: 0.1826\n",
            "  Batch 6700/8989 - Loss: 0.1828\n",
            "  Batch 6800/8989 - Loss: 0.1829\n",
            "  Batch 6900/8989 - Loss: 0.1830\n",
            "  Batch 7000/8989 - Loss: 0.1831\n",
            "  Batch 7100/8989 - Loss: 0.1832\n",
            "  Batch 7200/8989 - Loss: 0.1833\n",
            "  Batch 7300/8989 - Loss: 0.1834\n",
            "  Batch 7400/8989 - Loss: 0.1836\n",
            "  Batch 7500/8989 - Loss: 0.1838\n",
            "  Batch 7600/8989 - Loss: 0.1839\n",
            "  Batch 7700/8989 - Loss: 0.1840\n",
            "  Batch 7800/8989 - Loss: 0.1840\n",
            "  Batch 7900/8989 - Loss: 0.1842\n",
            "  Batch 8000/8989 - Loss: 0.1844\n",
            "  Batch 8100/8989 - Loss: 0.1845\n",
            "  Batch 8200/8989 - Loss: 0.1846\n",
            "  Batch 8300/8989 - Loss: 0.1846\n",
            "  Batch 8400/8989 - Loss: 0.1848\n",
            "  Batch 8500/8989 - Loss: 0.1849\n",
            "  Batch 8600/8989 - Loss: 0.1850\n",
            "  Batch 8700/8989 - Loss: 0.1851\n",
            "  Batch 8800/8989 - Loss: 0.1852\n",
            "  Batch 8900/8989 - Loss: 0.1853\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1854\n",
            "  HR@10: 0.7210\n",
            "  NDCG@10: 0.4408\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 14/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1635\n",
            "  Batch 200/8989 - Loss: 0.1666\n",
            "  Batch 300/8989 - Loss: 0.1675\n",
            "  Batch 400/8989 - Loss: 0.1681\n",
            "  Batch 500/8989 - Loss: 0.1686\n",
            "  Batch 600/8989 - Loss: 0.1683\n",
            "  Batch 700/8989 - Loss: 0.1687\n",
            "  Batch 800/8989 - Loss: 0.1689\n",
            "  Batch 900/8989 - Loss: 0.1686\n",
            "  Batch 1000/8989 - Loss: 0.1689\n",
            "  Batch 1100/8989 - Loss: 0.1693\n",
            "  Batch 1200/8989 - Loss: 0.1696\n",
            "  Batch 1300/8989 - Loss: 0.1701\n",
            "  Batch 1400/8989 - Loss: 0.1703\n",
            "  Batch 1500/8989 - Loss: 0.1705\n",
            "  Batch 1600/8989 - Loss: 0.1708\n",
            "  Batch 1700/8989 - Loss: 0.1708\n",
            "  Batch 1800/8989 - Loss: 0.1709\n",
            "  Batch 1900/8989 - Loss: 0.1711\n",
            "  Batch 2000/8989 - Loss: 0.1711\n",
            "  Batch 2100/8989 - Loss: 0.1714\n",
            "  Batch 2200/8989 - Loss: 0.1716\n",
            "  Batch 2300/8989 - Loss: 0.1720\n",
            "  Batch 2400/8989 - Loss: 0.1722\n",
            "  Batch 2500/8989 - Loss: 0.1723\n",
            "  Batch 2600/8989 - Loss: 0.1726\n",
            "  Batch 2700/8989 - Loss: 0.1728\n",
            "  Batch 2800/8989 - Loss: 0.1730\n",
            "  Batch 2900/8989 - Loss: 0.1732\n",
            "  Batch 3000/8989 - Loss: 0.1735\n",
            "  Batch 3100/8989 - Loss: 0.1737\n",
            "  Batch 3200/8989 - Loss: 0.1738\n",
            "  Batch 3300/8989 - Loss: 0.1739\n",
            "  Batch 3400/8989 - Loss: 0.1742\n",
            "  Batch 3500/8989 - Loss: 0.1742\n",
            "  Batch 3600/8989 - Loss: 0.1744\n",
            "  Batch 3700/8989 - Loss: 0.1745\n",
            "  Batch 3800/8989 - Loss: 0.1746\n",
            "  Batch 3900/8989 - Loss: 0.1749\n",
            "  Batch 4000/8989 - Loss: 0.1749\n",
            "  Batch 4100/8989 - Loss: 0.1750\n",
            "  Batch 4200/8989 - Loss: 0.1753\n",
            "  Batch 4300/8989 - Loss: 0.1755\n",
            "  Batch 4400/8989 - Loss: 0.1756\n",
            "  Batch 4500/8989 - Loss: 0.1757\n",
            "  Batch 4600/8989 - Loss: 0.1759\n",
            "  Batch 4700/8989 - Loss: 0.1761\n",
            "  Batch 4800/8989 - Loss: 0.1762\n",
            "  Batch 4900/8989 - Loss: 0.1763\n",
            "  Batch 5000/8989 - Loss: 0.1764\n",
            "  Batch 5100/8989 - Loss: 0.1765\n",
            "  Batch 5200/8989 - Loss: 0.1767\n",
            "  Batch 5300/8989 - Loss: 0.1768\n",
            "  Batch 5400/8989 - Loss: 0.1770\n",
            "  Batch 5500/8989 - Loss: 0.1771\n",
            "  Batch 5600/8989 - Loss: 0.1773\n",
            "  Batch 5700/8989 - Loss: 0.1774\n",
            "  Batch 5800/8989 - Loss: 0.1775\n",
            "  Batch 5900/8989 - Loss: 0.1776\n",
            "  Batch 6000/8989 - Loss: 0.1777\n",
            "  Batch 6100/8989 - Loss: 0.1778\n",
            "  Batch 6200/8989 - Loss: 0.1779\n",
            "  Batch 6300/8989 - Loss: 0.1780\n",
            "  Batch 6400/8989 - Loss: 0.1782\n",
            "  Batch 6500/8989 - Loss: 0.1784\n",
            "  Batch 6600/8989 - Loss: 0.1785\n",
            "  Batch 6700/8989 - Loss: 0.1787\n",
            "  Batch 6800/8989 - Loss: 0.1788\n",
            "  Batch 6900/8989 - Loss: 0.1789\n",
            "  Batch 7000/8989 - Loss: 0.1790\n",
            "  Batch 7100/8989 - Loss: 0.1790\n",
            "  Batch 7200/8989 - Loss: 0.1792\n",
            "  Batch 7300/8989 - Loss: 0.1794\n",
            "  Batch 7400/8989 - Loss: 0.1796\n",
            "  Batch 7500/8989 - Loss: 0.1797\n",
            "  Batch 7600/8989 - Loss: 0.1797\n",
            "  Batch 7700/8989 - Loss: 0.1799\n",
            "  Batch 7800/8989 - Loss: 0.1800\n",
            "  Batch 7900/8989 - Loss: 0.1801\n",
            "  Batch 8000/8989 - Loss: 0.1802\n",
            "  Batch 8100/8989 - Loss: 0.1803\n",
            "  Batch 8200/8989 - Loss: 0.1804\n",
            "  Batch 8300/8989 - Loss: 0.1805\n",
            "  Batch 8400/8989 - Loss: 0.1807\n",
            "  Batch 8500/8989 - Loss: 0.1808\n",
            "  Batch 8600/8989 - Loss: 0.1809\n",
            "  Batch 8700/8989 - Loss: 0.1810\n",
            "  Batch 8800/8989 - Loss: 0.1811\n",
            "  Batch 8900/8989 - Loss: 0.1812\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1813\n",
            "  HR@10: 0.7209\n",
            "  NDCG@10: 0.4430\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 15/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1683\n",
            "  Batch 200/8989 - Loss: 0.1630\n",
            "  Batch 300/8989 - Loss: 0.1641\n",
            "  Batch 400/8989 - Loss: 0.1638\n",
            "  Batch 500/8989 - Loss: 0.1644\n",
            "  Batch 600/8989 - Loss: 0.1649\n",
            "  Batch 700/8989 - Loss: 0.1657\n",
            "  Batch 800/8989 - Loss: 0.1659\n",
            "  Batch 900/8989 - Loss: 0.1658\n",
            "  Batch 1000/8989 - Loss: 0.1662\n",
            "  Batch 1100/8989 - Loss: 0.1663\n",
            "  Batch 1200/8989 - Loss: 0.1665\n",
            "  Batch 1300/8989 - Loss: 0.1668\n",
            "  Batch 1400/8989 - Loss: 0.1671\n",
            "  Batch 1500/8989 - Loss: 0.1671\n",
            "  Batch 1600/8989 - Loss: 0.1673\n",
            "  Batch 1700/8989 - Loss: 0.1674\n",
            "  Batch 1800/8989 - Loss: 0.1675\n",
            "  Batch 1900/8989 - Loss: 0.1677\n",
            "  Batch 2000/8989 - Loss: 0.1677\n",
            "  Batch 2100/8989 - Loss: 0.1679\n",
            "  Batch 2200/8989 - Loss: 0.1680\n",
            "  Batch 2300/8989 - Loss: 0.1682\n",
            "  Batch 2400/8989 - Loss: 0.1683\n",
            "  Batch 2500/8989 - Loss: 0.1686\n",
            "  Batch 2600/8989 - Loss: 0.1689\n",
            "  Batch 2700/8989 - Loss: 0.1691\n",
            "  Batch 2800/8989 - Loss: 0.1692\n",
            "  Batch 2900/8989 - Loss: 0.1693\n",
            "  Batch 3000/8989 - Loss: 0.1694\n",
            "  Batch 3100/8989 - Loss: 0.1696\n",
            "  Batch 3200/8989 - Loss: 0.1697\n",
            "  Batch 3300/8989 - Loss: 0.1698\n",
            "  Batch 3400/8989 - Loss: 0.1701\n",
            "  Batch 3500/8989 - Loss: 0.1703\n",
            "  Batch 3600/8989 - Loss: 0.1704\n",
            "  Batch 3700/8989 - Loss: 0.1705\n",
            "  Batch 3800/8989 - Loss: 0.1707\n",
            "  Batch 3900/8989 - Loss: 0.1708\n",
            "  Batch 4000/8989 - Loss: 0.1708\n",
            "  Batch 4100/8989 - Loss: 0.1710\n",
            "  Batch 4200/8989 - Loss: 0.1711\n",
            "  Batch 4300/8989 - Loss: 0.1713\n",
            "  Batch 4400/8989 - Loss: 0.1716\n",
            "  Batch 4500/8989 - Loss: 0.1717\n",
            "  Batch 4600/8989 - Loss: 0.1719\n",
            "  Batch 4700/8989 - Loss: 0.1720\n",
            "  Batch 4800/8989 - Loss: 0.1723\n",
            "  Batch 4900/8989 - Loss: 0.1725\n",
            "  Batch 5000/8989 - Loss: 0.1727\n",
            "  Batch 5100/8989 - Loss: 0.1728\n",
            "  Batch 5200/8989 - Loss: 0.1729\n",
            "  Batch 5300/8989 - Loss: 0.1730\n",
            "  Batch 5400/8989 - Loss: 0.1732\n",
            "  Batch 5500/8989 - Loss: 0.1734\n",
            "  Batch 5600/8989 - Loss: 0.1735\n",
            "  Batch 5700/8989 - Loss: 0.1736\n",
            "  Batch 5800/8989 - Loss: 0.1737\n",
            "  Batch 5900/8989 - Loss: 0.1738\n",
            "  Batch 6000/8989 - Loss: 0.1740\n",
            "  Batch 6100/8989 - Loss: 0.1740\n",
            "  Batch 6200/8989 - Loss: 0.1742\n",
            "  Batch 6300/8989 - Loss: 0.1743\n",
            "  Batch 6400/8989 - Loss: 0.1744\n",
            "  Batch 6500/8989 - Loss: 0.1746\n",
            "  Batch 6600/8989 - Loss: 0.1747\n",
            "  Batch 6700/8989 - Loss: 0.1748\n",
            "  Batch 6800/8989 - Loss: 0.1749\n",
            "  Batch 6900/8989 - Loss: 0.1751\n",
            "  Batch 7000/8989 - Loss: 0.1752\n",
            "  Batch 7100/8989 - Loss: 0.1753\n",
            "  Batch 7200/8989 - Loss: 0.1754\n",
            "  Batch 7300/8989 - Loss: 0.1756\n",
            "  Batch 7400/8989 - Loss: 0.1757\n",
            "  Batch 7500/8989 - Loss: 0.1758\n",
            "  Batch 7600/8989 - Loss: 0.1759\n",
            "  Batch 7700/8989 - Loss: 0.1761\n",
            "  Batch 7800/8989 - Loss: 0.1763\n",
            "  Batch 7900/8989 - Loss: 0.1764\n",
            "  Batch 8000/8989 - Loss: 0.1765\n",
            "  Batch 8100/8989 - Loss: 0.1765\n",
            "  Batch 8200/8989 - Loss: 0.1766\n",
            "  Batch 8300/8989 - Loss: 0.1767\n",
            "  Batch 8400/8989 - Loss: 0.1768\n",
            "  Batch 8500/8989 - Loss: 0.1769\n",
            "  Batch 8600/8989 - Loss: 0.1770\n",
            "  Batch 8700/8989 - Loss: 0.1771\n",
            "  Batch 8800/8989 - Loss: 0.1772\n",
            "  Batch 8900/8989 - Loss: 0.1773\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1774\n",
            "  HR@10: 0.7180\n",
            "  NDCG@10: 0.4410\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 16/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1595\n",
            "  Batch 200/8989 - Loss: 0.1609\n",
            "  Batch 300/8989 - Loss: 0.1618\n",
            "  Batch 400/8989 - Loss: 0.1625\n",
            "  Batch 500/8989 - Loss: 0.1623\n",
            "  Batch 600/8989 - Loss: 0.1618\n",
            "  Batch 700/8989 - Loss: 0.1615\n",
            "  Batch 800/8989 - Loss: 0.1618\n",
            "  Batch 900/8989 - Loss: 0.1615\n",
            "  Batch 1000/8989 - Loss: 0.1619\n",
            "  Batch 1100/8989 - Loss: 0.1621\n",
            "  Batch 1200/8989 - Loss: 0.1622\n",
            "  Batch 1300/8989 - Loss: 0.1626\n",
            "  Batch 1400/8989 - Loss: 0.1629\n",
            "  Batch 1500/8989 - Loss: 0.1632\n",
            "  Batch 1600/8989 - Loss: 0.1632\n",
            "  Batch 1700/8989 - Loss: 0.1634\n",
            "  Batch 1800/8989 - Loss: 0.1639\n",
            "  Batch 1900/8989 - Loss: 0.1642\n",
            "  Batch 2000/8989 - Loss: 0.1641\n",
            "  Batch 2100/8989 - Loss: 0.1642\n",
            "  Batch 2200/8989 - Loss: 0.1642\n",
            "  Batch 2300/8989 - Loss: 0.1643\n",
            "  Batch 2400/8989 - Loss: 0.1642\n",
            "  Batch 2500/8989 - Loss: 0.1644\n",
            "  Batch 2600/8989 - Loss: 0.1647\n",
            "  Batch 2700/8989 - Loss: 0.1650\n",
            "  Batch 2800/8989 - Loss: 0.1652\n",
            "  Batch 2900/8989 - Loss: 0.1654\n",
            "  Batch 3000/8989 - Loss: 0.1655\n",
            "  Batch 3100/8989 - Loss: 0.1655\n",
            "  Batch 3200/8989 - Loss: 0.1657\n",
            "  Batch 3300/8989 - Loss: 0.1660\n",
            "  Batch 3400/8989 - Loss: 0.1660\n",
            "  Batch 3500/8989 - Loss: 0.1663\n",
            "  Batch 3600/8989 - Loss: 0.1665\n",
            "  Batch 3700/8989 - Loss: 0.1665\n",
            "  Batch 3800/8989 - Loss: 0.1667\n",
            "  Batch 3900/8989 - Loss: 0.1667\n",
            "  Batch 4000/8989 - Loss: 0.1668\n",
            "  Batch 4100/8989 - Loss: 0.1671\n",
            "  Batch 4200/8989 - Loss: 0.1672\n",
            "  Batch 4300/8989 - Loss: 0.1674\n",
            "  Batch 4400/8989 - Loss: 0.1675\n",
            "  Batch 4500/8989 - Loss: 0.1677\n",
            "  Batch 4600/8989 - Loss: 0.1679\n",
            "  Batch 4700/8989 - Loss: 0.1680\n",
            "  Batch 4800/8989 - Loss: 0.1682\n",
            "  Batch 4900/8989 - Loss: 0.1684\n",
            "  Batch 5000/8989 - Loss: 0.1686\n",
            "  Batch 5100/8989 - Loss: 0.1687\n",
            "  Batch 5200/8989 - Loss: 0.1687\n",
            "  Batch 5300/8989 - Loss: 0.1689\n",
            "  Batch 5400/8989 - Loss: 0.1690\n",
            "  Batch 5500/8989 - Loss: 0.1691\n",
            "  Batch 5600/8989 - Loss: 0.1692\n",
            "  Batch 5700/8989 - Loss: 0.1692\n",
            "  Batch 5800/8989 - Loss: 0.1693\n",
            "  Batch 5900/8989 - Loss: 0.1695\n",
            "  Batch 6000/8989 - Loss: 0.1696\n",
            "  Batch 6100/8989 - Loss: 0.1696\n",
            "  Batch 6200/8989 - Loss: 0.1698\n",
            "  Batch 6300/8989 - Loss: 0.1699\n",
            "  Batch 6400/8989 - Loss: 0.1700\n",
            "  Batch 6500/8989 - Loss: 0.1702\n",
            "  Batch 6600/8989 - Loss: 0.1703\n",
            "  Batch 6700/8989 - Loss: 0.1705\n",
            "  Batch 6800/8989 - Loss: 0.1706\n",
            "  Batch 6900/8989 - Loss: 0.1707\n",
            "  Batch 7000/8989 - Loss: 0.1708\n",
            "  Batch 7100/8989 - Loss: 0.1710\n",
            "  Batch 7200/8989 - Loss: 0.1711\n",
            "  Batch 7300/8989 - Loss: 0.1712\n",
            "  Batch 7400/8989 - Loss: 0.1713\n",
            "  Batch 7500/8989 - Loss: 0.1715\n",
            "  Batch 7600/8989 - Loss: 0.1716\n",
            "  Batch 7700/8989 - Loss: 0.1717\n",
            "  Batch 7800/8989 - Loss: 0.1718\n",
            "  Batch 7900/8989 - Loss: 0.1719\n",
            "  Batch 8000/8989 - Loss: 0.1721\n",
            "  Batch 8100/8989 - Loss: 0.1722\n",
            "  Batch 8200/8989 - Loss: 0.1724\n",
            "  Batch 8300/8989 - Loss: 0.1725\n",
            "  Batch 8400/8989 - Loss: 0.1725\n",
            "  Batch 8500/8989 - Loss: 0.1727\n",
            "  Batch 8600/8989 - Loss: 0.1728\n",
            "  Batch 8700/8989 - Loss: 0.1729\n",
            "  Batch 8800/8989 - Loss: 0.1731\n",
            "  Batch 8900/8989 - Loss: 0.1731\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1732\n",
            "  HR@10: 0.7159\n",
            "  NDCG@10: 0.4370\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 17/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1596\n",
            "  Batch 200/8989 - Loss: 0.1589\n",
            "  Batch 300/8989 - Loss: 0.1581\n",
            "  Batch 400/8989 - Loss: 0.1579\n",
            "  Batch 500/8989 - Loss: 0.1577\n",
            "  Batch 600/8989 - Loss: 0.1564\n",
            "  Batch 700/8989 - Loss: 0.1568\n",
            "  Batch 800/8989 - Loss: 0.1573\n",
            "  Batch 900/8989 - Loss: 0.1578\n",
            "  Batch 1000/8989 - Loss: 0.1581\n",
            "  Batch 1100/8989 - Loss: 0.1582\n",
            "  Batch 1200/8989 - Loss: 0.1583\n",
            "  Batch 1300/8989 - Loss: 0.1586\n",
            "  Batch 1400/8989 - Loss: 0.1589\n",
            "  Batch 1500/8989 - Loss: 0.1591\n",
            "  Batch 1600/8989 - Loss: 0.1591\n",
            "  Batch 1700/8989 - Loss: 0.1590\n",
            "  Batch 1800/8989 - Loss: 0.1593\n",
            "  Batch 1900/8989 - Loss: 0.1593\n",
            "  Batch 2000/8989 - Loss: 0.1594\n",
            "  Batch 2100/8989 - Loss: 0.1596\n",
            "  Batch 2200/8989 - Loss: 0.1597\n",
            "  Batch 2300/8989 - Loss: 0.1598\n",
            "  Batch 2400/8989 - Loss: 0.1600\n",
            "  Batch 2500/8989 - Loss: 0.1602\n",
            "  Batch 2600/8989 - Loss: 0.1605\n",
            "  Batch 2700/8989 - Loss: 0.1607\n",
            "  Batch 2800/8989 - Loss: 0.1610\n",
            "  Batch 2900/8989 - Loss: 0.1613\n",
            "  Batch 3000/8989 - Loss: 0.1616\n",
            "  Batch 3100/8989 - Loss: 0.1618\n",
            "  Batch 3200/8989 - Loss: 0.1619\n",
            "  Batch 3300/8989 - Loss: 0.1621\n",
            "  Batch 3400/8989 - Loss: 0.1622\n",
            "  Batch 3500/8989 - Loss: 0.1626\n",
            "  Batch 3600/8989 - Loss: 0.1627\n",
            "  Batch 3700/8989 - Loss: 0.1629\n",
            "  Batch 3800/8989 - Loss: 0.1629\n",
            "  Batch 3900/8989 - Loss: 0.1630\n",
            "  Batch 4000/8989 - Loss: 0.1631\n",
            "  Batch 4100/8989 - Loss: 0.1633\n",
            "  Batch 4200/8989 - Loss: 0.1634\n",
            "  Batch 4300/8989 - Loss: 0.1636\n",
            "  Batch 4400/8989 - Loss: 0.1638\n",
            "  Batch 4500/8989 - Loss: 0.1640\n",
            "  Batch 4600/8989 - Loss: 0.1643\n",
            "  Batch 4700/8989 - Loss: 0.1644\n",
            "  Batch 4800/8989 - Loss: 0.1645\n",
            "  Batch 4900/8989 - Loss: 0.1647\n",
            "  Batch 5000/8989 - Loss: 0.1650\n",
            "  Batch 5100/8989 - Loss: 0.1653\n",
            "  Batch 5200/8989 - Loss: 0.1654\n",
            "  Batch 5300/8989 - Loss: 0.1654\n",
            "  Batch 5400/8989 - Loss: 0.1655\n",
            "  Batch 5500/8989 - Loss: 0.1655\n",
            "  Batch 5600/8989 - Loss: 0.1657\n",
            "  Batch 5700/8989 - Loss: 0.1659\n",
            "  Batch 5800/8989 - Loss: 0.1660\n",
            "  Batch 5900/8989 - Loss: 0.1661\n",
            "  Batch 6000/8989 - Loss: 0.1662\n",
            "  Batch 6100/8989 - Loss: 0.1664\n",
            "  Batch 6200/8989 - Loss: 0.1664\n",
            "  Batch 6300/8989 - Loss: 0.1666\n",
            "  Batch 6400/8989 - Loss: 0.1667\n",
            "  Batch 6500/8989 - Loss: 0.1669\n",
            "  Batch 6600/8989 - Loss: 0.1670\n",
            "  Batch 6700/8989 - Loss: 0.1672\n",
            "  Batch 6800/8989 - Loss: 0.1673\n",
            "  Batch 6900/8989 - Loss: 0.1675\n",
            "  Batch 7000/8989 - Loss: 0.1676\n",
            "  Batch 7100/8989 - Loss: 0.1677\n",
            "  Batch 7200/8989 - Loss: 0.1678\n",
            "  Batch 7300/8989 - Loss: 0.1680\n",
            "  Batch 7400/8989 - Loss: 0.1681\n",
            "  Batch 7500/8989 - Loss: 0.1682\n",
            "  Batch 7600/8989 - Loss: 0.1683\n",
            "  Batch 7700/8989 - Loss: 0.1684\n",
            "  Batch 7800/8989 - Loss: 0.1685\n",
            "  Batch 7900/8989 - Loss: 0.1686\n",
            "  Batch 8000/8989 - Loss: 0.1687\n",
            "  Batch 8100/8989 - Loss: 0.1688\n",
            "  Batch 8200/8989 - Loss: 0.1689\n",
            "  Batch 8300/8989 - Loss: 0.1690\n",
            "  Batch 8400/8989 - Loss: 0.1692\n",
            "  Batch 8500/8989 - Loss: 0.1692\n",
            "  Batch 8600/8989 - Loss: 0.1693\n",
            "  Batch 8700/8989 - Loss: 0.1695\n",
            "  Batch 8800/8989 - Loss: 0.1696\n",
            "  Batch 8900/8989 - Loss: 0.1697\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1698\n",
            "  HR@10: 0.7137\n",
            "  NDCG@10: 0.4348\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 18/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1532\n",
            "  Batch 200/8989 - Loss: 0.1553\n",
            "  Batch 300/8989 - Loss: 0.1542\n",
            "  Batch 400/8989 - Loss: 0.1542\n",
            "  Batch 500/8989 - Loss: 0.1543\n",
            "  Batch 600/8989 - Loss: 0.1534\n",
            "  Batch 700/8989 - Loss: 0.1539\n",
            "  Batch 800/8989 - Loss: 0.1539\n",
            "  Batch 900/8989 - Loss: 0.1542\n",
            "  Batch 1000/8989 - Loss: 0.1549\n",
            "  Batch 1100/8989 - Loss: 0.1549\n",
            "  Batch 1200/8989 - Loss: 0.1552\n",
            "  Batch 1300/8989 - Loss: 0.1557\n",
            "  Batch 1400/8989 - Loss: 0.1558\n",
            "  Batch 1500/8989 - Loss: 0.1562\n",
            "  Batch 1600/8989 - Loss: 0.1565\n",
            "  Batch 1700/8989 - Loss: 0.1569\n",
            "  Batch 1800/8989 - Loss: 0.1572\n",
            "  Batch 1900/8989 - Loss: 0.1574\n",
            "  Batch 2000/8989 - Loss: 0.1576\n",
            "  Batch 2100/8989 - Loss: 0.1577\n",
            "  Batch 2200/8989 - Loss: 0.1580\n",
            "  Batch 2300/8989 - Loss: 0.1580\n",
            "  Batch 2400/8989 - Loss: 0.1581\n",
            "  Batch 2500/8989 - Loss: 0.1581\n",
            "  Batch 2600/8989 - Loss: 0.1582\n",
            "  Batch 2700/8989 - Loss: 0.1583\n",
            "  Batch 2800/8989 - Loss: 0.1583\n",
            "  Batch 2900/8989 - Loss: 0.1585\n",
            "  Batch 3000/8989 - Loss: 0.1588\n",
            "  Batch 3100/8989 - Loss: 0.1591\n",
            "  Batch 3200/8989 - Loss: 0.1593\n",
            "  Batch 3300/8989 - Loss: 0.1595\n",
            "  Batch 3400/8989 - Loss: 0.1596\n",
            "  Batch 3500/8989 - Loss: 0.1597\n",
            "  Batch 3600/8989 - Loss: 0.1598\n",
            "  Batch 3700/8989 - Loss: 0.1600\n",
            "  Batch 3800/8989 - Loss: 0.1601\n",
            "  Batch 3900/8989 - Loss: 0.1603\n",
            "  Batch 4000/8989 - Loss: 0.1603\n",
            "  Batch 4100/8989 - Loss: 0.1603\n",
            "  Batch 4200/8989 - Loss: 0.1603\n",
            "  Batch 4300/8989 - Loss: 0.1605\n",
            "  Batch 4400/8989 - Loss: 0.1607\n",
            "  Batch 4500/8989 - Loss: 0.1608\n",
            "  Batch 4600/8989 - Loss: 0.1609\n",
            "  Batch 4700/8989 - Loss: 0.1612\n",
            "  Batch 4800/8989 - Loss: 0.1613\n",
            "  Batch 4900/8989 - Loss: 0.1615\n",
            "  Batch 5000/8989 - Loss: 0.1617\n",
            "  Batch 5100/8989 - Loss: 0.1617\n",
            "  Batch 5200/8989 - Loss: 0.1619\n",
            "  Batch 5300/8989 - Loss: 0.1620\n",
            "  Batch 5400/8989 - Loss: 0.1621\n",
            "  Batch 5500/8989 - Loss: 0.1622\n",
            "  Batch 5600/8989 - Loss: 0.1624\n",
            "  Batch 5700/8989 - Loss: 0.1625\n",
            "  Batch 5800/8989 - Loss: 0.1627\n",
            "  Batch 5900/8989 - Loss: 0.1629\n",
            "  Batch 6000/8989 - Loss: 0.1629\n",
            "  Batch 6100/8989 - Loss: 0.1631\n",
            "  Batch 6200/8989 - Loss: 0.1633\n",
            "  Batch 6300/8989 - Loss: 0.1634\n",
            "  Batch 6400/8989 - Loss: 0.1636\n",
            "  Batch 6500/8989 - Loss: 0.1636\n",
            "  Batch 6600/8989 - Loss: 0.1638\n",
            "  Batch 6700/8989 - Loss: 0.1638\n",
            "  Batch 6800/8989 - Loss: 0.1640\n",
            "  Batch 6900/8989 - Loss: 0.1641\n",
            "  Batch 7000/8989 - Loss: 0.1643\n",
            "  Batch 7100/8989 - Loss: 0.1644\n",
            "  Batch 7200/8989 - Loss: 0.1645\n",
            "  Batch 7300/8989 - Loss: 0.1646\n",
            "  Batch 7400/8989 - Loss: 0.1647\n",
            "  Batch 7500/8989 - Loss: 0.1648\n",
            "  Batch 7600/8989 - Loss: 0.1650\n",
            "  Batch 7700/8989 - Loss: 0.1651\n",
            "  Batch 7800/8989 - Loss: 0.1651\n",
            "  Batch 7900/8989 - Loss: 0.1651\n",
            "  Batch 8000/8989 - Loss: 0.1653\n",
            "  Batch 8100/8989 - Loss: 0.1654\n",
            "  Batch 8200/8989 - Loss: 0.1656\n",
            "  Batch 8300/8989 - Loss: 0.1657\n",
            "  Batch 8400/8989 - Loss: 0.1657\n",
            "  Batch 8500/8989 - Loss: 0.1658\n",
            "  Batch 8600/8989 - Loss: 0.1659\n",
            "  Batch 8700/8989 - Loss: 0.1661\n",
            "  Batch 8800/8989 - Loss: 0.1661\n",
            "  Batch 8900/8989 - Loss: 0.1662\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:39\n",
            "  Loss: 0.1663\n",
            "  HR@10: 0.7110\n",
            "  NDCG@10: 0.4321\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 19/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1514\n",
            "  Batch 200/8989 - Loss: 0.1498\n",
            "  Batch 300/8989 - Loss: 0.1516\n",
            "  Batch 400/8989 - Loss: 0.1520\n",
            "  Batch 500/8989 - Loss: 0.1513\n",
            "  Batch 600/8989 - Loss: 0.1513\n",
            "  Batch 700/8989 - Loss: 0.1514\n",
            "  Batch 800/8989 - Loss: 0.1518\n",
            "  Batch 900/8989 - Loss: 0.1522\n",
            "  Batch 1000/8989 - Loss: 0.1527\n",
            "  Batch 1100/8989 - Loss: 0.1522\n",
            "  Batch 1200/8989 - Loss: 0.1528\n",
            "  Batch 1300/8989 - Loss: 0.1528\n",
            "  Batch 1400/8989 - Loss: 0.1530\n",
            "  Batch 1500/8989 - Loss: 0.1534\n",
            "  Batch 1600/8989 - Loss: 0.1536\n",
            "  Batch 1700/8989 - Loss: 0.1538\n",
            "  Batch 1800/8989 - Loss: 0.1539\n",
            "  Batch 1900/8989 - Loss: 0.1540\n",
            "  Batch 2000/8989 - Loss: 0.1541\n",
            "  Batch 2100/8989 - Loss: 0.1541\n",
            "  Batch 2200/8989 - Loss: 0.1543\n",
            "  Batch 2300/8989 - Loss: 0.1545\n",
            "  Batch 2400/8989 - Loss: 0.1548\n",
            "  Batch 2500/8989 - Loss: 0.1550\n",
            "  Batch 2600/8989 - Loss: 0.1551\n",
            "  Batch 2700/8989 - Loss: 0.1554\n",
            "  Batch 2800/8989 - Loss: 0.1554\n",
            "  Batch 2900/8989 - Loss: 0.1557\n",
            "  Batch 3000/8989 - Loss: 0.1558\n",
            "  Batch 3100/8989 - Loss: 0.1559\n",
            "  Batch 3200/8989 - Loss: 0.1560\n",
            "  Batch 3300/8989 - Loss: 0.1562\n",
            "  Batch 3400/8989 - Loss: 0.1565\n",
            "  Batch 3500/8989 - Loss: 0.1567\n",
            "  Batch 3600/8989 - Loss: 0.1568\n",
            "  Batch 3700/8989 - Loss: 0.1569\n",
            "  Batch 3800/8989 - Loss: 0.1571\n",
            "  Batch 3900/8989 - Loss: 0.1573\n",
            "  Batch 4000/8989 - Loss: 0.1575\n",
            "  Batch 4100/8989 - Loss: 0.1577\n",
            "  Batch 4200/8989 - Loss: 0.1577\n",
            "  Batch 4300/8989 - Loss: 0.1578\n",
            "  Batch 4400/8989 - Loss: 0.1579\n",
            "  Batch 4500/8989 - Loss: 0.1580\n",
            "  Batch 4600/8989 - Loss: 0.1581\n",
            "  Batch 4700/8989 - Loss: 0.1582\n",
            "  Batch 4800/8989 - Loss: 0.1583\n",
            "  Batch 4900/8989 - Loss: 0.1584\n",
            "  Batch 5000/8989 - Loss: 0.1586\n",
            "  Batch 5100/8989 - Loss: 0.1585\n",
            "  Batch 5200/8989 - Loss: 0.1588\n",
            "  Batch 5300/8989 - Loss: 0.1589\n",
            "  Batch 5400/8989 - Loss: 0.1590\n",
            "  Batch 5500/8989 - Loss: 0.1591\n",
            "  Batch 5600/8989 - Loss: 0.1593\n",
            "  Batch 5700/8989 - Loss: 0.1594\n",
            "  Batch 5800/8989 - Loss: 0.1594\n",
            "  Batch 5900/8989 - Loss: 0.1595\n",
            "  Batch 6000/8989 - Loss: 0.1596\n",
            "  Batch 6100/8989 - Loss: 0.1597\n",
            "  Batch 6200/8989 - Loss: 0.1598\n",
            "  Batch 6300/8989 - Loss: 0.1600\n",
            "  Batch 6400/8989 - Loss: 0.1601\n",
            "  Batch 6500/8989 - Loss: 0.1603\n",
            "  Batch 6600/8989 - Loss: 0.1604\n",
            "  Batch 6700/8989 - Loss: 0.1605\n",
            "  Batch 6800/8989 - Loss: 0.1607\n",
            "  Batch 6900/8989 - Loss: 0.1608\n",
            "  Batch 7000/8989 - Loss: 0.1609\n",
            "  Batch 7100/8989 - Loss: 0.1609\n",
            "  Batch 7200/8989 - Loss: 0.1611\n",
            "  Batch 7300/8989 - Loss: 0.1612\n",
            "  Batch 7400/8989 - Loss: 0.1613\n",
            "  Batch 7500/8989 - Loss: 0.1614\n",
            "  Batch 7600/8989 - Loss: 0.1615\n",
            "  Batch 7700/8989 - Loss: 0.1616\n",
            "  Batch 7800/8989 - Loss: 0.1617\n",
            "  Batch 7900/8989 - Loss: 0.1617\n",
            "  Batch 8000/8989 - Loss: 0.1618\n",
            "  Batch 8100/8989 - Loss: 0.1619\n",
            "  Batch 8200/8989 - Loss: 0.1620\n",
            "  Batch 8300/8989 - Loss: 0.1622\n",
            "  Batch 8400/8989 - Loss: 0.1623\n",
            "  Batch 8500/8989 - Loss: 0.1624\n",
            "  Batch 8600/8989 - Loss: 0.1624\n",
            "  Batch 8700/8989 - Loss: 0.1625\n",
            "  Batch 8800/8989 - Loss: 0.1627\n",
            "  Batch 8900/8989 - Loss: 0.1628\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:40\n",
            "  Loss: 0.1629\n",
            "  HR@10: 0.7103\n",
            "  NDCG@10: 0.4338\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 20/20\n",
            "----------------------------------------------------------------------\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1475\n",
            "  Batch 200/8989 - Loss: 0.1472\n",
            "  Batch 300/8989 - Loss: 0.1472\n",
            "  Batch 400/8989 - Loss: 0.1479\n",
            "  Batch 500/8989 - Loss: 0.1482\n",
            "  Batch 600/8989 - Loss: 0.1489\n",
            "  Batch 700/8989 - Loss: 0.1490\n",
            "  Batch 800/8989 - Loss: 0.1494\n",
            "  Batch 900/8989 - Loss: 0.1496\n",
            "  Batch 1000/8989 - Loss: 0.1496\n",
            "  Batch 1100/8989 - Loss: 0.1499\n",
            "  Batch 1200/8989 - Loss: 0.1498\n",
            "  Batch 1300/8989 - Loss: 0.1501\n",
            "  Batch 1400/8989 - Loss: 0.1503\n",
            "  Batch 1500/8989 - Loss: 0.1504\n",
            "  Batch 1600/8989 - Loss: 0.1508\n",
            "  Batch 1700/8989 - Loss: 0.1508\n",
            "  Batch 1800/8989 - Loss: 0.1512\n",
            "  Batch 1900/8989 - Loss: 0.1511\n",
            "  Batch 2000/8989 - Loss: 0.1512\n",
            "  Batch 2100/8989 - Loss: 0.1512\n",
            "  Batch 2200/8989 - Loss: 0.1513\n",
            "  Batch 2300/8989 - Loss: 0.1516\n",
            "  Batch 2400/8989 - Loss: 0.1517\n",
            "  Batch 2500/8989 - Loss: 0.1518\n",
            "  Batch 2600/8989 - Loss: 0.1521\n",
            "  Batch 2700/8989 - Loss: 0.1522\n",
            "  Batch 2800/8989 - Loss: 0.1525\n",
            "  Batch 2900/8989 - Loss: 0.1524\n",
            "  Batch 3000/8989 - Loss: 0.1525\n",
            "  Batch 3100/8989 - Loss: 0.1526\n",
            "  Batch 3200/8989 - Loss: 0.1528\n",
            "  Batch 3300/8989 - Loss: 0.1530\n",
            "  Batch 3400/8989 - Loss: 0.1532\n",
            "  Batch 3500/8989 - Loss: 0.1535\n",
            "  Batch 3600/8989 - Loss: 0.1536\n",
            "  Batch 3700/8989 - Loss: 0.1536\n",
            "  Batch 3800/8989 - Loss: 0.1536\n",
            "  Batch 3900/8989 - Loss: 0.1538\n",
            "  Batch 4000/8989 - Loss: 0.1541\n",
            "  Batch 4100/8989 - Loss: 0.1542\n",
            "  Batch 4200/8989 - Loss: 0.1543\n",
            "  Batch 4300/8989 - Loss: 0.1544\n",
            "  Batch 4400/8989 - Loss: 0.1546\n",
            "  Batch 4500/8989 - Loss: 0.1547\n",
            "  Batch 4600/8989 - Loss: 0.1548\n",
            "  Batch 4700/8989 - Loss: 0.1550\n",
            "  Batch 4800/8989 - Loss: 0.1551\n",
            "  Batch 4900/8989 - Loss: 0.1553\n",
            "  Batch 5000/8989 - Loss: 0.1554\n",
            "  Batch 5100/8989 - Loss: 0.1556\n",
            "  Batch 5200/8989 - Loss: 0.1557\n",
            "  Batch 5300/8989 - Loss: 0.1558\n",
            "  Batch 5400/8989 - Loss: 0.1559\n",
            "  Batch 5500/8989 - Loss: 0.1560\n",
            "  Batch 5600/8989 - Loss: 0.1562\n",
            "  Batch 5700/8989 - Loss: 0.1564\n",
            "  Batch 5800/8989 - Loss: 0.1565\n",
            "  Batch 5900/8989 - Loss: 0.1566\n",
            "  Batch 6000/8989 - Loss: 0.1568\n",
            "  Batch 6100/8989 - Loss: 0.1570\n",
            "  Batch 6200/8989 - Loss: 0.1570\n",
            "  Batch 6300/8989 - Loss: 0.1572\n",
            "  Batch 6400/8989 - Loss: 0.1573\n",
            "  Batch 6500/8989 - Loss: 0.1573\n",
            "  Batch 6600/8989 - Loss: 0.1574\n",
            "  Batch 6700/8989 - Loss: 0.1576\n",
            "  Batch 6800/8989 - Loss: 0.1577\n",
            "  Batch 6900/8989 - Loss: 0.1578\n",
            "  Batch 7000/8989 - Loss: 0.1579\n",
            "  Batch 7100/8989 - Loss: 0.1581\n",
            "  Batch 7200/8989 - Loss: 0.1582\n",
            "  Batch 7300/8989 - Loss: 0.1583\n",
            "  Batch 7400/8989 - Loss: 0.1584\n",
            "  Batch 7500/8989 - Loss: 0.1585\n",
            "  Batch 7600/8989 - Loss: 0.1587\n",
            "  Batch 7700/8989 - Loss: 0.1587\n",
            "  Batch 7800/8989 - Loss: 0.1588\n",
            "  Batch 7900/8989 - Loss: 0.1589\n",
            "  Batch 8000/8989 - Loss: 0.1590\n",
            "  Batch 8100/8989 - Loss: 0.1591\n",
            "  Batch 8200/8989 - Loss: 0.1591\n",
            "  Batch 8300/8989 - Loss: 0.1593\n",
            "  Batch 8400/8989 - Loss: 0.1594\n",
            "  Batch 8500/8989 - Loss: 0.1595\n",
            "  Batch 8600/8989 - Loss: 0.1595\n",
            "  Batch 8700/8989 - Loss: 0.1596\n",
            "  Batch 8800/8989 - Loss: 0.1597\n",
            "  Batch 8900/8989 - Loss: 0.1598\n",
            "  Evaluating on test set...\n",
            "  Time: 00:00:38\n",
            "  Loss: 0.1598\n",
            "  HR@10: 0.7081\n",
            "  NDCG@10: 0.4312\n",
            "  (Best: HR@10: 0.7404 at epoch 3)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best model at epoch 3:\n",
            "  HR@10: 0.7404\n",
            "  NDCG@10: 0.4565\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.2 TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.2: Starting Training\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training for {epochs} epochs...\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track best performance\n",
        "best_hr = 0.0\n",
        "best_ndcg = 0.0\n",
        "best_epoch = 0\n",
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'hr': [],\n",
        "    'ndcg': [],\n",
        "    'time': []\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ========================================================================\n",
        "    # TRAINING PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to training mode\n",
        "    # This enables dropout and other training-specific behaviors\n",
        "    ncf_model.train()\n",
        "    \n",
        "    # Start timer for this epoch\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Generate negative samples for this epoch\n",
        "    # Important: We generate fresh negatives each epoch for better learning\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 70)\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    # Track loss for this epoch\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        # Move data to device (GPU or CPU)\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        # ================================================================\n",
        "        # FORWARD PASS\n",
        "        # ================================================================\n",
        "        # Clear gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get model predictions (raw scores/logits)\n",
        "        prediction = ncf_model(user, item)  # [batch_size]\n",
        "        \n",
        "        # ================================================================\n",
        "        # COMPUTE LOSS\n",
        "        # ================================================================\n",
        "        # Compare predictions with true labels (1 for positive, 0 for negative)\n",
        "        loss = loss_function(prediction, label)\n",
        "        \n",
        "        # ================================================================\n",
        "        # BACKWARD PASS\n",
        "        # ================================================================\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track loss\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Print progress every 100 batches\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Calculate average loss for this epoch\n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EVALUATION PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    # This disables dropout and uses deterministic behavior\n",
        "    ncf_model.eval()\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"  Evaluating on test set...\")\n",
        "    HR, NDCG = evaluate_metrics(ncf_model, test_loader, top_k, device)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['hr'].append(HR)\n",
        "    training_history['ndcg'].append(NDCG)\n",
        "    training_history['time'].append(elapsed_time)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"  Time: {time_str}\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  HR@{top_k}: {HR:.4f}\")\n",
        "    print(f\"  NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # SAVE BEST MODEL\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Check if this is the best model so far\n",
        "    if HR > best_hr:\n",
        "        best_hr = HR\n",
        "        best_ndcg = NDCG\n",
        "        best_epoch = epoch + 1\n",
        "        \n",
        "        print(f\"  ✓ New best model! (HR@{top_k}: {HR:.4f})\")\n",
        "        \n",
        "        # Save model if enabled\n",
        "        if save_model:\n",
        "            if not os.path.exists(model_path):\n",
        "                os.makedirs(model_path)\n",
        "            \n",
        "            model_filename = os.path.join(model_path, f'{model_name}.pth')\n",
        "            torch.save(ncf_model, model_filename)\n",
        "            print(f\"  ✓ Model saved to {model_filename}\")\n",
        "    else:\n",
        "        print(f\"  (Best: HR@{top_k}: {best_hr:.4f} at epoch {best_epoch})\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ========================================================================\n",
        "# TRAINING COMPLETE\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best model at epoch {best_epoch}:\")\n",
        "print(f\"  HR@{top_k}: {best_hr:.4f}\")\n",
        "print(f\"  NDCG@{top_k}: {best_ndcg:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store the NeuMF-end model for later comparison\n",
        "ncf_model_neumf_end = ncf_model\n",
        "best_hr_neumf_end = best_hr\n",
        "best_ndcg_neumf_end = best_ndcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7.5: Train Models Separately (GMF, MLP, NeuMF-pre)\n",
        "\n",
        "In this step, we'll train each model architecture separately:\n",
        "1. **Train GMF model** - Generalized Matrix Factorization only\n",
        "2. **Train MLP model** - Multi-Layer Perceptron only  \n",
        "3. **Train NeuMF-pre** - Neural Matrix Factorization using pre-trained GMF and MLP weights\n",
        "\n",
        "This approach (NeuMF-pre) typically gives the best performance as it leverages pre-trained components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.5: Training Models Separately\n",
            "======================================================================\n",
            "This will train:\n",
            "  1. GMF model (Generalized Matrix Factorization)\n",
            "  2. MLP model (Multi-Layer Perceptron)\n",
            "  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 7.5.1: Training GMF Model\n",
            "======================================================================\n",
            "Training GMF for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:20 - Loss: 0.3577 - HR@10: 0.6525 - NDCG@10: 0.3838\n",
            "  ✓ Saved best GMF model (HR@10: 0.6525)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:20 - Loss: 0.2979 - HR@10: 0.7042 - NDCG@10: 0.4259\n",
            "  ✓ Saved best GMF model (HR@10: 0.7042)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:19 - Loss: 0.2781 - HR@10: 0.7249 - NDCG@10: 0.4432\n",
            "  ✓ Saved best GMF model (HR@10: 0.7249)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:19 - Loss: 0.2663 - HR@10: 0.7370 - NDCG@10: 0.4539\n",
            "  ✓ Saved best GMF model (HR@10: 0.7370)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:20 - Loss: 0.2566 - HR@10: 0.7437 - NDCG@10: 0.4598\n",
            "  ✓ Saved best GMF model (HR@10: 0.7437)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:20 - Loss: 0.2489 - HR@10: 0.7481 - NDCG@10: 0.4633\n",
            "  ✓ Saved best GMF model (HR@10: 0.7481)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:20 - Loss: 0.2413 - HR@10: 0.7512 - NDCG@10: 0.4661\n",
            "  ✓ Saved best GMF model (HR@10: 0.7512)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:19 - Loss: 0.2345 - HR@10: 0.7499 - NDCG@10: 0.4651\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:20 - Loss: 0.2290 - HR@10: 0.7485 - NDCG@10: 0.4647\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:20 - Loss: 0.2246 - HR@10: 0.7470 - NDCG@10: 0.4631\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:19 - Loss: 0.2216 - HR@10: 0.7471 - NDCG@10: 0.4631\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:19 - Loss: 0.2187 - HR@10: 0.7462 - NDCG@10: 0.4626\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:20 - Loss: 0.2162 - HR@10: 0.7452 - NDCG@10: 0.4622\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:19 - Loss: 0.2145 - HR@10: 0.7450 - NDCG@10: 0.4614\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:20 - Loss: 0.2127 - HR@10: 0.7459 - NDCG@10: 0.4613\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:19 - Loss: 0.2111 - HR@10: 0.7450 - NDCG@10: 0.4610\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:20 - Loss: 0.2092 - HR@10: 0.7458 - NDCG@10: 0.4618\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:20 - Loss: 0.2085 - HR@10: 0.7446 - NDCG@10: 0.4609\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:19 - Loss: 0.2076 - HR@10: 0.7453 - NDCG@10: 0.4620\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:19 - Loss: 0.2065 - HR@10: 0.7459 - NDCG@10: 0.4622\n",
            "\n",
            "✓ GMF Training Complete!\n",
            "  Best epoch: 7\n",
            "  Best HR@10: 0.7512\n",
            "  Best NDCG@10: 0.4661\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7.5: TRAIN MODELS SEPARATELY (GMF, MLP, NeuMF-pre)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.5: Training Models Separately\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will train:\")\n",
        "print(\"  1. GMF model (Generalized Matrix Factorization)\")\n",
        "print(\"  2. MLP model (Multi-Layer Perceptron)\")\n",
        "print(\"  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store trained models\n",
        "trained_models = {}\n",
        "\n",
        "# ============================================================================\n",
        "# 7.5.1 TRAIN GMF MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.1: Training GMF Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create GMF model\n",
        "gmf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='GMF',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gmf_model = gmf_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "gmf_optimizer = optim.Adam(gmf_model.parameters(), lr=learning_rate)\n",
        "gmf_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for GMF\n",
        "print(f\"Training GMF for {epochs} epochs...\")\n",
        "best_hr_gmf = 0.0\n",
        "best_ndcg_gmf = 0.0\n",
        "best_epoch_gmf = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gmf_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        gmf_optimizer.zero_grad()\n",
        "        prediction = gmf_model(user, item)\n",
        "        loss = gmf_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        gmf_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    gmf_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(gmf_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_gmf:\n",
        "        best_hr_gmf = HR\n",
        "        best_ndcg_gmf = NDCG\n",
        "        best_epoch_gmf = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(gmf_model, GMF_model_path)\n",
        "            print(f\"  ✓ Saved best GMF model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ GMF Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_gmf}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_gmf:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_gmf:.4f}\")\n",
        "\n",
        "trained_models['GMF'] = {\n",
        "    'model': gmf_model,\n",
        "    'hr': best_hr_gmf,\n",
        "    'ndcg': best_ndcg_gmf,\n",
        "    'epoch': best_epoch_gmf\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.2: Training MLP Model\n",
            "======================================================================\n",
            "Training MLP for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:33 - Loss: 0.3491 - HR@10: 0.5890 - NDCG@10: 0.3364\n",
            "  ✓ Saved best MLP model (HR@10: 0.5890)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:32 - Loss: 0.3182 - HR@10: 0.6518 - NDCG@10: 0.3826\n",
            "  ✓ Saved best MLP model (HR@10: 0.6518)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:33 - Loss: 0.2971 - HR@10: 0.6923 - NDCG@10: 0.4143\n",
            "  ✓ Saved best MLP model (HR@10: 0.6923)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:32 - Loss: 0.2796 - HR@10: 0.7145 - NDCG@10: 0.4338\n",
            "  ✓ Saved best MLP model (HR@10: 0.7145)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:33 - Loss: 0.2693 - HR@10: 0.7271 - NDCG@10: 0.4442\n",
            "  ✓ Saved best MLP model (HR@10: 0.7271)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:33 - Loss: 0.2616 - HR@10: 0.7334 - NDCG@10: 0.4502\n",
            "  ✓ Saved best MLP model (HR@10: 0.7334)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:32 - Loss: 0.2552 - HR@10: 0.7377 - NDCG@10: 0.4554\n",
            "  ✓ Saved best MLP model (HR@10: 0.7377)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:32 - Loss: 0.2492 - HR@10: 0.7404 - NDCG@10: 0.4575\n",
            "  ✓ Saved best MLP model (HR@10: 0.7404)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:33 - Loss: 0.2433 - HR@10: 0.7424 - NDCG@10: 0.4603\n",
            "  ✓ Saved best MLP model (HR@10: 0.7424)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:32 - Loss: 0.2376 - HR@10: 0.7439 - NDCG@10: 0.4619\n",
            "  ✓ Saved best MLP model (HR@10: 0.7439)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:33 - Loss: 0.2325 - HR@10: 0.7435 - NDCG@10: 0.4606\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:32 - Loss: 0.2272 - HR@10: 0.7432 - NDCG@10: 0.4637\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:33 - Loss: 0.2220 - HR@10: 0.7410 - NDCG@10: 0.4624\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:32 - Loss: 0.2174 - HR@10: 0.7417 - NDCG@10: 0.4617\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:33 - Loss: 0.2123 - HR@10: 0.7388 - NDCG@10: 0.4613\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:33 - Loss: 0.2084 - HR@10: 0.7370 - NDCG@10: 0.4600\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:32 - Loss: 0.2032 - HR@10: 0.7359 - NDCG@10: 0.4579\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:32 - Loss: 0.1991 - HR@10: 0.7337 - NDCG@10: 0.4576\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:33 - Loss: 0.1949 - HR@10: 0.7279 - NDCG@10: 0.4533\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:32 - Loss: 0.1911 - HR@10: 0.7258 - NDCG@10: 0.4523\n",
            "\n",
            "✓ MLP Training Complete!\n",
            "  Best epoch: 10\n",
            "  Best HR@10: 0.7439\n",
            "  Best NDCG@10: 0.4619\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.2 TRAIN MLP MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.2: Training MLP Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create MLP model\n",
        "mlp_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='MLP',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mlp_model = mlp_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "mlp_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for MLP\n",
        "print(f\"Training MLP for {epochs} epochs...\")\n",
        "best_hr_mlp = 0.0\n",
        "best_ndcg_mlp = 0.0\n",
        "best_epoch_mlp = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        mlp_optimizer.zero_grad()\n",
        "        prediction = mlp_model(user, item)\n",
        "        loss = mlp_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    mlp_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(mlp_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_mlp:\n",
        "        best_hr_mlp = HR\n",
        "        best_ndcg_mlp = NDCG\n",
        "        best_epoch_mlp = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(mlp_model, MLP_model_path)\n",
        "            print(f\"  ✓ Saved best MLP model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ MLP Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_mlp}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_mlp:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_mlp:.4f}\")\n",
        "\n",
        "trained_models['MLP'] = {\n",
        "    'model': mlp_model,\n",
        "    'hr': best_hr_mlp,\n",
        "    'ndcg': best_ndcg_mlp,\n",
        "    'epoch': best_epoch_mlp\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.3: Training NeuMF-pre Model\n",
            "======================================================================\n",
            "Creating NeuMF model with pre-trained GMF and MLP weights...\n",
            "Training NeuMF-pre for 20 epochs...\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:28 - Loss: 0.1674 - HR@10: 0.7554 - NDCG@10: 0.4782\n",
            "  ✓ Saved best NeuMF-pre model (HR@10: 0.7554)\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:28 - Loss: 0.1663 - HR@10: 0.7531 - NDCG@10: 0.4764\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:28 - Loss: 0.1648 - HR@10: 0.7517 - NDCG@10: 0.4752\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:28 - Loss: 0.1642 - HR@10: 0.7508 - NDCG@10: 0.4744\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:28 - Loss: 0.1638 - HR@10: 0.7501 - NDCG@10: 0.4739\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:28 - Loss: 0.1635 - HR@10: 0.7497 - NDCG@10: 0.4733\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:28 - Loss: 0.1633 - HR@10: 0.7492 - NDCG@10: 0.4729\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:28 - Loss: 0.1627 - HR@10: 0.7488 - NDCG@10: 0.4728\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:28 - Loss: 0.1628 - HR@10: 0.7487 - NDCG@10: 0.4724\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:28 - Loss: 0.1627 - HR@10: 0.7483 - NDCG@10: 0.4722\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:28 - Loss: 0.1622 - HR@10: 0.7481 - NDCG@10: 0.4721\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:28 - Loss: 0.1623 - HR@10: 0.7481 - NDCG@10: 0.4719\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:28 - Loss: 0.1623 - HR@10: 0.7481 - NDCG@10: 0.4720\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:28 - Loss: 0.1618 - HR@10: 0.7478 - NDCG@10: 0.4717\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:28 - Loss: 0.1619 - HR@10: 0.7477 - NDCG@10: 0.4715\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:28 - Loss: 0.1616 - HR@10: 0.7476 - NDCG@10: 0.4714\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:28 - Loss: 0.1615 - HR@10: 0.7474 - NDCG@10: 0.4713\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:28 - Loss: 0.1615 - HR@10: 0.7470 - NDCG@10: 0.4710\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:28 - Loss: 0.1616 - HR@10: 0.7468 - NDCG@10: 0.4708\n",
            "Generating 4 negative samples per positive pair...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:28 - Loss: 0.1614 - HR@10: 0.7466 - NDCG@10: 0.4707\n",
            "\n",
            "✓ NeuMF-pre Training Complete!\n",
            "  Best epoch: 1\n",
            "  Best HR@10: 0.7554\n",
            "  Best NDCG@10: 0.4782\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Model           HR@{top_k}   NDCG@{top_k} Best Epoch  \n",
            "----------------------------------------------------------------------\n",
            "GMF             0.7512       0.4661       7           \n",
            "MLP             0.7439       0.4619       10          \n",
            "NeuMF-end       0.7404       0.4565       3           \n",
            "NeuMF-pre       0.7554       0.4782       1           \n",
            "======================================================================\n",
            "\n",
            "🏆 Best Model: NeuMF-pre\n",
            "   HR@10: 0.7554\n",
            "   NDCG@10: 0.4782\n",
            "\n",
            "✓ Using NeuMF-pre model for recommendations\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.3 TRAIN NeuMF-pre MODEL (Using Pre-trained GMF and MLP)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.3: Training NeuMF-pre Model\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Creating NeuMF model with pre-trained GMF and MLP weights...\")\n",
        "\n",
        "# Create NeuMF-pre model using pre-trained GMF and MLP\n",
        "neumf_pre_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='NeuMF-pre',\n",
        "    GMF_model=gmf_model,\n",
        "    MLP_model=mlp_model\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    neumf_pre_model = neumf_pre_model.cuda()\n",
        "\n",
        "# Setup optimizer (SGD is typically used for NeuMF-pre)\n",
        "neumf_pre_optimizer = optim.SGD(neumf_pre_model.parameters(), lr=learning_rate)\n",
        "neumf_pre_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for NeuMF-pre\n",
        "print(f\"Training NeuMF-pre for {epochs} epochs...\")\n",
        "best_hr_neumf_pre = 0.0\n",
        "best_ndcg_neumf_pre = 0.0\n",
        "best_epoch_neumf_pre = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    neumf_pre_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        neumf_pre_optimizer.zero_grad()\n",
        "        prediction = neumf_pre_model(user, item)\n",
        "        loss = neumf_pre_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        neumf_pre_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    neumf_pre_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(neumf_pre_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_neumf_pre:\n",
        "        best_hr_neumf_pre = HR\n",
        "        best_ndcg_neumf_pre = NDCG\n",
        "        best_epoch_neumf_pre = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(neumf_pre_model, NeuMF_model_path)\n",
        "            print(f\"  ✓ Saved best NeuMF-pre model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ NeuMF-pre Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_neumf_pre}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_neumf_pre:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_neumf_pre:.4f}\")\n",
        "\n",
        "trained_models['NeuMF-pre'] = {\n",
        "    'model': neumf_pre_model,\n",
        "    'hr': best_hr_neumf_pre,\n",
        "    'ndcg': best_ndcg_neumf_pre,\n",
        "    'epoch': best_epoch_neumf_pre\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON OF ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'HR@{top_k}':<12} {'NDCG@{top_k}':<12} {'Best Epoch':<12}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'GMF':<15} {best_hr_gmf:<12.4f} {best_ndcg_gmf:<12.4f} {best_epoch_gmf:<12}\")\n",
        "print(f\"{'MLP':<15} {best_hr_mlp:<12.4f} {best_ndcg_mlp:<12.4f} {best_epoch_mlp:<12}\")\n",
        "print(f\"{'NeuMF-end':<15} {best_hr_neumf_end:<12.4f} {best_ndcg_neumf_end:<12.4f} {best_epoch:<12}\")\n",
        "print(f\"{'NeuMF-pre':<15} {best_hr_neumf_pre:<12.4f} {best_ndcg_neumf_pre:<12.4f} {best_epoch_neumf_pre:<12}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find best model\n",
        "all_results = [\n",
        "    ('GMF', best_hr_gmf, best_ndcg_gmf),\n",
        "    ('MLP', best_hr_mlp, best_ndcg_mlp),\n",
        "    ('NeuMF-end', best_hr_neumf_end, best_ndcg_neumf_end),\n",
        "    ('NeuMF-pre', best_hr_neumf_pre, best_ndcg_neumf_pre)\n",
        "]\n",
        "best_model_name, best_hr_overall, best_ndcg_overall = max(all_results, key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   HR@{top_k}: {best_hr_overall:.4f}\")\n",
        "print(f\"   NDCG@{top_k}: {best_ndcg_overall:.4f}\")\n",
        "\n",
        "# Set the best model as the main model for recommendations\n",
        "if best_model_name == 'GMF':\n",
        "    ncf_model = gmf_model\n",
        "elif best_model_name == 'MLP':\n",
        "    ncf_model = mlp_model\n",
        "elif best_model_name == 'NeuMF-pre':\n",
        "    ncf_model = neumf_pre_model\n",
        "else:\n",
        "    ncf_model = ncf_model_neumf_end\n",
        "\n",
        "print(f\"\\n✓ Using {best_model_name} model for recommendations\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation of Step 7.5:\n",
        "\n",
        "**Why Train Separately?**\n",
        "\n",
        "1. **GMF (Generalized Matrix Factorization)**:\n",
        "   - Simple linear model\n",
        "   - Fast to train\n",
        "   - Good baseline performance\n",
        "   - Captures linear user-item interactions\n",
        "\n",
        "2. **MLP (Multi-Layer Perceptron)**:\n",
        "   - Deep non-linear model\n",
        "   - Can learn complex patterns\n",
        "   - Complements GMF's linear approach\n",
        "\n",
        "3. **NeuMF-pre (Pre-trained Neural Matrix Factorization)**:\n",
        "   - Combines pre-trained GMF and MLP\n",
        "   - Initializes with learned embeddings from both models\n",
        "   - Typically achieves best performance\n",
        "   - Fine-tunes the combined model\n",
        "\n",
        "**Training Strategy:**\n",
        "- Train GMF and MLP independently\n",
        "- Use their learned embeddings to initialize NeuMF\n",
        "- Fine-tune NeuMF with SGD (more stable than Adam for pre-trained models)\n",
        "- This gives better performance than training NeuMF from scratch\n",
        "\n",
        "**Model Comparison:**\n",
        "- GMF: Fast, simple, good baseline\n",
        "- MLP: Complex patterns, non-linear\n",
        "- NeuMF-end: Trained from scratch, good balance\n",
        "- NeuMF-pre: Best performance, uses pre-trained components\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 7.5 Complete!**\n",
        "\n",
        "All models have been trained separately. The best model is now available for recommendations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 7:\n",
        "\n",
        "#### 7.1 Loss Function and Optimizer\n",
        "\n",
        "**Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss):**\n",
        "\n",
        "**What it does:**\n",
        "- Combines sigmoid activation + binary cross-entropy loss\n",
        "- More numerically stable than applying sigmoid separately\n",
        "- Formula: `loss = -[y*log(σ(x)) + (1-y)*log(1-σ(x))]`\n",
        "  - Where σ(x) = sigmoid(x), y = true label (0 or 1), x = model output\n",
        "\n",
        "**Why this loss?**\n",
        "- Our task: Predict if user will like item (binary classification)\n",
        "- Labels: 1 (positive interaction) or 0 (negative interaction)\n",
        "- Model outputs: Raw scores (logits), not probabilities\n",
        "- BCEWithLogitsLoss handles the conversion internally\n",
        "\n",
        "**Numerical Stability:**\n",
        "- Direct sigmoid can cause overflow/underflow\n",
        "- BCEWithLogitsLoss uses log-sum-exp trick for stability\n",
        "- Prevents NaN/inf values during training\n",
        "\n",
        "**Adam Optimizer:**\n",
        "\n",
        "**What is Adam?**\n",
        "- Adaptive Moment Estimation\n",
        "- Combines benefits of:\n",
        "  - Momentum: Uses moving average of gradients (smoother updates)\n",
        "  - RMSprop: Adapts learning rate per parameter\n",
        "  - Bias correction: Accounts for initialization bias\n",
        "\n",
        "**Why Adam?**\n",
        "- Works well out-of-the-box (default hyperparameters)\n",
        "- Adapts learning rate automatically\n",
        "- Faster convergence than SGD for most problems\n",
        "- Good for recommendation systems\n",
        "\n",
        "**Learning Rate:**\n",
        "- 0.001 is a safe default for Adam\n",
        "- Too high: Training unstable, loss might explode\n",
        "- Too low: Training very slow, might not converge\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.2 Training Loop - Step by Step\n",
        "\n",
        "**Epoch Structure:**\n",
        "\n",
        "1. **Set Training Mode**:\n",
        "   ```python\n",
        "   model.train()  # Enables dropout, batch norm training mode\n",
        "   ```\n",
        "\n",
        "2. **Generate Negative Samples**:\n",
        "   ```python\n",
        "   train_dataset.ng_sample()  # Fresh negatives each epoch\n",
        "   ```\n",
        "   - Important: New negatives each epoch improves learning\n",
        "   - Prevents overfitting to specific negative samples\n",
        "\n",
        "3. **Iterate Through Batches**:\n",
        "   ```python\n",
        "   for user, item, label in train_loader:\n",
        "       # Process batch\n",
        "   ```\n",
        "\n",
        "4. **Forward Pass**:\n",
        "   ```python\n",
        "   prediction = model(user, item)  # Get model predictions\n",
        "   ```\n",
        "   - Model outputs raw scores (logits)\n",
        "   - Higher score = more likely user will like item\n",
        "\n",
        "5. **Compute Loss**:\n",
        "   ```python\n",
        "   loss = loss_function(prediction, label)\n",
        "   ```\n",
        "   - Compares predictions with true labels\n",
        "   - Measures how wrong the model is\n",
        "\n",
        "6. **Backward Pass**:\n",
        "   ```python\n",
        "   loss.backward()  # Compute gradients\n",
        "   optimizer.step()  # Update weights\n",
        "   ```\n",
        "   - Gradients tell us how to adjust weights\n",
        "   - Optimizer updates weights to reduce loss\n",
        "\n",
        "7. **Evaluation**:\n",
        "   ```python\n",
        "   model.eval()  # Disable dropout\n",
        "   HR, NDCG = evaluate_metrics(model, test_loader, top_k)\n",
        "   ```\n",
        "   - Test model on held-out test set\n",
        "   - Measure performance with Hit Rate and NDCG\n",
        "\n",
        "8. **Save Best Model**:\n",
        "   ```python\n",
        "   if HR > best_hr:\n",
        "       torch.save(model, 'best_model.pth')\n",
        "   ```\n",
        "   - Save model with best validation performance\n",
        "   - Prevents losing good models if training degrades\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.3 Understanding Training Progress\n",
        "\n",
        "**What to Watch:**\n",
        "\n",
        "1. **Loss Decreasing**:\n",
        "   - Should decrease over epochs\n",
        "   - If increases: learning rate too high or model unstable\n",
        "   - If plateaus: model converged or needs more capacity\n",
        "\n",
        "2. **HR and NDCG Increasing**:\n",
        "   - Should improve over epochs\n",
        "   - Early epochs: rapid improvement\n",
        "   - Later epochs: slower improvement (diminishing returns)\n",
        "\n",
        "3. **Best Model Tracking**:\n",
        "   - Model might overfit (train loss decreases, test HR decreases)\n",
        "   - We save best model based on test HR\n",
        "   - This prevents using overfitted model\n",
        "\n",
        "**Typical Training Curve:**\n",
        "```\n",
        "Epoch 1:  HR: 0.200, NDCG: 0.100  (random)\n",
        "Epoch 5:  HR: 0.500, NDCG: 0.300  (learning)\n",
        "Epoch 10: HR: 0.650, NDCG: 0.420  (improving)\n",
        "Epoch 15: HR: 0.680, NDCG: 0.440  (slowing)\n",
        "Epoch 20: HR: 0.690, NDCG: 0.445  (converged)\n",
        "```\n",
        "\n",
        "**When to Stop:**\n",
        "- If HR stops improving for several epochs\n",
        "- If test HR starts decreasing (overfitting)\n",
        "- After specified number of epochs\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.4 Model Saving\n",
        "\n",
        "**Why Save Models?**\n",
        "- Training takes time (minutes to hours)\n",
        "- Best model might not be the last epoch\n",
        "- Allows loading and using model later\n",
        "- Enables comparison of different runs\n",
        "\n",
        "**What Gets Saved:**\n",
        "- All model parameters (weights and biases)\n",
        "- Model architecture (can reconstruct model)\n",
        "- Optimizer state (optional, for resuming training)\n",
        "\n",
        "**Loading Saved Model:**\n",
        "```python\n",
        "model = torch.load('best_model.pth')\n",
        "model.eval()  # Set to evaluation mode\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 7.5 Training Tips\n",
        "\n",
        "**If Training is Slow:**\n",
        "- Reduce batch size (if memory allows)\n",
        "- Use GPU if available\n",
        "- Reduce number of epochs (if converged early)\n",
        "\n",
        "**If Loss Not Decreasing:**\n",
        "- Check learning rate (might be too high/low)\n",
        "- Check data loading (make sure data is correct)\n",
        "- Check model architecture (might have bugs)\n",
        "\n",
        "**If Overfitting (train good, test bad):**\n",
        "- Increase dropout rate\n",
        "- Reduce model capacity (fewer layers/embeddings)\n",
        "- Get more training data\n",
        "- Use early stopping\n",
        "\n",
        "**If Underfitting (both train and test bad):**\n",
        "- Increase model capacity\n",
        "- Train for more epochs\n",
        "- Reduce dropout\n",
        "- Check if learning rate is appropriate\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 7 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Complete training loop implemented\n",
        "- Model training and evaluation\n",
        "- Best model saving\n",
        "- Progress tracking\n",
        "\n",
        "---\n",
        "\n",
        "## Step 8: Using the Trained Model for Recommendations\n",
        "\n",
        "Now that we have a trained model, let's learn how to use it to make recommendations! This step shows:\n",
        "1. How to load a saved model\n",
        "2. How to get recommendations for a user\n",
        "3. How to predict user-item interaction scores\n",
        "4. Practical examples\n",
        "\n",
        "This is where the model becomes useful for real-world applications!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 8: Using Trained Model for Recommendations\n",
            "======================================================================\n",
            "Using the trained model from Step 7...\n",
            "✓ Model ready for inference\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: USING THE TRAINED MODEL FOR RECOMMENDATIONS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step demonstrates how to use the trained NCF model to make recommendations.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 8.1 LOAD SAVED MODEL (Optional - if you want to load from disk)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 8: Using Trained Model for Recommendations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# If you saved a model and want to load it later, use this:\n",
        "# model_path_to_load = os.path.join(model_path, f'{model_name}.pth')\n",
        "# if os.path.exists(model_path_to_load):\n",
        "#     print(f\"Loading model from {model_path_to_load}...\")\n",
        "#     ncf_model = torch.load(model_path_to_load)\n",
        "#     if torch.cuda.is_available():\n",
        "#         ncf_model = ncf_model.cuda()\n",
        "#     ncf_model.eval()\n",
        "#     print(\"✓ Model loaded successfully!\")\n",
        "# else:\n",
        "#     print(\"Using currently trained model...\")\n",
        "\n",
        "# For now, we'll use the model we just trained\n",
        "print(\"Using the trained model from Step 7...\")\n",
        "model_path_to_load = os.path.join(model_path, f'{model_name}.pth')\n",
        "if os.path.exists(model_path_to_load):\n",
        "\n",
        "    print(\"✓ Model ready for inference\")\n",
        "    model = torch.load('models/NeuMF-end.pth' , weights_only=False)\n",
        "    model.eval()  # Important: set to evaluation mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.2: Recommendation Function\n",
            "======================================================================\n",
            "✓ get_top_k_recommendations() function defined\n",
            "  - Takes user ID and candidate items\n",
            "  - Returns top-K recommendations with scores\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.2 GET TOP-K RECOMMENDATIONS FOR A USER\n",
        "# ============================================================================\n",
        "\n",
        "def get_top_k_recommendations(model, user_id, item_ids, k=10, device='cpu'):\n",
        "    \"\"\"\n",
        "    Get top-K item recommendations for a given user.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - user_id: ID of the user (integer)\n",
        "    - item_ids: List of item IDs to consider (e.g., all items or candidate items)\n",
        "    - k: Number of recommendations to return\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - top_k_items: List of top-K recommended item IDs\n",
        "    - top_k_scores: List of corresponding prediction scores\n",
        "    \"\"\"\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    # Convert to tensors\n",
        "    user_tensor = torch.LongTensor([user_id] * len(item_ids))\n",
        "    item_tensor = torch.LongTensor(item_ids)\n",
        "    \n",
        "    # Move to device\n",
        "    if device == 'cuda' and torch.cuda.is_available():\n",
        "        user_tensor = user_tensor.cuda()\n",
        "        item_tensor = item_tensor.cuda()\n",
        "    \n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        scores = model(user_tensor, item_tensor)\n",
        "        scores = scores.cpu().numpy()\n",
        "    \n",
        "    # Get top-K items\n",
        "    top_k_indices = np.argsort(scores)[::-1][:k]  # Sort descending, take top K\n",
        "    top_k_items = [item_ids[i] for i in top_k_indices]\n",
        "    top_k_scores = scores[top_k_indices].tolist()\n",
        "    \n",
        "    return top_k_items, top_k_scores\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.2: Recommendation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ get_top_k_recommendations() function defined\")\n",
        "print(\"  - Takes user ID and candidate items\")\n",
        "print(\"  - Returns top-K recommendations with scores\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.3: Prediction Function\n",
            "======================================================================\n",
            "✓ predict_interaction_score() function defined\n",
            "  - Predicts score for a single user-item pair\n",
            "  - Returns a single score value\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.3 PREDICT USER-ITEM INTERACTION SCORE\n",
        "# ============================================================================\n",
        "\n",
        "def predict_interaction_score(model, user_id, item_id, device='cpu'):\n",
        "    \"\"\"\n",
        "    Predict the interaction score for a specific user-item pair.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - user_id: ID of the user (integer)\n",
        "    - item_id: ID of the item (integer)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - score: Prediction score (higher = more likely user will like item)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Convert to tensors\n",
        "    user_tensor = torch.LongTensor([user_id])\n",
        "    item_tensor = torch.LongTensor([item_id])\n",
        "    \n",
        "    # Move to device\n",
        "    if device == 'cuda' and torch.cuda.is_available():\n",
        "        user_tensor = user_tensor.cuda()\n",
        "        item_tensor = item_tensor.cuda()\n",
        "    \n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        score = model(user_tensor, item_tensor)\n",
        "        score = score.cpu().item()\n",
        "    \n",
        "    return score\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.3: Prediction Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ predict_interaction_score() function defined\")\n",
        "print(\"  - Predicts score for a single user-item pair\")\n",
        "print(\"  - Returns a single score value\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.4: Example Usage\n",
            "======================================================================\n",
            "\n",
            "Getting top-10 recommendations for User 0...\n",
            "\n",
            "✓ Top-10 Recommendations for User 0:\n",
            "----------------------------------------------------------------------\n",
            "  1. Item   565 - Score:  4.2599\n",
            "  2. Item   559 - Score:  3.9050\n",
            "  3. Item    33 - Score:  3.8329\n",
            "  4. Item   563 - Score:  3.7590\n",
            "  5. Item  1125 - Score:  3.5868\n",
            "  6. Item  1113 - Score:  3.5455\n",
            "  7. Item   344 - Score:  3.4968\n",
            "  8. Item  1811 - Score:  3.4611\n",
            "  9. Item   970 - Score:  3.4368\n",
            "  10. Item   299 - Score:  3.3611\n",
            "\n",
            "======================================================================\n",
            "Example: Predicting interaction score\n",
            "======================================================================\n",
            "User 0 - Item 565: Score = 4.2599\n",
            "  → Higher score means user is more likely to like this item\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.4 EXAMPLE: GET RECOMMENDATIONS FOR A USER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.4: Example Usage\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example: Get recommendations for user 0\n",
        "example_user_id = 0\n",
        "print(f\"\\nGetting top-{top_k} recommendations for User {example_user_id}...\")\n",
        "\n",
        "# Get all item IDs (excluding items user already interacted with in training)\n",
        "# In practice, you might want to filter out items the user has already seen\n",
        "all_item_ids = list(range(item_num))\n",
        "\n",
        "# Get top-K recommendations\n",
        "recommended_items, recommended_scores = get_top_k_recommendations(\n",
        "    model,\n",
        "    example_user_id,\n",
        "    all_item_ids,\n",
        "    k=top_k,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Top-{top_k} Recommendations for User {example_user_id}:\")\n",
        "print(\"-\" * 70)\n",
        "for i, (item_id, score) in enumerate(zip(recommended_items, recommended_scores), 1):\n",
        "    print(f\"  {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "\n",
        "# Example: Predict score for a specific user-item pair\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"Example: Predicting interaction score\")\n",
        "print(\"=\" * 70)\n",
        "example_item_id = recommended_items[0]  # Use the top recommended item\n",
        "score = predict_interaction_score(model, example_user_id, example_item_id, device=device)\n",
        "print(f\"User {example_user_id} - Item {example_item_id}: Score = {score:.4f}\")\n",
        "print(f\"  → Higher score means user is more likely to like this item\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 8.5: Filtered Recommendations\n",
            "======================================================================\n",
            "\n",
            "Getting filtered recommendations for User 10...\n",
            "(Excluding items user already interacted with in training)\n",
            "\n",
            "✓ Top-10 NEW Recommendations for User 10:\n",
            "----------------------------------------------------------------------\n",
            "  1. Item  2526 - Score:  3.8752\n",
            "  2. Item  2659 - Score:  3.4374\n",
            "  3. Item   939 - Score:  3.2481\n",
            "  4. Item  1325 - Score:  3.1528\n",
            "  5. Item  1650 - Score:  3.1314\n",
            "  6. Item   211 - Score:  2.9503\n",
            "  7. Item  2093 - Score:  2.9223\n",
            "  8. Item  1499 - Score:  2.9186\n",
            "  9. Item   278 - Score:  2.8437\n",
            "  10. Item   324 - Score:  2.7961\n",
            "\n",
            "======================================================================\n",
            "✓ Recommendation system ready!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 8.5 FILTER OUT ITEMS USER ALREADY INTERACTED WITH\n",
        "# ============================================================================\n",
        "\n",
        "def get_recommendations_excluding_training(user_id, model, train_mat, all_items, k=10, device='cpu'):\n",
        "    \"\"\"\n",
        "    Get recommendations for a user, excluding items they've already interacted with.\n",
        "    \n",
        "    This is more realistic - we don't want to recommend items the user already knows about.\n",
        "    \n",
        "    Parameters:\n",
        "    - user_id: ID of the user\n",
        "    - model: Trained NCF model\n",
        "    - train_mat: Training interaction matrix (to check what user already interacted with)\n",
        "    - all_items: List of all item IDs\n",
        "    - k: Number of recommendations\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - top_k_items: Recommended item IDs (excluding training items)\n",
        "    - top_k_scores: Corresponding scores\n",
        "    \"\"\"\n",
        "    # Filter out items user already interacted with\n",
        "    candidate_items = [item_id for item_id in all_items \n",
        "                       if (user_id, item_id) not in train_mat]\n",
        "    \n",
        "    if len(candidate_items) < k:\n",
        "        print(f\"Warning: Only {len(candidate_items)} candidate items available (requested {k})\")\n",
        "        k = len(candidate_items)\n",
        "    \n",
        "    # Get recommendations from candidate items\n",
        "    top_k_items, top_k_scores = get_top_k_recommendations(\n",
        "        model, user_id, candidate_items, k=k, device=device\n",
        "    )\n",
        "    \n",
        "    return top_k_items, top_k_scores\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 8.5: Filtered Recommendations\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example with filtering\n",
        "example_user_id = 10\n",
        "print(f\"\\nGetting filtered recommendations for User {example_user_id}...\")\n",
        "print(\"(Excluding items user already interacted with in training)\")\n",
        "\n",
        "filtered_items, filtered_scores = get_recommendations_excluding_training(\n",
        "    example_user_id,\n",
        "    ncf_model,\n",
        "    train_mat,\n",
        "    list(range(item_num)),\n",
        "    k=top_k,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Top-{top_k} NEW Recommendations for User {example_user_id}:\")\n",
        "print(\"-\" * 70)\n",
        "for i, (item_id, score) in enumerate(zip(filtered_items, filtered_scores), 1):\n",
        "    print(f\"  {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"✓ Recommendation system ready!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 8:\n",
        "\n",
        "#### 8.1 Loading a Saved Model\n",
        "\n",
        "**When to Load:**\n",
        "- After training, if you saved the model and want to use it later\n",
        "- When deploying the model in production\n",
        "- When sharing the model with others\n",
        "\n",
        "**How to Load:**\n",
        "```python\n",
        "model = torch.load('path/to/model.pth')\n",
        "model.eval()  # Important: set to evaluation mode\n",
        "```\n",
        "\n",
        "**Why `model.eval()`?**\n",
        "- Disables dropout (uses all neurons)\n",
        "- Uses deterministic behavior\n",
        "- Required for consistent predictions\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.2 Getting Top-K Recommendations\n",
        "\n",
        "**How It Works:**\n",
        "1. **Input**: User ID and list of candidate items\n",
        "2. **Process**: \n",
        "   - Get model predictions for all candidate items\n",
        "   - Sort items by prediction score (descending)\n",
        "   - Return top-K items\n",
        "3. **Output**: List of recommended item IDs and their scores\n",
        "\n",
        "**Example Use Case:**\n",
        "```python\n",
        "# Recommend 10 movies for user 123\n",
        "recommendations, scores = get_top_k_recommendations(\n",
        "    model, user_id=123, item_ids=all_movie_ids, k=10\n",
        ")\n",
        "```\n",
        "\n",
        "**Performance Considerations:**\n",
        "- If you have many items (millions), consider:\n",
        "  - Pre-filtering candidates (e.g., by genre, popularity)\n",
        "  - Using approximate nearest neighbor search\n",
        "  - Caching embeddings for faster computation\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.3 Predicting Single Interaction Score\n",
        "\n",
        "**When to Use:**\n",
        "- Check if a specific user will like a specific item\n",
        "- Rank a small set of items\n",
        "- A/B testing different items\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "score = predict_interaction_score(model, user_id=123, item_id=456)\n",
        "if score > 0.5:  # Threshold (adjust based on your data)\n",
        "    print(\"User will likely like this item\")\n",
        "```\n",
        "\n",
        "**Interpreting Scores:**\n",
        "- Higher score = more likely user will like item\n",
        "- Scores are logits (not probabilities)\n",
        "- To get probabilities: `prob = torch.sigmoid(torch.tensor(score))`\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.4 Filtering Training Items\n",
        "\n",
        "**Why Filter?**\n",
        "- Users have already seen/interacted with training items\n",
        "- We want to recommend NEW items\n",
        "- More realistic recommendation scenario\n",
        "\n",
        "**How It Works:**\n",
        "1. Check training matrix: `(user_id, item_id) in train_mat`\n",
        "2. Exclude items user already interacted with\n",
        "3. Get recommendations from remaining items\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Get new recommendations (excluding items user already knows)\n",
        "recommendations = get_recommendations_excluding_training(\n",
        "    user_id, model, train_mat, all_items, k=10\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 8.5 Real-World Usage Tips\n",
        "\n",
        "**1. Pre-compute Embeddings (Optional):**\n",
        "```python\n",
        "# For faster recommendations, pre-compute item embeddings\n",
        "with torch.no_grad():\n",
        "    item_embeddings = model.embed_item_GMF(torch.arange(item_num))\n",
        "# Then use these for faster similarity search\n",
        "```\n",
        "\n",
        "**2. Batch Predictions:**\n",
        "```python\n",
        "# Predict for multiple users at once (faster)\n",
        "user_ids = torch.LongTensor([1, 2, 3, 4, 5])\n",
        "item_ids = torch.LongTensor([10, 20, 30, 40, 50])\n",
        "scores = model(user_ids, item_ids)  # Batch prediction\n",
        "```\n",
        "\n",
        "**3. Cold Start Problem:**\n",
        "- New users: No interaction history\n",
        "- Solutions: Use popularity-based recommendations, ask for preferences\n",
        "- New items: No interaction history\n",
        "- Solutions: Use content-based features, wait for initial interactions\n",
        "\n",
        "**4. Evaluation in Production:**\n",
        "- A/B testing: Compare different models\n",
        "- Online metrics: Click-through rate, conversion rate\n",
        "- Offline metrics: HR, NDCG (what we used)\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 8 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Functions to get recommendations\n",
        "- Functions to predict interaction scores\n",
        "- Example usage code\n",
        "- Understanding of how to use the model in practice\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 Congratulations! You've completed the full NCF Tutorial!**\n",
        "\n",
        "**What You've Learned:**\n",
        "1. ✅ Environment setup and imports\n",
        "2. ✅ Configuration and hyperparameters\n",
        "3. ✅ Data downloading and preprocessing\n",
        "4. ✅ PyTorch Dataset class implementation\n",
        "5. ✅ NCF model architecture (with detailed visualization)\n",
        "6. ✅ Evaluation metrics (Hit Rate and NDCG)\n",
        "7. ✅ Training loop implementation\n",
        "8. ✅ Using the model for recommendations\n",
        "\n",
        "**The Complete Pipeline:**\n",
        "```\n",
        "Data → Preprocessing → Dataset → Model → Training → Evaluation → Recommendations\n",
        "```\n",
        "\n",
        "**Next Steps:**\n",
        "- Experiment with different hyperparameters\n",
        "- Try different model architectures (GMF, MLP, NeuMF)\n",
        "- Test on different datasets\n",
        "- Deploy for production use\n",
        "- Explore advanced techniques (attention mechanisms, graph neural networks)\n",
        "\n",
        "**Thank you for following along! Happy recommending! 🚀**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 9: Loading and Using Saved Models\n",
        "\n",
        "This final step shows how to load the saved models and use them for recommendations. All models (GMF, MLP, NeuMF-end, NeuMF-pre) have been saved and can be loaded independently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 9: Loading and Using Saved Models\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 9.1: Loading Saved Models\n",
            "======================================================================\n",
            "Loading GMF model from ./models/GMF.pth...\n"
          ]
        },
        {
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.NCF was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.NCF])` or the `torch.serialization.safe_globals([__main__.NCF])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(GMF_model_path):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading GMF model from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGMF_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     gmf_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGMF_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     24\u001b[0m         gmf_loaded \u001b[38;5;241m=\u001b[39m gmf_loaded\u001b[38;5;241m.\u001b[39mcpu()\n",
            "File \u001b[0;32m~/Documents/Codes/thesis/NCF/.venv/lib/python3.9/site-packages/torch/serialization.py:1529\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1522\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1523\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1527\u001b[0m                 )\n\u001b[1;32m   1528\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1529\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1531\u001b[0m             opened_zipfile,\n\u001b[1;32m   1532\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1536\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.NCF was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.NCF])` or the `torch.serialization.safe_globals([__main__.NCF])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 9: LOADING AND USING SAVED MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 9: Loading and Using Saved Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# ============================================================================\n",
        "# 9.1 LOAD ALL SAVED MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 9.1: Loading Saved Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "loaded_models = {}\n",
        "\n",
        "# Load GMF model\n",
        "if os.path.exists(GMF_model_path):\n",
        "    print(f\"Loading GMF model from {GMF_model_path}...\")\n",
        "    gmf_loaded = torch.load(GMF_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        gmf_loaded = gmf_loaded.cpu()\n",
        "    gmf_loaded.eval()\n",
        "    loaded_models['GMF'] = gmf_loaded\n",
        "    print(\"✓ GMF model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ GMF model not found at {GMF_model_path}\")\n",
        "\n",
        "# Load MLP model\n",
        "if os.path.exists(MLP_model_path):\n",
        "    print(f\"Loading MLP model from {MLP_model_path}...\")\n",
        "    mlp_loaded = torch.load(MLP_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        mlp_loaded = mlp_loaded.cpu()\n",
        "    mlp_loaded.eval()\n",
        "    loaded_models['MLP'] = mlp_loaded\n",
        "    print(\"✓ MLP model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ MLP model not found at {MLP_model_path}\")\n",
        "\n",
        "# Load NeuMF-end model\n",
        "neumf_end_path = os.path.join(model_path, 'NeuMF-end.pth')\n",
        "if os.path.exists(neumf_end_path):\n",
        "    print(f\"Loading NeuMF-end model from {neumf_end_path}...\")\n",
        "    neumf_end_loaded = torch.load(neumf_end_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        neumf_end_loaded = neumf_end_loaded.cpu()\n",
        "    neumf_end_loaded.eval()\n",
        "    loaded_models['NeuMF-end'] = neumf_end_loaded\n",
        "    print(\"✓ NeuMF-end model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ NeuMF-end model not found at {neumf_end_path}\")\n",
        "\n",
        "# Load NeuMF-pre model\n",
        "if os.path.exists(NeuMF_model_path):\n",
        "    print(f\"Loading NeuMF-pre model from {NeuMF_model_path}...\")\n",
        "    neumf_pre_loaded = torch.load(NeuMF_model_path, map_location=device)\n",
        "    if device == 'cpu':\n",
        "        neumf_pre_loaded = neumf_pre_loaded.cpu()\n",
        "    neumf_pre_loaded.eval()\n",
        "    loaded_models['NeuMF-pre'] = neumf_pre_loaded\n",
        "    print(\"✓ NeuMF-pre model loaded\")\n",
        "else:\n",
        "    print(f\"⚠ NeuMF-pre model not found at {NeuMF_model_path}\")\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(loaded_models)} model(s)\")\n",
        "print(f\"  Available models: {list(loaded_models.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 9.2 COMPARE RECOMMENDATIONS FROM ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 9.2: Comparing Recommendations from All Models\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Example user\n",
        "example_user_id = 0\n",
        "print(f\"\\nGetting recommendations for User {example_user_id} using all models...\")\n",
        "\n",
        "# Get recommendations from each model\n",
        "all_recommendations = {}\n",
        "\n",
        "for model_name, model in loaded_models.items():\n",
        "    if model_name in loaded_models:\n",
        "        print(f\"\\n{model_name} recommendations:\")\n",
        "        recommendations, scores = get_top_k_recommendations(\n",
        "            model, example_user_id, list(range(item_num)), k=top_k, device=device\n",
        "        )\n",
        "        all_recommendations[model_name] = (recommendations, scores)\n",
        "        \n",
        "        print(f\"  Top-{top_k} items:\")\n",
        "        for i, (item_id, score) in enumerate(zip(recommendations[:5], scores[:5]), 1):\n",
        "            print(f\"    {i}. Item {item_id:5d} - Score: {score:7.4f}\")\n",
        "        if top_k > 5:\n",
        "            print(f\"    ... and {top_k - 5} more\")\n",
        "\n",
        "# Compare overlap between models\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Recommendation Overlap Analysis\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if len(all_recommendations) >= 2:\n",
        "    model_names = list(all_recommendations.keys())\n",
        "    for i, model1 in enumerate(model_names):\n",
        "        for model2 in model_names[i+1:]:\n",
        "            items1 = set(all_recommendations[model1][0])\n",
        "            items2 = set(all_recommendations[model2][0])\n",
        "            overlap = len(items1 & items2)\n",
        "            print(f\"{model1} vs {model2}: {overlap}/{top_k} items overlap ({overlap/top_k*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
