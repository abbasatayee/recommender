{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Walk up the directory tree until we find 'src'\n",
        "path = current_dir\n",
        "src_path = None\n",
        "\n",
        "while True:\n",
        "    if os.path.basename(path) == \"src\":\n",
        "        src_path = path\n",
        "        break\n",
        "    parent = os.path.dirname(path)\n",
        "    if parent == path:  # reached filesystem root\n",
        "        break\n",
        "    path = parent\n",
        "\n",
        "# Add src to sys.path if found\n",
        "if src_path and src_path not in sys.path:\n",
        "    sys.path.insert(0, src_path)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "\n",
        "# Local imports\n",
        "from helpers import download_ml1m_dataset\n",
        "from utils.ml_to_ncf import preprocess_ml1m_to_ncf_format\n",
        "from utils.ncfdata import NCFData\n",
        "from helpers.ncf_model import NCF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model architecture: NeuMF-end\n",
            "✓ Directories configured\n",
            "  - Data directory: /Users/abbas/Documents/Codes/thesis/recommender/src/../data (will be created/used for downloaded data)\n",
            "  - Model save path: /Users/abbas/Documents/Codes/thesis/recommender/src/../models\n"
          ]
        }
      ],
      "source": [
        "dataset = 'ml-1m'\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Choose which model architecture to use\n",
        "# Options:\n",
        "#   - 'MLP': Multi-Layer Perceptron only (non-linear interactions)\n",
        "#   - 'GMF': Generalized Matrix Factorization only (linear interactions)\n",
        "#   - 'NeuMF-end': Neural Matrix Factorization trained from scratch (end-to-end)\n",
        "#   - 'NeuMF-pre': Neural Matrix Factorization with pre-trained GMF and MLP models\n",
        "model_name = 'NeuMF-end'\n",
        "assert model_name in ['MLP', 'GMF', 'NeuMF-end', 'NeuMF-pre'], \\\n",
        "    f\"Model must be 'MLP', 'GMF', 'NeuMF-end', or 'NeuMF-pre', got '{model_name}'\"\n",
        "\n",
        "print(f\"✓ Model architecture: {model_name}\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATA AND MODEL PATHS CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Data will be downloaded automatically during training\n",
        "# We'll create a local data directory to store downloaded files\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "# Model saving directory\n",
        "model_path = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "GMF_model_path = os.path.join(model_path, 'GMF.pth')\n",
        "MLP_model_path = os.path.join(model_path, 'MLP.pth')\n",
        "NeuMF_model_path = os.path.join(model_path, 'NeuMF.pth')\n",
        "\n",
        "print(f\"✓ Directories configured\")\n",
        "print(f\"  - Data directory: {data_dir} (will be created/used for downloaded data)\")\n",
        "print(f\"  - Model save path: {model_path}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2.4 TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# Learning rate: Controls how big steps the optimizer takes during training\n",
        "# Too high: training might be unstable or diverge\n",
        "# Too low: training will be very slow\n",
        "# Typical range: 0.0001 to 0.01\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dropout rate: Regularization technique to prevent overfitting\n",
        "# Randomly sets some neurons to zero during training\n",
        "# Range: 0.0 (no dropout) to 0.9 (very aggressive dropout)\n",
        "# 0.0 means no dropout (all neurons active)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Batch size: Number of training examples processed together in one iteration\n",
        "# Larger batch size: more stable gradients, but requires more memory\n",
        "# Smaller batch size: less memory, but noisier gradients\n",
        "# Typical values: 64, 128, 256, 512\n",
        "batch_size = 256\n",
        "\n",
        "# Number of training epochs: How many times we'll iterate through the entire dataset\n",
        "# More epochs: better learning, but risk of overfitting\n",
        "# Too few epochs: model might not learn enough\n",
        "epochs = 20\n",
        "\n",
        "# Top-K for evaluation: When evaluating, we recommend top K items to each user\n",
        "# We measure if the true item is in the top K recommendations\n",
        "# Common values: 5, 10, 20\n",
        "top_k = 10\n",
        "\n",
        "# Factor number: Dimension of the embedding vectors for users and items\n",
        "# Larger: more capacity to learn complex patterns, but more parameters\n",
        "# Smaller: fewer parameters, faster training, but less capacity\n",
        "# Common values: 8, 16, 32, 64\n",
        "factor_num = 32\n",
        "\n",
        "# Number of MLP layers: Depth of the Multi-Layer Perceptron component\n",
        "# More layers: can learn more complex non-linear patterns\n",
        "# Fewer layers: simpler model, faster training\n",
        "# Typical range: 1 to 5 layers\n",
        "num_layers = 3\n",
        "\n",
        "# Number of negative samples for training: For each positive (user, item) pair,\n",
        "# we sample this many negative items (items the user hasn't interacted with)\n",
        "# More negatives: better learning signal, but slower training\n",
        "# Fewer negatives: faster training, but potentially weaker learning\n",
        "# Common values: 1, 4, 8\n",
        "num_ng = 4\n",
        "\n",
        "# Number of negative samples for testing: During evaluation, for each test item,\n",
        "# we also provide this many negative items. The model should rank the true item higher.\n",
        "# Typically 99 negatives + 1 positive = 100 items total per test case\n",
        "test_num_ng = 99\n",
        "\n",
        "# Whether to save the trained model\n",
        "save_model = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 3.1: Downloading MovieLens 1M Dataset\n",
            "======================================================================\n",
            "✓ Dataset already exists at /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 70)\n",
        "print(\"STEP 3.1: Downloading MovieLens 1M Dataset\")\n",
        "print(\"=\" * 70)\n",
        "ratings_file = download_ml1m_dataset(data_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Loaded 1000209 ratings\n",
            "  - Unique users: 6040\n",
            "  - Unique movies: 3706\n",
            "\n",
            "Filtering positive interactions (ratings >= 4)...\n",
            "✓ 575281 positive interactions (out of 1000209 total)\n",
            "\n",
            "Remapping user and item IDs to be contiguous...\n",
            "✓ Remapped to 6038 users and 3533 items\n",
            "\n",
            "Splitting data (train: 80%, test: 20%)...\n",
            "✓ Training pairs: 460225\n",
            "✓ Test pairs: 115056\n",
            "\n",
            "Saving training data to /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Saved 460225 training pairs\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Generating test negative samples (99 negatives per test case)...\n",
            "✓ Generated test negative samples: 115056 test cases\n"
          ]
        }
      ],
      "source": [
        "train_rating_path, test_rating_path, test_negative_path, user_num, item_num, train_mat = \\\n",
        "    preprocess_ml1m_to_ncf_format(ratings_file, data_dir, test_ratio=0.2, test_negatives=99)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.train.rating...\n",
            "✓ Loaded 460225 training pairs\n",
            "  - Users: 6038\n",
            "  - Items: 3533\n",
            "\n",
            "Creating training interaction matrix...\n",
            "✓ Training matrix created: 460225 interactions\n",
            "\n",
            "Loading test data from /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m.test.negative...\n",
            "✓ Loaded 11505600 test pairs (including negatives)\n",
            "\n",
            "======================================================================\n",
            "✓ Data loading complete!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def load_all_data(train_rating_path, test_negative_path):    \n",
        "    # Load training data\n",
        "    print(f\"Loading training data from {train_rating_path}...\")\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating_path,\n",
        "        sep='\\t',\n",
        "        header=None,\n",
        "        names=['user', 'item'],\n",
        "        usecols=[0, 1],\n",
        "        dtype={0: np.int32, 1: np.int32}\n",
        "    )\n",
        "    \n",
        "    # Calculate number of users and items\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "    \n",
        "    print(f\"✓ Loaded {len(train_data)} training pairs\")\n",
        "    print(f\"  - Users: {user_num}\")\n",
        "    print(f\"  - Items: {item_num}\")\n",
        "    \n",
        "    # Convert to list of lists for easier processing\n",
        "    train_data = train_data.values.tolist()\n",
        "    \n",
        "    # Create sparse training matrix (Dictionary of Keys format)\n",
        "    # This is used to quickly check if a user-item pair exists in training data\n",
        "    print(\"\\nCreating training interaction matrix...\")\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for u, i in train_data:\n",
        "        train_mat[u, i] = 1.0\n",
        "    print(f\"✓ Training matrix created: {train_mat.nnz} interactions\")\n",
        "    \n",
        "    # Load test data with negative samples\n",
        "    print(f\"\\nLoading test data from {test_negative_path}...\")\n",
        "    test_data = []\n",
        "    with open(test_negative_path, 'r') as fd:\n",
        "        line = fd.readline()\n",
        "        while line is not None and line != '':\n",
        "            # Format: (user, item)\\tneg1\\tneg2\\t...\\tneg99\n",
        "            arr = line.strip().split('\\t')\n",
        "            \n",
        "            # Parse the positive pair: (user, item)\n",
        "            # eval() converts string \"(123, 456)\" to tuple (123, 456)\n",
        "            positive_pair = eval(arr[0])\n",
        "            u = positive_pair[0]\n",
        "            i = positive_pair[1]\n",
        "            \n",
        "            # Add the positive pair\n",
        "            test_data.append([u, i])\n",
        "            \n",
        "            # Add all negative items for this user\n",
        "            for neg_item in arr[1:]:\n",
        "                if neg_item:  # Skip empty strings\n",
        "                    test_data.append([u, int(neg_item)])\n",
        "            \n",
        "            line = fd.readline()\n",
        "    \n",
        "    print(f\"✓ Loaded {len(test_data)} test pairs (including negatives)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"✓ Data loading complete!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    return train_data, test_data, user_num, item_num, train_mat\n",
        "\n",
        "# Load all data\n",
        "train_data, test_data, user_num, item_num, train_mat = load_all_data(\n",
        "    train_rating_path, test_negative_path\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = NCFData(\n",
        "    train_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=num_ng,  # From Step 2 configuration\n",
        "    is_training=True\n",
        ")\n",
        "\n",
        "test_dataset = NCFData(\n",
        "    test_data,\n",
        "    item_num,\n",
        "    train_mat,\n",
        "    num_ng=0,  # No negative sampling for testing\n",
        "    is_training=False\n",
        ")\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,  # From Step 2 configuration\n",
        "    shuffle=True,  # Shuffle training data each epoch\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks (avoids pickling errors)\n",
        "    pin_memory=True if torch.cuda.is_available() else False  # Faster GPU transfer\n",
        ")\n",
        "\n",
        "test_loader = data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_num_ng + 1,  # 1 positive + test_num_ng negatives\n",
        "    shuffle=False,  # Don't shuffle test data\n",
        "    num_workers=0,  # MUST be 0 for Jupyter notebooks\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "# Verify num_workers is 0 (safety check)\n",
        "assert train_loader.num_workers == 0, f\"ERROR: train_loader.num_workers is {train_loader.num_workers}, must be 0!\"\n",
        "assert test_loader.num_workers == 0, f\"ERROR: test_loader.num_workers is {test_loader.num_workers}, must be 0!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 5.2: Creating and Initializing Model\n",
            "======================================================================\n",
            "✓ Model on CPU\n",
            "\n",
            "✓ Model created successfully!\n",
            "  - Total parameters: 1,574,657\n",
            "  - Trainable parameters: 1,574,657\n",
            "\n",
            "Model Architecture:\n",
            "  - Users: 6,038\n",
            "  - Items: 3,533\n",
            "  - GMF embeddings: 32 dimensions\n",
            "  - MLP embeddings: 128 dimensions\n",
            "  - MLP layers: 3 (with dropout=0.1)\n",
            "  - Prediction layer: 64 → 1\n",
            "\n",
            "======================================================================\n",
            "COMPLETE MODEL STRUCTURE:\n",
            "======================================================================\n",
            "NCF(\n",
            "  (embed_user_GMF): Embedding(6038, 32)\n",
            "  (embed_item_GMF): Embedding(3533, 32)\n",
            "  (embed_user_MLP): Embedding(6038, 128)\n",
            "  (embed_item_MLP): Embedding(3533, 128)\n",
            "  (MLP_layers): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Dropout(p=0.1, inplace=False)\n",
            "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Dropout(p=0.1, inplace=False)\n",
            "    (7): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (8): ReLU()\n",
            "  )\n",
            "  (predict_layer): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "LAYER-BY-LAYER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "[GMF Path - Generalized Matrix Factorization]\n",
            "  embed_user_GMF: Embedding(6038, 32)\n",
            "    → Converts user IDs to 32-dimensional vectors\n",
            "  embed_item_GMF: Embedding(3533, 32)\n",
            "    → Converts item IDs to 32-dimensional vectors\n",
            "  Element-wise product: user_emb * item_emb\n",
            "    → Output shape: [batch_size, 32]\n",
            "\n",
            "[MLP Path - Multi-Layer Perceptron]\n",
            "  embed_user_MLP: Embedding(6038, 128)\n",
            "    → Converts user IDs to 128-dimensional vectors\n",
            "  embed_item_MLP: Embedding(3533, 128)\n",
            "    → Converts item IDs to 128-dimensional vectors\n",
            "  Concatenation: [user_emb, item_emb]\n",
            "    → Output shape: [batch_size, 256]\n",
            "\n",
            "  MLP Layers (3 layers):\n",
            "    Layer 1:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(256, 128)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 128]\n",
            "    Layer 2:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(128, 64)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 64]\n",
            "    Layer 3:\n",
            "      Dropout(p=0.1)\n",
            "      Linear(64, 32)\n",
            "      ReLU()\n",
            "      → Output shape: [batch_size, 32]\n",
            "\n",
            "[Prediction Layer]\n",
            "  Input: Concatenated GMF + MLP [64 dimensions]\n",
            "    → GMF: [32 dims] + MLP: [32 dims]\n",
            "  Linear(64, 1)\n",
            "    → Output: [batch_size, 1] (interaction score)\n",
            "    → Higher score = more likely user will like item\n",
            "\n",
            "======================================================================\n",
            "PARAMETER BREAKDOWN:\n",
            "======================================================================\n",
            "\n",
            "GMF Embeddings:\n",
            "  User embeddings: 6,038 × 32 = 193,216 parameters\n",
            "  Item embeddings: 3,533 × 32 = 113,056 parameters\n",
            "  GMF Total: 306,272 parameters\n",
            "\n",
            "MLP Embeddings:\n",
            "  User embeddings: 6,038 × 128 = 772,864 parameters\n",
            "  Item embeddings: 3,533 × 128 = 452,224 parameters\n",
            "  MLP Embeddings Total: 1,225,088 parameters\n",
            "\n",
            "MLP Layers:\n",
            "  Layer 1 (Linear(256, 128)): 32,896 parameters\n",
            "  Layer 2 (Linear(128, 64)): 8,256 parameters\n",
            "  Layer 3 (Linear(64, 32)): 2,080 parameters\n",
            "  MLP Layers Total: 43,232 parameters\n",
            "\n",
            "Prediction Layer:\n",
            "  Linear(64, 1): 65 parameters\n",
            "\n",
            "======================================================================\n",
            "TOTAL MODEL PARAMETERS: 1,574,657\n",
            "Model Size (float32): ~6.01 MB\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 5.2 CREATE AND INITIALIZE THE MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 5.2: Creating and Initializing Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if we need pre-trained models (for NeuMF-pre)\n",
        "if model_name == 'NeuMF-pre':\n",
        "    # For NeuMF-pre, we would load pre-trained GMF and MLP models\n",
        "    # For now, we'll use NeuMF-end (training from scratch)\n",
        "    print(\"⚠ NeuMF-pre requires pre-trained models.\")\n",
        "    print(\"  Switching to NeuMF-end (training from scratch)...\")\n",
        "    model_name = 'NeuMF-end'\n",
        "\n",
        "# Create the model\n",
        "ncf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name=model_name,\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    ncf_model = ncf_model.cuda()\n",
        "    print(\"✓ Model moved to GPU\")\n",
        "else:\n",
        "    print(\"✓ Model on CPU\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n✓ Model created successfully!\")\n",
        "print(f\"  - Total parameters: {total_params:,}\")\n",
        "print(f\"  - Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  - Users: {user_num:,}\")\n",
        "print(f\"  - Items: {item_num:,}\")\n",
        "if model_name != 'MLP':\n",
        "    print(f\"  - GMF embeddings: {factor_num} dimensions\")\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(f\"  - MLP embeddings: {mlp_embed_dim} dimensions\")\n",
        "    print(f\"  - MLP layers: {num_layers} (with dropout={dropout_rate})\")\n",
        "print(f\"  - Prediction layer: {factor_num if model_name in ['MLP', 'GMF'] else factor_num * 2} → 1\")\n",
        "\n",
        "\n",
        "\n",
        "# Print the full model structure\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPLETE MODEL STRUCTURE:\")\n",
        "print(\"=\" * 70)\n",
        "print(ncf_model)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Print detailed layer information\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"LAYER-BY-LAYER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "if model_name != 'MLP':\n",
        "    print(\"\\n[GMF Path - Generalized Matrix Factorization]\")\n",
        "    print(f\"  embed_user_GMF: Embedding({user_num}, {factor_num})\")\n",
        "    print(f\"    → Converts user IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  embed_item_GMF: Embedding({item_num}, {factor_num})\")\n",
        "    print(f\"    → Converts item IDs to {factor_num}-dimensional vectors\")\n",
        "    print(f\"  Element-wise product: user_emb * item_emb\")\n",
        "    print(f\"    → Output shape: [batch_size, {factor_num}]\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    print(\"\\n[MLP Path - Multi-Layer Perceptron]\")\n",
        "    print(f\"  embed_user_MLP: Embedding({user_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts user IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  embed_item_MLP: Embedding({item_num}, {mlp_embed_dim})\")\n",
        "    print(f\"    → Converts item IDs to {mlp_embed_dim}-dimensional vectors\")\n",
        "    print(f\"  Concatenation: [user_emb, item_emb]\")\n",
        "    print(f\"    → Output shape: [batch_size, {mlp_embed_dim * 2}]\")\n",
        "    \n",
        "    print(f\"\\n  MLP Layers ({num_layers} layers):\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        print(f\"    Layer {i+1}:\")\n",
        "        print(f\"      Dropout(p={dropout_rate})\")\n",
        "        print(f\"      Linear({input_size}, {output_size})\")\n",
        "        print(f\"      ReLU()\")\n",
        "        print(f\"      → Output shape: [batch_size, {output_size}]\")\n",
        "\n",
        "print(\"\\n[Prediction Layer]\")\n",
        "if model_name == 'GMF':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: GMF output [{factor_num} dimensions]\")\n",
        "elif model_name == 'MLP':\n",
        "    predict_input = factor_num\n",
        "    print(f\"  Input: MLP output [{factor_num} dimensions]\")\n",
        "else:  # NeuMF\n",
        "    predict_input = factor_num * 2\n",
        "    print(f\"  Input: Concatenated GMF + MLP [{factor_num * 2} dimensions]\")\n",
        "    print(f\"    → GMF: [{factor_num} dims] + MLP: [{factor_num} dims]\")\n",
        "\n",
        "print(f\"  Linear({predict_input}, 1)\")\n",
        "print(f\"    → Output: [batch_size, 1] (interaction score)\")\n",
        "print(f\"    → Higher score = more likely user will like item\")\n",
        "\n",
        "# Calculate and print parameter breakdown\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"PARAMETER BREAKDOWN:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "total_params = 0\n",
        "if model_name != 'MLP':\n",
        "    gmf_user_params = user_num * factor_num\n",
        "    gmf_item_params = item_num * factor_num\n",
        "    gmf_total = gmf_user_params + gmf_item_params\n",
        "    total_params += gmf_total\n",
        "    print(f\"\\nGMF Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {factor_num} = {gmf_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {factor_num} = {gmf_item_params:,} parameters\")\n",
        "    print(f\"  GMF Total: {gmf_total:,} parameters\")\n",
        "\n",
        "if model_name != 'GMF':\n",
        "    mlp_embed_dim = factor_num * (2 ** (num_layers - 1))\n",
        "    mlp_user_params = user_num * mlp_embed_dim\n",
        "    mlp_item_params = item_num * mlp_embed_dim\n",
        "    mlp_embed_total = mlp_user_params + mlp_item_params\n",
        "    total_params += mlp_embed_total\n",
        "    print(f\"\\nMLP Embeddings:\")\n",
        "    print(f\"  User embeddings: {user_num:,} × {mlp_embed_dim} = {mlp_user_params:,} parameters\")\n",
        "    print(f\"  Item embeddings: {item_num:,} × {mlp_embed_dim} = {mlp_item_params:,} parameters\")\n",
        "    print(f\"  MLP Embeddings Total: {mlp_embed_total:,} parameters\")\n",
        "    \n",
        "    # MLP layers parameters\n",
        "    mlp_layer_params = 0\n",
        "    print(f\"\\nMLP Layers:\")\n",
        "    for i in range(num_layers):\n",
        "        input_size = factor_num * (2 ** (num_layers - i))\n",
        "        output_size = input_size // 2\n",
        "        layer_params = (input_size * output_size) + output_size  # weights + bias\n",
        "        mlp_layer_params += layer_params\n",
        "        print(f\"  Layer {i+1} (Linear({input_size}, {output_size})): {layer_params:,} parameters\")\n",
        "    total_params += mlp_layer_params\n",
        "    print(f\"  MLP Layers Total: {mlp_layer_params:,} parameters\")\n",
        "\n",
        "# Prediction layer\n",
        "if model_name in ['MLP', 'GMF']:\n",
        "    predict_input = factor_num\n",
        "else:\n",
        "    predict_input = factor_num * 2\n",
        "predict_params = (predict_input * 1) + 1  # weights + bias\n",
        "total_params += predict_params\n",
        "print(f\"\\nPrediction Layer:\")\n",
        "print(f\"  Linear({predict_input}, 1): {predict_params:,} parameters\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"TOTAL MODEL PARAMETERS: {total_params:,}\")\n",
        "print(f\"Model Size (float32): ~{total_params * 4 / 1024 / 1024:.2f} MB\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.1: Hit Rate Metric\n",
            "======================================================================\n",
            "✓ Hit Rate function defined\n",
            "  - Returns 1 if true item is in top-K recommendations\n",
            "  - Returns 0 otherwise\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: EVALUATION METRICS\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements evaluation metrics for recommendation systems:\n",
        "- Hit Rate (HR@K): Binary metric - is the true item in top K?\n",
        "- NDCG (Normalized Discounted Cumulative Gain@K): Ranking quality metric\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 6.1 HIT RATE METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def hit(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Hit Rate for a single test case.\n",
        "    \n",
        "    Hit Rate is 1 if the ground truth item is in the predicted top-K items,\n",
        "    otherwise 0.\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - 1 if gt_item is in pred_items, 0 otherwise\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.1: Hit Rate Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Hit Rate function defined\")\n",
        "print(\"  - Returns 1 if true item is in top-K recommendations\")\n",
        "print(\"  - Returns 0 otherwise\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.2: NDCG Metric\n",
            "======================================================================\n",
            "✓ NDCG function defined\n",
            "  - Measures ranking quality\n",
            "  - Higher score for items ranked higher\n",
            "  - Returns 0 if true item not in recommendations\n",
            "\n",
            "NDCG Examples:\n",
            "  Top-5 recommendations: [10, 20, 30, 40, 50]\n",
            "  If true item is at position 0: NDCG = 1.000\n",
            "  If true item is at position 2: NDCG = 0.500\n",
            "  If true item is at position 4: NDCG = 0.387\n",
            "  If true item not in list: NDCG = 0.000\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.2 NDCG METRIC\n",
        "# ============================================================================\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "    \"\"\"\n",
        "    Calculate Normalized Discounted Cumulative Gain (NDCG) for a single test case.\n",
        "    \n",
        "    NDCG measures ranking quality by:\n",
        "    1. Giving more weight to items ranked higher (position matters)\n",
        "    2. Using logarithmic discounting (relevance decreases with position)\n",
        "    \n",
        "    Formula: NDCG = 1 / log2(position + 2)\n",
        "    - Position 0 (top): 1 / log2(2) = 1.0\n",
        "    - Position 1: 1 / log2(3) ≈ 0.63\n",
        "    - Position 2: 1 / log2(4) = 0.5\n",
        "    - Position 9: 1 / log2(11) ≈ 0.29\n",
        "    \n",
        "    Parameters:\n",
        "    - gt_item: Ground truth item ID (the item user actually interacted with)\n",
        "    - pred_items: List of top-K predicted item IDs (recommended items)\n",
        "    \n",
        "    Returns:\n",
        "    - NDCG score (0.0 to 1.0) if gt_item is in pred_items\n",
        "    - 0.0 if gt_item is not in pred_items\n",
        "    \"\"\"\n",
        "    if gt_item in pred_items:\n",
        "        # Find the position (index) of the ground truth item\n",
        "        index = pred_items.index(gt_item)\n",
        "        # Calculate NDCG: 1 / log2(position + 2)\n",
        "        # +2 because: position 0 should give 1/log2(2) = 1.0\n",
        "        return np.reciprocal(np.log2(index + 2))\n",
        "    return 0.0\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.2: NDCG Metric\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ NDCG function defined\")\n",
        "print(\"  - Measures ranking quality\")\n",
        "print(\"  - Higher score for items ranked higher\")\n",
        "print(\"  - Returns 0 if true item not in recommendations\")\n",
        "\n",
        "# Example to demonstrate NDCG\n",
        "print(\"\\nNDCG Examples:\")\n",
        "example_items = [10, 20, 30, 40, 50]\n",
        "print(f\"  Top-5 recommendations: {example_items}\")\n",
        "print(f\"  If true item is at position 0: NDCG = {ndcg(10, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 2: NDCG = {ndcg(30, example_items):.3f}\")\n",
        "print(f\"  If true item is at position 4: NDCG = {ndcg(50, example_items):.3f}\")\n",
        "print(f\"  If true item not in list: NDCG = {ndcg(99, example_items):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 6.3: Evaluation Function\n",
            "======================================================================\n",
            "✓ Evaluation function defined\n",
            "  - Evaluates model on test data\n",
            "  - Calculates average Hit Rate and NDCG\n",
            "  - Works with GPU or CPU\n",
            "\n",
            "✓ Evaluation ready (device: cpu)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 6.3 EVALUATION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def evaluate_metrics(model, test_loader, top_k, device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on test data.\n",
        "    \n",
        "    This function:\n",
        "    1. For each test case (1 positive + 99 negatives):\n",
        "       - Gets model predictions for all 100 items\n",
        "       - Selects top-K items with highest scores\n",
        "       - Checks if the true item is in top-K (Hit Rate)\n",
        "       - Calculates NDCG based on true item's position\n",
        "    2. Averages metrics across all test cases\n",
        "    \n",
        "    Parameters:\n",
        "    - model: Trained NCF model\n",
        "    - test_loader: DataLoader with test data\n",
        "    - top_k: Number of top items to consider (e.g., 10)\n",
        "    - device: 'cuda' or 'cpu'\n",
        "    \n",
        "    Returns:\n",
        "    - mean_HR: Average Hit Rate across all test cases\n",
        "    - mean_NDCG: Average NDCG across all test cases\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout)\n",
        "    \n",
        "    HR_list = []  # List to store Hit Rate for each test case\n",
        "    NDCG_list = []  # List to store NDCG for each test case\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient computation (faster, saves memory)\n",
        "        for user, item, label in test_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                user = user.cuda()\n",
        "                item = item.cuda()\n",
        "            else:\n",
        "                device = 'cpu'\n",
        "            \n",
        "            # Get model predictions for all items in this batch\n",
        "            # Batch size = test_num_ng + 1 = 100 (1 positive + 99 negatives)\n",
        "            predictions = model(user, item)  # [100] tensor of scores\n",
        "            \n",
        "            # Get top-K items with highest prediction scores\n",
        "            # torch.topk returns (values, indices)\n",
        "            _, indices = torch.topk(predictions, top_k)\n",
        "            \n",
        "            # Get the actual item IDs for top-K recommendations\n",
        "            # torch.take extracts items at given indices\n",
        "            recommends = torch.take(item, indices).cpu().numpy().tolist()\n",
        "            \n",
        "            # The first item in the batch is always the positive (true) item\n",
        "            gt_item = item[0].item()  # Ground truth item ID\n",
        "            \n",
        "            # Calculate metrics for this test case\n",
        "            HR_list.append(hit(gt_item, recommends))\n",
        "            NDCG_list.append(ndcg(gt_item, recommends))\n",
        "    \n",
        "    # Calculate average metrics across all test cases\n",
        "    mean_HR = np.mean(HR_list)\n",
        "    mean_NDCG = np.mean(NDCG_list)\n",
        "    \n",
        "    return mean_HR, mean_NDCG\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 6.3: Evaluation Function\")\n",
        "print(\"=\" * 70)\n",
        "print(\"✓ Evaluation function defined\")\n",
        "print(\"  - Evaluates model on test data\")\n",
        "print(\"  - Calculates average Hit Rate and NDCG\")\n",
        "print(\"  - Works with GPU or CPU\")\n",
        "\n",
        "# Determine device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n✓ Evaluation ready (device: {device})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detailed Explanation of Step 6:\n",
        "\n",
        "#### 6.1 Hit Rate (HR@K) - Binary Metric\n",
        "\n",
        "**What is Hit Rate?**\n",
        "- Measures whether the true item appears in the top-K recommendations\n",
        "- Binary metric: 1 if found, 0 if not found\n",
        "- Simple and intuitive: \"Did we recommend the right item?\"\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "Hit Rate = 1 (item is in top-10)\n",
        "```\n",
        "\n",
        "**Why Hit Rate?**\n",
        "- Users typically only see top-K recommendations\n",
        "- If true item is in top-K, recommendation is successful\n",
        "- Easy to interpret: \"X% of test cases had correct item in top-K\"\n",
        "\n",
        "**Limitations:**\n",
        "- Doesn't consider position (item at position 1 vs position 10 both get 1)\n",
        "- Binary: doesn't measure how good the ranking is\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.2 NDCG (Normalized Discounted Cumulative Gain) - Ranking Metric\n",
        "\n",
        "**What is NDCG?**\n",
        "- Measures ranking quality, not just presence\n",
        "- Gives more weight to items ranked higher\n",
        "- Uses logarithmic discounting (relevance decreases with position)\n",
        "\n",
        "**NDCG Formula:**\n",
        "```\n",
        "NDCG = 1 / log2(position + 2)\n",
        "```\n",
        "\n",
        "**Position vs NDCG Score:**\n",
        "| Position | NDCG Score | Meaning |\n",
        "|----------|-----------|---------|\n",
        "| 0 (top)  | 1.000     | Perfect! Item ranked #1 |\n",
        "| 1        | 0.631     | Item ranked #2 |\n",
        "| 2        | 0.500     | Item ranked #3 |\n",
        "| 3        | 0.431     | Item ranked #4 |\n",
        "| 4        | 0.387     | Item ranked #5 |\n",
        "| 9        | 0.289     | Item ranked #10 |\n",
        "\n",
        "**Why Logarithmic Discounting?**\n",
        "- Position 0 → 1: Big difference (user sees it first)\n",
        "- Position 9 → 10: Small difference (both far down)\n",
        "- Logarithmic function captures this diminishing importance\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "True item: Movie #42\n",
        "Top-10 recommendations: [15, 23, 42, 7, 89, 12, 56, 3, 91, 8]\n",
        "                        ↑\n",
        "                    Found at position 2!\n",
        "NDCG = 1 / log2(2 + 2) = 1 / log2(4) = 1 / 2 = 0.5\n",
        "```\n",
        "\n",
        "**Why NDCG?**\n",
        "- Considers position: Better ranking = Higher score\n",
        "- More informative than Hit Rate\n",
        "- Standard metric in information retrieval and recommendation systems\n",
        "\n",
        "**Limitations:**\n",
        "- More complex than Hit Rate\n",
        "- Requires understanding of logarithmic discounting\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.3 Evaluation Process\n",
        "\n",
        "**How Evaluation Works:**\n",
        "\n",
        "1. **For each test case** (1 positive + 99 negatives):\n",
        "   ```\n",
        "   User: 123\n",
        "   Items: [42 (positive), 1, 5, 7, 9, ... (99 negatives)]\n",
        "   ```\n",
        "\n",
        "2. **Get predictions**:\n",
        "   ```\n",
        "   Model scores: [0.8, 0.3, 0.2, 0.1, 0.05, ...]\n",
        "   Item 42 gets score 0.8 (highest!)\n",
        "   ```\n",
        "\n",
        "3. **Select top-K** (e.g., K=10):\n",
        "   ```\n",
        "   Top-10 items: [42, 1, 5, 7, 9, 12, 15, 18, 20, 23]\n",
        "   ```\n",
        "\n",
        "4. **Calculate metrics**:\n",
        "   - Hit Rate: Is 42 in top-10? Yes → HR = 1\n",
        "   - NDCG: Position of 42? Position 0 → NDCG = 1.0\n",
        "\n",
        "5. **Average across all test cases**:\n",
        "   ```\n",
        "   Mean HR = (1 + 0 + 1 + 1 + ...) / N\n",
        "   Mean NDCG = (1.0 + 0.0 + 0.5 + 0.63 + ...) / N\n",
        "   ```\n",
        "\n",
        "**Test Data Structure:**\n",
        "- Each batch: 100 items (1 positive + 99 negatives)\n",
        "- Model should rank the positive item higher than negatives\n",
        "- We measure if positive is in top-K\n",
        "\n",
        "**Why 99 Negatives?**\n",
        "- Simulates real-world scenario: recommend 1 from 100 candidates\n",
        "- Standard evaluation protocol (used in research papers)\n",
        "- Makes evaluation realistic and challenging\n",
        "\n",
        "---\n",
        "\n",
        "#### 6.4 Understanding the Results\n",
        "\n",
        "**Good Performance:**\n",
        "- HR@10 > 0.6: 60% of test cases have true item in top-10\n",
        "- NDCG@10 > 0.4: Good ranking quality on average\n",
        "\n",
        "**Excellent Performance:**\n",
        "- HR@10 > 0.7: 70% success rate\n",
        "- NDCG@10 > 0.5: Very good ranking quality\n",
        "\n",
        "**What to Expect:**\n",
        "- Random baseline: HR@10 ≈ 0.1 (10% chance)\n",
        "- Good model: HR@10 ≈ 0.6-0.7\n",
        "- State-of-the-art: HR@10 > 0.7\n",
        "\n",
        "**Interpreting Results:**\n",
        "- **HR higher than NDCG**: Model finds items but doesn't rank them well\n",
        "- **NDCG close to HR**: Model ranks items well (good positions)\n",
        "- **Both low**: Model needs more training or better architecture\n",
        "\n",
        "---\n",
        "\n",
        "**✅ Step 6 Complete!**\n",
        "\n",
        "We now have:\n",
        "- Hit Rate metric for binary evaluation\n",
        "- NDCG metric for ranking quality\n",
        "- Complete evaluation function ready to use\n",
        "\n",
        "---\n",
        "\n",
        "## Step 7: Training Loop\n",
        "\n",
        "In this step, we'll implement the complete training process:\n",
        "1. **Loss Function**: Binary Cross-Entropy with Logits (for binary classification)\n",
        "2. **Optimizer**: Adam optimizer (adaptive learning rate)\n",
        "3. **Training Loop**: Iterate through epochs, train on batches, evaluate periodically\n",
        "4. **Model Saving**: Save the best model based on validation performance\n",
        "\n",
        "This is where the model learns to make good recommendations!\n",
        "\n",
        "Let's implement this step by step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.1: Setting Up Loss Function and Optimizer\n",
            "======================================================================\n",
            "✓ Loss function: BCEWithLogitsLoss\n",
            "  - For binary classification (like/dislike)\n",
            "  - Combines sigmoid + cross-entropy for stability\n",
            "\n",
            "✓ Optimizer: Adam\n",
            "  - Learning rate: 0.001\n",
            "  - Adaptive: adjusts learning rate automatically\n",
            "\n",
            "✓ Model ready for training\n",
            "  - Trainable parameters: 1,574,657\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "This step implements the complete training process for the NCF model.\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# 7.1 SETUP LOSS FUNCTION AND OPTIMIZER\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.1: Setting Up Loss Function and Optimizer\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Loss Function: Binary Cross-Entropy with Logits\n",
        "# This combines sigmoid activation + binary cross-entropy loss\n",
        "# More numerically stable than applying sigmoid separately\n",
        "# \n",
        "# Why BCEWithLogitsLoss?\n",
        "# - Our task: Predict if user will like item (binary: 1 or 0)\n",
        "# - Model outputs raw scores (logits), not probabilities\n",
        "# - BCEWithLogitsLoss applies sigmoid internally and computes loss\n",
        "# - More stable than: sigmoid(output) then BCE(sigmoid_output, label)\n",
        "\n",
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "print(\"✓ Loss function: BCEWithLogitsLoss\")\n",
        "print(\"  - For binary classification (like/dislike)\")\n",
        "print(\"  - Combines sigmoid + cross-entropy for stability\")\n",
        "\n",
        "# Optimizer: Adam (Adaptive Moment Estimation)\n",
        "# Adam is an adaptive learning rate optimizer that:\n",
        "# - Adjusts learning rate per parameter\n",
        "# - Uses momentum (moving average of gradients)\n",
        "# - Works well for most deep learning tasks\n",
        "# - Better than SGD for this problem\n",
        "\n",
        "optimizer = optim.Adam(ncf_model.parameters(), lr=learning_rate)\n",
        "print(f\"\\n✓ Optimizer: Adam\")\n",
        "print(f\"  - Learning rate: {learning_rate}\")\n",
        "print(f\"  - Adaptive: adjusts learning rate automatically\")\n",
        "\n",
        "# Count trainable parameters\n",
        "total_params = sum(p.numel() for p in ncf_model.parameters() if p.requires_grad)\n",
        "print(f\"\\n✓ Model ready for training\")\n",
        "print(f\"  - Trainable parameters: {total_params:,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.2: Starting Training\n",
            "======================================================================\n",
            "Training for 20 epochs...\n",
            "Model: NeuMF-end\n",
            "Device: cpu\n",
            "======================================================================\n",
            "\n",
            "Epoch 1/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.4958\n",
            "  Batch 200/8989 - Loss: 0.4441\n",
            "  Batch 300/8989 - Loss: 0.4200\n",
            "  Batch 400/8989 - Loss: 0.4063\n",
            "  Batch 500/8989 - Loss: 0.3970\n",
            "  Batch 600/8989 - Loss: 0.3908\n",
            "  Batch 700/8989 - Loss: 0.3861\n",
            "  Batch 800/8989 - Loss: 0.3829\n",
            "  Batch 900/8989 - Loss: 0.3800\n",
            "  Batch 1000/8989 - Loss: 0.3769\n",
            "  Batch 1100/8989 - Loss: 0.3750\n",
            "  Batch 1200/8989 - Loss: 0.3735\n",
            "  Batch 1300/8989 - Loss: 0.3716\n",
            "  Batch 1400/8989 - Loss: 0.3703\n",
            "  Batch 1500/8989 - Loss: 0.3693\n",
            "  Batch 1600/8989 - Loss: 0.3684\n",
            "  Batch 1700/8989 - Loss: 0.3675\n",
            "  Batch 1800/8989 - Loss: 0.3665\n",
            "  Batch 1900/8989 - Loss: 0.3655\n",
            "  Batch 2000/8989 - Loss: 0.3646\n",
            "  Batch 2100/8989 - Loss: 0.3639\n",
            "  Batch 2200/8989 - Loss: 0.3631\n",
            "  Batch 2300/8989 - Loss: 0.3623\n",
            "  Batch 2400/8989 - Loss: 0.3616\n",
            "  Batch 2500/8989 - Loss: 0.3611\n",
            "  Batch 2600/8989 - Loss: 0.3606\n",
            "  Batch 2700/8989 - Loss: 0.3597\n",
            "  Batch 2800/8989 - Loss: 0.3589\n",
            "  Batch 2900/8989 - Loss: 0.3581\n",
            "  Batch 3000/8989 - Loss: 0.3575\n",
            "  Batch 3100/8989 - Loss: 0.3569\n",
            "  Batch 3200/8989 - Loss: 0.3559\n",
            "  Batch 3300/8989 - Loss: 0.3553\n",
            "  Batch 3400/8989 - Loss: 0.3546\n",
            "  Batch 3500/8989 - Loss: 0.3539\n",
            "  Batch 3600/8989 - Loss: 0.3531\n",
            "  Batch 3700/8989 - Loss: 0.3526\n",
            "  Batch 3800/8989 - Loss: 0.3519\n",
            "  Batch 3900/8989 - Loss: 0.3513\n",
            "  Batch 4000/8989 - Loss: 0.3506\n",
            "  Batch 4100/8989 - Loss: 0.3499\n",
            "  Batch 4200/8989 - Loss: 0.3492\n",
            "  Batch 4300/8989 - Loss: 0.3485\n",
            "  Batch 4400/8989 - Loss: 0.3480\n",
            "  Batch 4500/8989 - Loss: 0.3475\n",
            "  Batch 4600/8989 - Loss: 0.3469\n",
            "  Batch 4700/8989 - Loss: 0.3464\n",
            "  Batch 4800/8989 - Loss: 0.3459\n",
            "  Batch 4900/8989 - Loss: 0.3452\n",
            "  Batch 5000/8989 - Loss: 0.3447\n",
            "  Batch 5100/8989 - Loss: 0.3442\n",
            "  Batch 5200/8989 - Loss: 0.3437\n",
            "  Batch 5300/8989 - Loss: 0.3432\n",
            "  Batch 5400/8989 - Loss: 0.3426\n",
            "  Batch 5500/8989 - Loss: 0.3421\n",
            "  Batch 5600/8989 - Loss: 0.3415\n",
            "  Batch 5700/8989 - Loss: 0.3410\n",
            "  Batch 5800/8989 - Loss: 0.3405\n",
            "  Batch 5900/8989 - Loss: 0.3399\n",
            "  Batch 6000/8989 - Loss: 0.3394\n",
            "  Batch 6100/8989 - Loss: 0.3388\n",
            "  Batch 6200/8989 - Loss: 0.3382\n",
            "  Batch 6300/8989 - Loss: 0.3378\n",
            "  Batch 6400/8989 - Loss: 0.3373\n",
            "  Batch 6500/8989 - Loss: 0.3369\n",
            "  Batch 6600/8989 - Loss: 0.3364\n",
            "  Batch 6700/8989 - Loss: 0.3358\n",
            "  Batch 6800/8989 - Loss: 0.3354\n",
            "  Batch 6900/8989 - Loss: 0.3349\n",
            "  Batch 7000/8989 - Loss: 0.3345\n",
            "  Batch 7100/8989 - Loss: 0.3341\n",
            "  Batch 7200/8989 - Loss: 0.3336\n",
            "  Batch 7300/8989 - Loss: 0.3332\n",
            "  Batch 7400/8989 - Loss: 0.3328\n",
            "  Batch 7500/8989 - Loss: 0.3323\n",
            "  Batch 7600/8989 - Loss: 0.3319\n",
            "  Batch 7700/8989 - Loss: 0.3314\n",
            "  Batch 7800/8989 - Loss: 0.3310\n",
            "  Batch 7900/8989 - Loss: 0.3306\n",
            "  Batch 8000/8989 - Loss: 0.3302\n",
            "  Batch 8100/8989 - Loss: 0.3298\n",
            "  Batch 8200/8989 - Loss: 0.3294\n",
            "  Batch 8300/8989 - Loss: 0.3290\n",
            "  Batch 8400/8989 - Loss: 0.3286\n",
            "  Batch 8500/8989 - Loss: 0.3282\n",
            "  Batch 8600/8989 - Loss: 0.3278\n",
            "  Batch 8700/8989 - Loss: 0.3274\n",
            "  Batch 8800/8989 - Loss: 0.3270\n",
            "  Batch 8900/8989 - Loss: 0.3267\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:13\n",
            "  Loss: 0.3263\n",
            "  HR@10: 0.7048\n",
            "  NDCG@10: 0.4269\n",
            "  ✓ New best model! (HR@10: 0.7048)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 2/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2763\n",
            "  Batch 200/8989 - Loss: 0.2780\n",
            "  Batch 300/8989 - Loss: 0.2777\n",
            "  Batch 400/8989 - Loss: 0.2786\n",
            "  Batch 500/8989 - Loss: 0.2784\n",
            "  Batch 600/8989 - Loss: 0.2779\n",
            "  Batch 700/8989 - Loss: 0.2782\n",
            "  Batch 800/8989 - Loss: 0.2783\n",
            "  Batch 900/8989 - Loss: 0.2783\n",
            "  Batch 1000/8989 - Loss: 0.2784\n",
            "  Batch 1100/8989 - Loss: 0.2780\n",
            "  Batch 1200/8989 - Loss: 0.2777\n",
            "  Batch 1300/8989 - Loss: 0.2772\n",
            "  Batch 1400/8989 - Loss: 0.2773\n",
            "  Batch 1500/8989 - Loss: 0.2770\n",
            "  Batch 1600/8989 - Loss: 0.2771\n",
            "  Batch 1700/8989 - Loss: 0.2769\n",
            "  Batch 1800/8989 - Loss: 0.2767\n",
            "  Batch 1900/8989 - Loss: 0.2767\n",
            "  Batch 2000/8989 - Loss: 0.2766\n",
            "  Batch 2100/8989 - Loss: 0.2766\n",
            "  Batch 2200/8989 - Loss: 0.2767\n",
            "  Batch 2300/8989 - Loss: 0.2768\n",
            "  Batch 2400/8989 - Loss: 0.2768\n",
            "  Batch 2500/8989 - Loss: 0.2769\n",
            "  Batch 2600/8989 - Loss: 0.2767\n",
            "  Batch 2700/8989 - Loss: 0.2769\n",
            "  Batch 2800/8989 - Loss: 0.2769\n",
            "  Batch 2900/8989 - Loss: 0.2766\n",
            "  Batch 3000/8989 - Loss: 0.2764\n",
            "  Batch 3100/8989 - Loss: 0.2762\n",
            "  Batch 3200/8989 - Loss: 0.2761\n",
            "  Batch 3300/8989 - Loss: 0.2762\n",
            "  Batch 3400/8989 - Loss: 0.2762\n",
            "  Batch 3500/8989 - Loss: 0.2762\n",
            "  Batch 3600/8989 - Loss: 0.2762\n",
            "  Batch 3700/8989 - Loss: 0.2761\n",
            "  Batch 3800/8989 - Loss: 0.2759\n",
            "  Batch 3900/8989 - Loss: 0.2758\n",
            "  Batch 4000/8989 - Loss: 0.2757\n",
            "  Batch 4100/8989 - Loss: 0.2757\n",
            "  Batch 4200/8989 - Loss: 0.2756\n",
            "  Batch 4300/8989 - Loss: 0.2755\n",
            "  Batch 4400/8989 - Loss: 0.2753\n",
            "  Batch 4500/8989 - Loss: 0.2752\n",
            "  Batch 4600/8989 - Loss: 0.2751\n",
            "  Batch 4700/8989 - Loss: 0.2751\n",
            "  Batch 4800/8989 - Loss: 0.2751\n",
            "  Batch 4900/8989 - Loss: 0.2751\n",
            "  Batch 5000/8989 - Loss: 0.2750\n",
            "  Batch 5100/8989 - Loss: 0.2749\n",
            "  Batch 5200/8989 - Loss: 0.2748\n",
            "  Batch 5300/8989 - Loss: 0.2746\n",
            "  Batch 5400/8989 - Loss: 0.2744\n",
            "  Batch 5500/8989 - Loss: 0.2744\n",
            "  Batch 5600/8989 - Loss: 0.2743\n",
            "  Batch 5700/8989 - Loss: 0.2741\n",
            "  Batch 5800/8989 - Loss: 0.2740\n",
            "  Batch 5900/8989 - Loss: 0.2740\n",
            "  Batch 6000/8989 - Loss: 0.2739\n",
            "  Batch 6100/8989 - Loss: 0.2737\n",
            "  Batch 6200/8989 - Loss: 0.2737\n",
            "  Batch 6300/8989 - Loss: 0.2736\n",
            "  Batch 6400/8989 - Loss: 0.2736\n",
            "  Batch 6500/8989 - Loss: 0.2736\n",
            "  Batch 6600/8989 - Loss: 0.2736\n",
            "  Batch 6700/8989 - Loss: 0.2736\n",
            "  Batch 6800/8989 - Loss: 0.2735\n",
            "  Batch 6900/8989 - Loss: 0.2734\n",
            "  Batch 7000/8989 - Loss: 0.2734\n",
            "  Batch 7100/8989 - Loss: 0.2733\n",
            "  Batch 7200/8989 - Loss: 0.2732\n",
            "  Batch 7300/8989 - Loss: 0.2731\n",
            "  Batch 7400/8989 - Loss: 0.2730\n",
            "  Batch 7500/8989 - Loss: 0.2729\n",
            "  Batch 7600/8989 - Loss: 0.2728\n",
            "  Batch 7700/8989 - Loss: 0.2728\n",
            "  Batch 7800/8989 - Loss: 0.2727\n",
            "  Batch 7900/8989 - Loss: 0.2726\n",
            "  Batch 8000/8989 - Loss: 0.2726\n",
            "  Batch 8100/8989 - Loss: 0.2725\n",
            "  Batch 8200/8989 - Loss: 0.2725\n",
            "  Batch 8300/8989 - Loss: 0.2724\n",
            "  Batch 8400/8989 - Loss: 0.2723\n",
            "  Batch 8500/8989 - Loss: 0.2723\n",
            "  Batch 8600/8989 - Loss: 0.2722\n",
            "  Batch 8700/8989 - Loss: 0.2721\n",
            "  Batch 8800/8989 - Loss: 0.2721\n",
            "  Batch 8900/8989 - Loss: 0.2720\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:12\n",
            "  Loss: 0.2720\n",
            "  HR@10: 0.7333\n",
            "  NDCG@10: 0.4512\n",
            "  ✓ New best model! (HR@10: 0.7333)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 3/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2451\n",
            "  Batch 200/8989 - Loss: 0.2444\n",
            "  Batch 300/8989 - Loss: 0.2445\n",
            "  Batch 400/8989 - Loss: 0.2447\n",
            "  Batch 500/8989 - Loss: 0.2450\n",
            "  Batch 600/8989 - Loss: 0.2448\n",
            "  Batch 700/8989 - Loss: 0.2455\n",
            "  Batch 800/8989 - Loss: 0.2458\n",
            "  Batch 900/8989 - Loss: 0.2457\n",
            "  Batch 1000/8989 - Loss: 0.2456\n",
            "  Batch 1100/8989 - Loss: 0.2461\n",
            "  Batch 1200/8989 - Loss: 0.2460\n",
            "  Batch 1300/8989 - Loss: 0.2464\n",
            "  Batch 1400/8989 - Loss: 0.2466\n",
            "  Batch 1500/8989 - Loss: 0.2464\n",
            "  Batch 1600/8989 - Loss: 0.2469\n",
            "  Batch 1700/8989 - Loss: 0.2468\n",
            "  Batch 1800/8989 - Loss: 0.2468\n",
            "  Batch 1900/8989 - Loss: 0.2470\n",
            "  Batch 2000/8989 - Loss: 0.2472\n",
            "  Batch 2100/8989 - Loss: 0.2473\n",
            "  Batch 2200/8989 - Loss: 0.2476\n",
            "  Batch 2300/8989 - Loss: 0.2475\n",
            "  Batch 2400/8989 - Loss: 0.2476\n",
            "  Batch 2500/8989 - Loss: 0.2477\n",
            "  Batch 2600/8989 - Loss: 0.2477\n",
            "  Batch 2700/8989 - Loss: 0.2476\n",
            "  Batch 2800/8989 - Loss: 0.2477\n",
            "  Batch 2900/8989 - Loss: 0.2478\n",
            "  Batch 3000/8989 - Loss: 0.2479\n",
            "  Batch 3100/8989 - Loss: 0.2482\n",
            "  Batch 3200/8989 - Loss: 0.2483\n",
            "  Batch 3300/8989 - Loss: 0.2483\n",
            "  Batch 3400/8989 - Loss: 0.2483\n",
            "  Batch 3500/8989 - Loss: 0.2483\n",
            "  Batch 3600/8989 - Loss: 0.2484\n",
            "  Batch 3700/8989 - Loss: 0.2484\n",
            "  Batch 3800/8989 - Loss: 0.2485\n",
            "  Batch 3900/8989 - Loss: 0.2485\n",
            "  Batch 4000/8989 - Loss: 0.2485\n",
            "  Batch 4100/8989 - Loss: 0.2484\n",
            "  Batch 4200/8989 - Loss: 0.2486\n",
            "  Batch 4300/8989 - Loss: 0.2485\n",
            "  Batch 4400/8989 - Loss: 0.2486\n",
            "  Batch 4500/8989 - Loss: 0.2486\n",
            "  Batch 4600/8989 - Loss: 0.2486\n",
            "  Batch 4700/8989 - Loss: 0.2486\n",
            "  Batch 4800/8989 - Loss: 0.2487\n",
            "  Batch 4900/8989 - Loss: 0.2488\n",
            "  Batch 5000/8989 - Loss: 0.2489\n",
            "  Batch 5100/8989 - Loss: 0.2489\n",
            "  Batch 5200/8989 - Loss: 0.2490\n",
            "  Batch 5300/8989 - Loss: 0.2491\n",
            "  Batch 5400/8989 - Loss: 0.2492\n",
            "  Batch 5500/8989 - Loss: 0.2493\n",
            "  Batch 5600/8989 - Loss: 0.2493\n",
            "  Batch 5700/8989 - Loss: 0.2494\n",
            "  Batch 5800/8989 - Loss: 0.2495\n",
            "  Batch 5900/8989 - Loss: 0.2496\n",
            "  Batch 6000/8989 - Loss: 0.2496\n",
            "  Batch 6100/8989 - Loss: 0.2496\n",
            "  Batch 6200/8989 - Loss: 0.2496\n",
            "  Batch 6300/8989 - Loss: 0.2497\n",
            "  Batch 6400/8989 - Loss: 0.2498\n",
            "  Batch 6500/8989 - Loss: 0.2498\n",
            "  Batch 6600/8989 - Loss: 0.2499\n",
            "  Batch 6700/8989 - Loss: 0.2499\n",
            "  Batch 6800/8989 - Loss: 0.2499\n",
            "  Batch 6900/8989 - Loss: 0.2500\n",
            "  Batch 7000/8989 - Loss: 0.2499\n",
            "  Batch 7100/8989 - Loss: 0.2499\n",
            "  Batch 7200/8989 - Loss: 0.2500\n",
            "  Batch 7300/8989 - Loss: 0.2500\n",
            "  Batch 7400/8989 - Loss: 0.2501\n",
            "  Batch 7500/8989 - Loss: 0.2502\n",
            "  Batch 7600/8989 - Loss: 0.2502\n",
            "  Batch 7700/8989 - Loss: 0.2503\n",
            "  Batch 7800/8989 - Loss: 0.2503\n",
            "  Batch 7900/8989 - Loss: 0.2503\n",
            "  Batch 8000/8989 - Loss: 0.2502\n",
            "  Batch 8100/8989 - Loss: 0.2502\n",
            "  Batch 8200/8989 - Loss: 0.2502\n",
            "  Batch 8300/8989 - Loss: 0.2502\n",
            "  Batch 8400/8989 - Loss: 0.2503\n",
            "  Batch 8500/8989 - Loss: 0.2503\n",
            "  Batch 8600/8989 - Loss: 0.2503\n",
            "  Batch 8700/8989 - Loss: 0.2504\n",
            "  Batch 8800/8989 - Loss: 0.2504\n",
            "  Batch 8900/8989 - Loss: 0.2504\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:11\n",
            "  Loss: 0.2504\n",
            "  HR@10: 0.7389\n",
            "  NDCG@10: 0.4551\n",
            "  ✓ New best model! (HR@10: 0.7389)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 4/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2339\n",
            "  Batch 200/8989 - Loss: 0.2325\n",
            "  Batch 300/8989 - Loss: 0.2317\n",
            "  Batch 400/8989 - Loss: 0.2321\n",
            "  Batch 500/8989 - Loss: 0.2317\n",
            "  Batch 600/8989 - Loss: 0.2325\n",
            "  Batch 700/8989 - Loss: 0.2312\n",
            "  Batch 800/8989 - Loss: 0.2311\n",
            "  Batch 900/8989 - Loss: 0.2317\n",
            "  Batch 1000/8989 - Loss: 0.2320\n",
            "  Batch 1100/8989 - Loss: 0.2320\n",
            "  Batch 1200/8989 - Loss: 0.2321\n",
            "  Batch 1300/8989 - Loss: 0.2325\n",
            "  Batch 1400/8989 - Loss: 0.2327\n",
            "  Batch 1500/8989 - Loss: 0.2330\n",
            "  Batch 1600/8989 - Loss: 0.2333\n",
            "  Batch 1700/8989 - Loss: 0.2332\n",
            "  Batch 1800/8989 - Loss: 0.2336\n",
            "  Batch 1900/8989 - Loss: 0.2336\n",
            "  Batch 2000/8989 - Loss: 0.2334\n",
            "  Batch 2100/8989 - Loss: 0.2335\n",
            "  Batch 2200/8989 - Loss: 0.2338\n",
            "  Batch 2300/8989 - Loss: 0.2340\n",
            "  Batch 2400/8989 - Loss: 0.2341\n",
            "  Batch 2500/8989 - Loss: 0.2340\n",
            "  Batch 2600/8989 - Loss: 0.2341\n",
            "  Batch 2700/8989 - Loss: 0.2341\n",
            "  Batch 2800/8989 - Loss: 0.2343\n",
            "  Batch 2900/8989 - Loss: 0.2343\n",
            "  Batch 3000/8989 - Loss: 0.2347\n",
            "  Batch 3100/8989 - Loss: 0.2347\n",
            "  Batch 3200/8989 - Loss: 0.2348\n",
            "  Batch 3300/8989 - Loss: 0.2350\n",
            "  Batch 3400/8989 - Loss: 0.2349\n",
            "  Batch 3500/8989 - Loss: 0.2348\n",
            "  Batch 3600/8989 - Loss: 0.2349\n",
            "  Batch 3700/8989 - Loss: 0.2349\n",
            "  Batch 3800/8989 - Loss: 0.2352\n",
            "  Batch 3900/8989 - Loss: 0.2353\n",
            "  Batch 4000/8989 - Loss: 0.2353\n",
            "  Batch 4100/8989 - Loss: 0.2354\n",
            "  Batch 4200/8989 - Loss: 0.2355\n",
            "  Batch 4300/8989 - Loss: 0.2356\n",
            "  Batch 4400/8989 - Loss: 0.2357\n",
            "  Batch 4500/8989 - Loss: 0.2358\n",
            "  Batch 4600/8989 - Loss: 0.2360\n",
            "  Batch 4700/8989 - Loss: 0.2360\n",
            "  Batch 4800/8989 - Loss: 0.2360\n",
            "  Batch 4900/8989 - Loss: 0.2361\n",
            "  Batch 5000/8989 - Loss: 0.2362\n",
            "  Batch 5100/8989 - Loss: 0.2363\n",
            "  Batch 5200/8989 - Loss: 0.2363\n",
            "  Batch 5300/8989 - Loss: 0.2363\n",
            "  Batch 5400/8989 - Loss: 0.2364\n",
            "  Batch 5500/8989 - Loss: 0.2364\n",
            "  Batch 5600/8989 - Loss: 0.2366\n",
            "  Batch 5700/8989 - Loss: 0.2366\n",
            "  Batch 5800/8989 - Loss: 0.2367\n",
            "  Batch 5900/8989 - Loss: 0.2369\n",
            "  Batch 6000/8989 - Loss: 0.2370\n",
            "  Batch 6100/8989 - Loss: 0.2371\n",
            "  Batch 6200/8989 - Loss: 0.2372\n",
            "  Batch 6300/8989 - Loss: 0.2373\n",
            "  Batch 6400/8989 - Loss: 0.2373\n",
            "  Batch 6500/8989 - Loss: 0.2374\n",
            "  Batch 6600/8989 - Loss: 0.2374\n",
            "  Batch 6700/8989 - Loss: 0.2375\n",
            "  Batch 6800/8989 - Loss: 0.2375\n",
            "  Batch 6900/8989 - Loss: 0.2376\n",
            "  Batch 7000/8989 - Loss: 0.2376\n",
            "  Batch 7100/8989 - Loss: 0.2377\n",
            "  Batch 7200/8989 - Loss: 0.2378\n",
            "  Batch 7300/8989 - Loss: 0.2379\n",
            "  Batch 7400/8989 - Loss: 0.2380\n",
            "  Batch 7500/8989 - Loss: 0.2380\n",
            "  Batch 7600/8989 - Loss: 0.2380\n",
            "  Batch 7700/8989 - Loss: 0.2380\n",
            "  Batch 7800/8989 - Loss: 0.2381\n",
            "  Batch 7900/8989 - Loss: 0.2382\n",
            "  Batch 8000/8989 - Loss: 0.2382\n",
            "  Batch 8100/8989 - Loss: 0.2383\n",
            "  Batch 8200/8989 - Loss: 0.2383\n",
            "  Batch 8300/8989 - Loss: 0.2384\n",
            "  Batch 8400/8989 - Loss: 0.2383\n",
            "  Batch 8500/8989 - Loss: 0.2384\n",
            "  Batch 8600/8989 - Loss: 0.2385\n",
            "  Batch 8700/8989 - Loss: 0.2384\n",
            "  Batch 8800/8989 - Loss: 0.2385\n",
            "  Batch 8900/8989 - Loss: 0.2385\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:12\n",
            "  Loss: 0.2386\n",
            "  HR@10: 0.7390\n",
            "  NDCG@10: 0.4572\n",
            "  ✓ New best model! (HR@10: 0.7390)\n",
            "  ✓ Model saved to /Users/abbas/Documents/Codes/thesis/recommender/src/../models/NeuMF-end.pth\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 5/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2175\n",
            "  Batch 200/8989 - Loss: 0.2198\n",
            "  Batch 300/8989 - Loss: 0.2192\n",
            "  Batch 400/8989 - Loss: 0.2195\n",
            "  Batch 500/8989 - Loss: 0.2200\n",
            "  Batch 600/8989 - Loss: 0.2204\n",
            "  Batch 700/8989 - Loss: 0.2201\n",
            "  Batch 800/8989 - Loss: 0.2201\n",
            "  Batch 900/8989 - Loss: 0.2205\n",
            "  Batch 1000/8989 - Loss: 0.2206\n",
            "  Batch 1100/8989 - Loss: 0.2208\n",
            "  Batch 1200/8989 - Loss: 0.2210\n",
            "  Batch 1300/8989 - Loss: 0.2209\n",
            "  Batch 1400/8989 - Loss: 0.2213\n",
            "  Batch 1500/8989 - Loss: 0.2215\n",
            "  Batch 1600/8989 - Loss: 0.2216\n",
            "  Batch 1700/8989 - Loss: 0.2219\n",
            "  Batch 1800/8989 - Loss: 0.2224\n",
            "  Batch 1900/8989 - Loss: 0.2225\n",
            "  Batch 2000/8989 - Loss: 0.2226\n",
            "  Batch 2100/8989 - Loss: 0.2228\n",
            "  Batch 2200/8989 - Loss: 0.2227\n",
            "  Batch 2300/8989 - Loss: 0.2230\n",
            "  Batch 2400/8989 - Loss: 0.2233\n",
            "  Batch 2500/8989 - Loss: 0.2234\n",
            "  Batch 2600/8989 - Loss: 0.2234\n",
            "  Batch 2700/8989 - Loss: 0.2236\n",
            "  Batch 2800/8989 - Loss: 0.2237\n",
            "  Batch 2900/8989 - Loss: 0.2238\n",
            "  Batch 3000/8989 - Loss: 0.2238\n",
            "  Batch 3100/8989 - Loss: 0.2240\n",
            "  Batch 3200/8989 - Loss: 0.2242\n",
            "  Batch 3300/8989 - Loss: 0.2242\n",
            "  Batch 3400/8989 - Loss: 0.2243\n",
            "  Batch 3500/8989 - Loss: 0.2245\n",
            "  Batch 3600/8989 - Loss: 0.2246\n",
            "  Batch 3700/8989 - Loss: 0.2247\n",
            "  Batch 3800/8989 - Loss: 0.2247\n",
            "  Batch 3900/8989 - Loss: 0.2249\n",
            "  Batch 4000/8989 - Loss: 0.2251\n",
            "  Batch 4100/8989 - Loss: 0.2254\n",
            "  Batch 4200/8989 - Loss: 0.2255\n",
            "  Batch 4300/8989 - Loss: 0.2255\n",
            "  Batch 4400/8989 - Loss: 0.2256\n",
            "  Batch 4500/8989 - Loss: 0.2259\n",
            "  Batch 4600/8989 - Loss: 0.2260\n",
            "  Batch 4700/8989 - Loss: 0.2263\n",
            "  Batch 4800/8989 - Loss: 0.2264\n",
            "  Batch 4900/8989 - Loss: 0.2265\n",
            "  Batch 5000/8989 - Loss: 0.2265\n",
            "  Batch 5100/8989 - Loss: 0.2267\n",
            "  Batch 5200/8989 - Loss: 0.2267\n",
            "  Batch 5300/8989 - Loss: 0.2268\n",
            "  Batch 5400/8989 - Loss: 0.2270\n",
            "  Batch 5500/8989 - Loss: 0.2271\n",
            "  Batch 5600/8989 - Loss: 0.2273\n",
            "  Batch 5700/8989 - Loss: 0.2272\n",
            "  Batch 5800/8989 - Loss: 0.2273\n",
            "  Batch 5900/8989 - Loss: 0.2273\n",
            "  Batch 6000/8989 - Loss: 0.2275\n",
            "  Batch 6100/8989 - Loss: 0.2275\n",
            "  Batch 6200/8989 - Loss: 0.2276\n",
            "  Batch 6300/8989 - Loss: 0.2278\n",
            "  Batch 6400/8989 - Loss: 0.2279\n",
            "  Batch 6500/8989 - Loss: 0.2280\n",
            "  Batch 6600/8989 - Loss: 0.2281\n",
            "  Batch 6700/8989 - Loss: 0.2282\n",
            "  Batch 6800/8989 - Loss: 0.2283\n",
            "  Batch 6900/8989 - Loss: 0.2284\n",
            "  Batch 7000/8989 - Loss: 0.2285\n",
            "  Batch 7100/8989 - Loss: 0.2285\n",
            "  Batch 7200/8989 - Loss: 0.2287\n",
            "  Batch 7300/8989 - Loss: 0.2288\n",
            "  Batch 7400/8989 - Loss: 0.2288\n",
            "  Batch 7500/8989 - Loss: 0.2289\n",
            "  Batch 7600/8989 - Loss: 0.2290\n",
            "  Batch 7700/8989 - Loss: 0.2291\n",
            "  Batch 7800/8989 - Loss: 0.2291\n",
            "  Batch 7900/8989 - Loss: 0.2292\n",
            "  Batch 8000/8989 - Loss: 0.2293\n",
            "  Batch 8100/8989 - Loss: 0.2293\n",
            "  Batch 8200/8989 - Loss: 0.2294\n",
            "  Batch 8300/8989 - Loss: 0.2295\n",
            "  Batch 8400/8989 - Loss: 0.2295\n",
            "  Batch 8500/8989 - Loss: 0.2296\n",
            "  Batch 8600/8989 - Loss: 0.2297\n",
            "  Batch 8700/8989 - Loss: 0.2297\n",
            "  Batch 8800/8989 - Loss: 0.2298\n",
            "  Batch 8900/8989 - Loss: 0.2299\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:11\n",
            "  Loss: 0.2300\n",
            "  HR@10: 0.7382\n",
            "  NDCG@10: 0.4535\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 6/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2137\n",
            "  Batch 200/8989 - Loss: 0.2101\n",
            "  Batch 300/8989 - Loss: 0.2128\n",
            "  Batch 400/8989 - Loss: 0.2138\n",
            "  Batch 500/8989 - Loss: 0.2135\n",
            "  Batch 600/8989 - Loss: 0.2136\n",
            "  Batch 700/8989 - Loss: 0.2142\n",
            "  Batch 800/8989 - Loss: 0.2145\n",
            "  Batch 900/8989 - Loss: 0.2152\n",
            "  Batch 1000/8989 - Loss: 0.2145\n",
            "  Batch 1100/8989 - Loss: 0.2149\n",
            "  Batch 1200/8989 - Loss: 0.2148\n",
            "  Batch 1300/8989 - Loss: 0.2149\n",
            "  Batch 1400/8989 - Loss: 0.2151\n",
            "  Batch 1500/8989 - Loss: 0.2153\n",
            "  Batch 1600/8989 - Loss: 0.2153\n",
            "  Batch 1700/8989 - Loss: 0.2153\n",
            "  Batch 1800/8989 - Loss: 0.2154\n",
            "  Batch 1900/8989 - Loss: 0.2155\n",
            "  Batch 2000/8989 - Loss: 0.2158\n",
            "  Batch 2100/8989 - Loss: 0.2159\n",
            "  Batch 2200/8989 - Loss: 0.2161\n",
            "  Batch 2300/8989 - Loss: 0.2162\n",
            "  Batch 2400/8989 - Loss: 0.2162\n",
            "  Batch 2500/8989 - Loss: 0.2163\n",
            "  Batch 2600/8989 - Loss: 0.2165\n",
            "  Batch 2700/8989 - Loss: 0.2166\n",
            "  Batch 2800/8989 - Loss: 0.2169\n",
            "  Batch 2900/8989 - Loss: 0.2171\n",
            "  Batch 3000/8989 - Loss: 0.2172\n",
            "  Batch 3100/8989 - Loss: 0.2174\n",
            "  Batch 3200/8989 - Loss: 0.2176\n",
            "  Batch 3300/8989 - Loss: 0.2178\n",
            "  Batch 3400/8989 - Loss: 0.2178\n",
            "  Batch 3500/8989 - Loss: 0.2180\n",
            "  Batch 3600/8989 - Loss: 0.2182\n",
            "  Batch 3700/8989 - Loss: 0.2182\n",
            "  Batch 3800/8989 - Loss: 0.2185\n",
            "  Batch 3900/8989 - Loss: 0.2186\n",
            "  Batch 4000/8989 - Loss: 0.2188\n",
            "  Batch 4100/8989 - Loss: 0.2190\n",
            "  Batch 4200/8989 - Loss: 0.2191\n",
            "  Batch 4300/8989 - Loss: 0.2193\n",
            "  Batch 4400/8989 - Loss: 0.2195\n",
            "  Batch 4500/8989 - Loss: 0.2195\n",
            "  Batch 4600/8989 - Loss: 0.2196\n",
            "  Batch 4700/8989 - Loss: 0.2197\n",
            "  Batch 4800/8989 - Loss: 0.2199\n",
            "  Batch 4900/8989 - Loss: 0.2200\n",
            "  Batch 5000/8989 - Loss: 0.2201\n",
            "  Batch 5100/8989 - Loss: 0.2203\n",
            "  Batch 5200/8989 - Loss: 0.2205\n",
            "  Batch 5300/8989 - Loss: 0.2206\n",
            "  Batch 5400/8989 - Loss: 0.2208\n",
            "  Batch 5500/8989 - Loss: 0.2208\n",
            "  Batch 5600/8989 - Loss: 0.2209\n",
            "  Batch 5700/8989 - Loss: 0.2211\n",
            "  Batch 5800/8989 - Loss: 0.2212\n",
            "  Batch 5900/8989 - Loss: 0.2214\n",
            "  Batch 6000/8989 - Loss: 0.2214\n",
            "  Batch 6100/8989 - Loss: 0.2214\n",
            "  Batch 6200/8989 - Loss: 0.2215\n",
            "  Batch 6300/8989 - Loss: 0.2216\n",
            "  Batch 6400/8989 - Loss: 0.2217\n",
            "  Batch 6500/8989 - Loss: 0.2219\n",
            "  Batch 6600/8989 - Loss: 0.2220\n",
            "  Batch 6700/8989 - Loss: 0.2221\n",
            "  Batch 6800/8989 - Loss: 0.2221\n",
            "  Batch 6900/8989 - Loss: 0.2222\n",
            "  Batch 7000/8989 - Loss: 0.2223\n",
            "  Batch 7100/8989 - Loss: 0.2224\n",
            "  Batch 7200/8989 - Loss: 0.2225\n",
            "  Batch 7300/8989 - Loss: 0.2225\n",
            "  Batch 7400/8989 - Loss: 0.2227\n",
            "  Batch 7500/8989 - Loss: 0.2228\n",
            "  Batch 7600/8989 - Loss: 0.2229\n",
            "  Batch 7700/8989 - Loss: 0.2229\n",
            "  Batch 7800/8989 - Loss: 0.2230\n",
            "  Batch 7900/8989 - Loss: 0.2230\n",
            "  Batch 8000/8989 - Loss: 0.2231\n",
            "  Batch 8100/8989 - Loss: 0.2232\n",
            "  Batch 8200/8989 - Loss: 0.2234\n",
            "  Batch 8300/8989 - Loss: 0.2235\n",
            "  Batch 8400/8989 - Loss: 0.2235\n",
            "  Batch 8500/8989 - Loss: 0.2236\n",
            "  Batch 8600/8989 - Loss: 0.2237\n",
            "  Batch 8700/8989 - Loss: 0.2238\n",
            "  Batch 8800/8989 - Loss: 0.2239\n",
            "  Batch 8900/8989 - Loss: 0.2240\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:16\n",
            "  Loss: 0.2240\n",
            "  HR@10: 0.7366\n",
            "  NDCG@10: 0.4516\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 7/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2031\n",
            "  Batch 200/8989 - Loss: 0.2058\n",
            "  Batch 300/8989 - Loss: 0.2074\n",
            "  Batch 400/8989 - Loss: 0.2084\n",
            "  Batch 500/8989 - Loss: 0.2078\n",
            "  Batch 600/8989 - Loss: 0.2074\n",
            "  Batch 700/8989 - Loss: 0.2083\n",
            "  Batch 800/8989 - Loss: 0.2084\n",
            "  Batch 900/8989 - Loss: 0.2085\n",
            "  Batch 1000/8989 - Loss: 0.2093\n",
            "  Batch 1100/8989 - Loss: 0.2096\n",
            "  Batch 1200/8989 - Loss: 0.2095\n",
            "  Batch 1300/8989 - Loss: 0.2098\n",
            "  Batch 1400/8989 - Loss: 0.2100\n",
            "  Batch 1500/8989 - Loss: 0.2101\n",
            "  Batch 1600/8989 - Loss: 0.2103\n",
            "  Batch 1700/8989 - Loss: 0.2103\n",
            "  Batch 1800/8989 - Loss: 0.2106\n",
            "  Batch 1900/8989 - Loss: 0.2105\n",
            "  Batch 2000/8989 - Loss: 0.2107\n",
            "  Batch 2100/8989 - Loss: 0.2107\n",
            "  Batch 2200/8989 - Loss: 0.2109\n",
            "  Batch 2300/8989 - Loss: 0.2115\n",
            "  Batch 2400/8989 - Loss: 0.2117\n",
            "  Batch 2500/8989 - Loss: 0.2118\n",
            "  Batch 2600/8989 - Loss: 0.2121\n",
            "  Batch 2700/8989 - Loss: 0.2122\n",
            "  Batch 2800/8989 - Loss: 0.2124\n",
            "  Batch 2900/8989 - Loss: 0.2124\n",
            "  Batch 3000/8989 - Loss: 0.2126\n",
            "  Batch 3100/8989 - Loss: 0.2127\n",
            "  Batch 3200/8989 - Loss: 0.2129\n",
            "  Batch 3300/8989 - Loss: 0.2131\n",
            "  Batch 3400/8989 - Loss: 0.2131\n",
            "  Batch 3500/8989 - Loss: 0.2132\n",
            "  Batch 3600/8989 - Loss: 0.2133\n",
            "  Batch 3700/8989 - Loss: 0.2135\n",
            "  Batch 3800/8989 - Loss: 0.2137\n",
            "  Batch 3900/8989 - Loss: 0.2139\n",
            "  Batch 4000/8989 - Loss: 0.2141\n",
            "  Batch 4100/8989 - Loss: 0.2143\n",
            "  Batch 4200/8989 - Loss: 0.2143\n",
            "  Batch 4300/8989 - Loss: 0.2144\n",
            "  Batch 4400/8989 - Loss: 0.2147\n",
            "  Batch 4500/8989 - Loss: 0.2148\n",
            "  Batch 4600/8989 - Loss: 0.2150\n",
            "  Batch 4700/8989 - Loss: 0.2150\n",
            "  Batch 4800/8989 - Loss: 0.2151\n",
            "  Batch 4900/8989 - Loss: 0.2151\n",
            "  Batch 5000/8989 - Loss: 0.2153\n",
            "  Batch 5100/8989 - Loss: 0.2154\n",
            "  Batch 5200/8989 - Loss: 0.2156\n",
            "  Batch 5300/8989 - Loss: 0.2157\n",
            "  Batch 5400/8989 - Loss: 0.2159\n",
            "  Batch 5500/8989 - Loss: 0.2161\n",
            "  Batch 5600/8989 - Loss: 0.2162\n",
            "  Batch 5700/8989 - Loss: 0.2164\n",
            "  Batch 5800/8989 - Loss: 0.2165\n",
            "  Batch 5900/8989 - Loss: 0.2165\n",
            "  Batch 6000/8989 - Loss: 0.2167\n",
            "  Batch 6100/8989 - Loss: 0.2168\n",
            "  Batch 6200/8989 - Loss: 0.2169\n",
            "  Batch 6300/8989 - Loss: 0.2171\n",
            "  Batch 6400/8989 - Loss: 0.2171\n",
            "  Batch 6500/8989 - Loss: 0.2172\n",
            "  Batch 6600/8989 - Loss: 0.2173\n",
            "  Batch 6700/8989 - Loss: 0.2174\n",
            "  Batch 6800/8989 - Loss: 0.2175\n",
            "  Batch 6900/8989 - Loss: 0.2175\n",
            "  Batch 7000/8989 - Loss: 0.2176\n",
            "  Batch 7100/8989 - Loss: 0.2177\n",
            "  Batch 7200/8989 - Loss: 0.2178\n",
            "  Batch 7300/8989 - Loss: 0.2178\n",
            "  Batch 7400/8989 - Loss: 0.2180\n",
            "  Batch 7500/8989 - Loss: 0.2180\n",
            "  Batch 7600/8989 - Loss: 0.2181\n",
            "  Batch 7700/8989 - Loss: 0.2181\n",
            "  Batch 7800/8989 - Loss: 0.2182\n",
            "  Batch 7900/8989 - Loss: 0.2183\n",
            "  Batch 8000/8989 - Loss: 0.2185\n",
            "  Batch 8100/8989 - Loss: 0.2186\n",
            "  Batch 8200/8989 - Loss: 0.2187\n",
            "  Batch 8300/8989 - Loss: 0.2188\n",
            "  Batch 8400/8989 - Loss: 0.2188\n",
            "  Batch 8500/8989 - Loss: 0.2189\n",
            "  Batch 8600/8989 - Loss: 0.2191\n",
            "  Batch 8700/8989 - Loss: 0.2191\n",
            "  Batch 8800/8989 - Loss: 0.2192\n",
            "  Batch 8900/8989 - Loss: 0.2194\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:16\n",
            "  Loss: 0.2194\n",
            "  HR@10: 0.7361\n",
            "  NDCG@10: 0.4501\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 8/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2043\n",
            "  Batch 200/8989 - Loss: 0.2049\n",
            "  Batch 300/8989 - Loss: 0.2048\n",
            "  Batch 400/8989 - Loss: 0.2056\n",
            "  Batch 500/8989 - Loss: 0.2054\n",
            "  Batch 600/8989 - Loss: 0.2058\n",
            "  Batch 700/8989 - Loss: 0.2054\n",
            "  Batch 800/8989 - Loss: 0.2060\n",
            "  Batch 900/8989 - Loss: 0.2065\n",
            "  Batch 1000/8989 - Loss: 0.2065\n",
            "  Batch 1100/8989 - Loss: 0.2064\n",
            "  Batch 1200/8989 - Loss: 0.2066\n",
            "  Batch 1300/8989 - Loss: 0.2073\n",
            "  Batch 1400/8989 - Loss: 0.2080\n",
            "  Batch 1500/8989 - Loss: 0.2080\n",
            "  Batch 1600/8989 - Loss: 0.2081\n",
            "  Batch 1700/8989 - Loss: 0.2082\n",
            "  Batch 1800/8989 - Loss: 0.2083\n",
            "  Batch 1900/8989 - Loss: 0.2086\n",
            "  Batch 2000/8989 - Loss: 0.2086\n",
            "  Batch 2100/8989 - Loss: 0.2089\n",
            "  Batch 2200/8989 - Loss: 0.2090\n",
            "  Batch 2300/8989 - Loss: 0.2093\n",
            "  Batch 2400/8989 - Loss: 0.2095\n",
            "  Batch 2500/8989 - Loss: 0.2097\n",
            "  Batch 2600/8989 - Loss: 0.2098\n",
            "  Batch 2700/8989 - Loss: 0.2099\n",
            "  Batch 2800/8989 - Loss: 0.2100\n",
            "  Batch 2900/8989 - Loss: 0.2100\n",
            "  Batch 3000/8989 - Loss: 0.2102\n",
            "  Batch 3100/8989 - Loss: 0.2102\n",
            "  Batch 3200/8989 - Loss: 0.2102\n",
            "  Batch 3300/8989 - Loss: 0.2105\n",
            "  Batch 3400/8989 - Loss: 0.2107\n",
            "  Batch 3500/8989 - Loss: 0.2108\n",
            "  Batch 3600/8989 - Loss: 0.2108\n",
            "  Batch 3700/8989 - Loss: 0.2109\n",
            "  Batch 3800/8989 - Loss: 0.2110\n",
            "  Batch 3900/8989 - Loss: 0.2111\n",
            "  Batch 4000/8989 - Loss: 0.2112\n",
            "  Batch 4100/8989 - Loss: 0.2113\n",
            "  Batch 4200/8989 - Loss: 0.2113\n",
            "  Batch 4300/8989 - Loss: 0.2115\n",
            "  Batch 4400/8989 - Loss: 0.2116\n",
            "  Batch 4500/8989 - Loss: 0.2118\n",
            "  Batch 4600/8989 - Loss: 0.2118\n",
            "  Batch 4700/8989 - Loss: 0.2121\n",
            "  Batch 4800/8989 - Loss: 0.2121\n",
            "  Batch 4900/8989 - Loss: 0.2122\n",
            "  Batch 5000/8989 - Loss: 0.2124\n",
            "  Batch 5100/8989 - Loss: 0.2125\n",
            "  Batch 5200/8989 - Loss: 0.2126\n",
            "  Batch 5300/8989 - Loss: 0.2128\n",
            "  Batch 5400/8989 - Loss: 0.2129\n",
            "  Batch 5500/8989 - Loss: 0.2130\n",
            "  Batch 5600/8989 - Loss: 0.2131\n",
            "  Batch 5700/8989 - Loss: 0.2132\n",
            "  Batch 5800/8989 - Loss: 0.2134\n",
            "  Batch 5900/8989 - Loss: 0.2135\n",
            "  Batch 6000/8989 - Loss: 0.2135\n",
            "  Batch 6100/8989 - Loss: 0.2135\n",
            "  Batch 6200/8989 - Loss: 0.2135\n",
            "  Batch 6300/8989 - Loss: 0.2137\n",
            "  Batch 6400/8989 - Loss: 0.2137\n",
            "  Batch 6500/8989 - Loss: 0.2139\n",
            "  Batch 6600/8989 - Loss: 0.2140\n",
            "  Batch 6700/8989 - Loss: 0.2140\n",
            "  Batch 6800/8989 - Loss: 0.2142\n",
            "  Batch 6900/8989 - Loss: 0.2143\n",
            "  Batch 7000/8989 - Loss: 0.2144\n",
            "  Batch 7100/8989 - Loss: 0.2145\n",
            "  Batch 7200/8989 - Loss: 0.2146\n",
            "  Batch 7300/8989 - Loss: 0.2147\n",
            "  Batch 7400/8989 - Loss: 0.2148\n",
            "  Batch 7500/8989 - Loss: 0.2148\n",
            "  Batch 7600/8989 - Loss: 0.2149\n",
            "  Batch 7700/8989 - Loss: 0.2150\n",
            "  Batch 7800/8989 - Loss: 0.2151\n",
            "  Batch 7900/8989 - Loss: 0.2152\n",
            "  Batch 8000/8989 - Loss: 0.2153\n",
            "  Batch 8100/8989 - Loss: 0.2154\n",
            "  Batch 8200/8989 - Loss: 0.2154\n",
            "  Batch 8300/8989 - Loss: 0.2156\n",
            "  Batch 8400/8989 - Loss: 0.2156\n",
            "  Batch 8500/8989 - Loss: 0.2157\n",
            "  Batch 8600/8989 - Loss: 0.2157\n",
            "  Batch 8700/8989 - Loss: 0.2158\n",
            "  Batch 8800/8989 - Loss: 0.2159\n",
            "  Batch 8900/8989 - Loss: 0.2159\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:13\n",
            "  Loss: 0.2160\n",
            "  HR@10: 0.7377\n",
            "  NDCG@10: 0.4546\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 9/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.2015\n",
            "  Batch 200/8989 - Loss: 0.1991\n",
            "  Batch 300/8989 - Loss: 0.1999\n",
            "  Batch 400/8989 - Loss: 0.2000\n",
            "  Batch 500/8989 - Loss: 0.2004\n",
            "  Batch 600/8989 - Loss: 0.2016\n",
            "  Batch 700/8989 - Loss: 0.2014\n",
            "  Batch 800/8989 - Loss: 0.2016\n",
            "  Batch 900/8989 - Loss: 0.2020\n",
            "  Batch 1000/8989 - Loss: 0.2022\n",
            "  Batch 1100/8989 - Loss: 0.2025\n",
            "  Batch 1200/8989 - Loss: 0.2029\n",
            "  Batch 1300/8989 - Loss: 0.2030\n",
            "  Batch 1400/8989 - Loss: 0.2032\n",
            "  Batch 1500/8989 - Loss: 0.2034\n",
            "  Batch 1600/8989 - Loss: 0.2036\n",
            "  Batch 1700/8989 - Loss: 0.2038\n",
            "  Batch 1800/8989 - Loss: 0.2040\n",
            "  Batch 1900/8989 - Loss: 0.2040\n",
            "  Batch 2000/8989 - Loss: 0.2042\n",
            "  Batch 2100/8989 - Loss: 0.2044\n",
            "  Batch 2200/8989 - Loss: 0.2046\n",
            "  Batch 2300/8989 - Loss: 0.2052\n",
            "  Batch 2400/8989 - Loss: 0.2055\n",
            "  Batch 2500/8989 - Loss: 0.2057\n",
            "  Batch 2600/8989 - Loss: 0.2056\n",
            "  Batch 2700/8989 - Loss: 0.2058\n",
            "  Batch 2800/8989 - Loss: 0.2059\n",
            "  Batch 2900/8989 - Loss: 0.2060\n",
            "  Batch 3000/8989 - Loss: 0.2061\n",
            "  Batch 3100/8989 - Loss: 0.2062\n",
            "  Batch 3200/8989 - Loss: 0.2062\n",
            "  Batch 3300/8989 - Loss: 0.2063\n",
            "  Batch 3400/8989 - Loss: 0.2064\n",
            "  Batch 3500/8989 - Loss: 0.2066\n",
            "  Batch 3600/8989 - Loss: 0.2066\n",
            "  Batch 3700/8989 - Loss: 0.2068\n",
            "  Batch 3800/8989 - Loss: 0.2070\n",
            "  Batch 3900/8989 - Loss: 0.2071\n",
            "  Batch 4000/8989 - Loss: 0.2072\n",
            "  Batch 4100/8989 - Loss: 0.2075\n",
            "  Batch 4200/8989 - Loss: 0.2077\n",
            "  Batch 4300/8989 - Loss: 0.2078\n",
            "  Batch 4400/8989 - Loss: 0.2080\n",
            "  Batch 4500/8989 - Loss: 0.2081\n",
            "  Batch 4600/8989 - Loss: 0.2082\n",
            "  Batch 4700/8989 - Loss: 0.2083\n",
            "  Batch 4800/8989 - Loss: 0.2084\n",
            "  Batch 4900/8989 - Loss: 0.2085\n",
            "  Batch 5000/8989 - Loss: 0.2088\n",
            "  Batch 5100/8989 - Loss: 0.2089\n",
            "  Batch 5200/8989 - Loss: 0.2089\n",
            "  Batch 5300/8989 - Loss: 0.2090\n",
            "  Batch 5400/8989 - Loss: 0.2092\n",
            "  Batch 5500/8989 - Loss: 0.2093\n",
            "  Batch 5600/8989 - Loss: 0.2094\n",
            "  Batch 5700/8989 - Loss: 0.2096\n",
            "  Batch 5800/8989 - Loss: 0.2098\n",
            "  Batch 5900/8989 - Loss: 0.2100\n",
            "  Batch 6000/8989 - Loss: 0.2100\n",
            "  Batch 6100/8989 - Loss: 0.2102\n",
            "  Batch 6200/8989 - Loss: 0.2102\n",
            "  Batch 6300/8989 - Loss: 0.2103\n",
            "  Batch 6400/8989 - Loss: 0.2105\n",
            "  Batch 6500/8989 - Loss: 0.2105\n",
            "  Batch 6600/8989 - Loss: 0.2106\n",
            "  Batch 6700/8989 - Loss: 0.2107\n",
            "  Batch 6800/8989 - Loss: 0.2108\n",
            "  Batch 6900/8989 - Loss: 0.2110\n",
            "  Batch 7000/8989 - Loss: 0.2111\n",
            "  Batch 7100/8989 - Loss: 0.2112\n",
            "  Batch 7200/8989 - Loss: 0.2113\n",
            "  Batch 7300/8989 - Loss: 0.2115\n",
            "  Batch 7400/8989 - Loss: 0.2115\n",
            "  Batch 7500/8989 - Loss: 0.2116\n",
            "  Batch 7600/8989 - Loss: 0.2118\n",
            "  Batch 7700/8989 - Loss: 0.2119\n",
            "  Batch 7800/8989 - Loss: 0.2120\n",
            "  Batch 7900/8989 - Loss: 0.2121\n",
            "  Batch 8000/8989 - Loss: 0.2122\n",
            "  Batch 8100/8989 - Loss: 0.2123\n",
            "  Batch 8200/8989 - Loss: 0.2124\n",
            "  Batch 8300/8989 - Loss: 0.2124\n",
            "  Batch 8400/8989 - Loss: 0.2125\n",
            "  Batch 8500/8989 - Loss: 0.2125\n",
            "  Batch 8600/8989 - Loss: 0.2126\n",
            "  Batch 8700/8989 - Loss: 0.2127\n",
            "  Batch 8800/8989 - Loss: 0.2128\n",
            "  Batch 8900/8989 - Loss: 0.2129\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:14\n",
            "  Loss: 0.2130\n",
            "  HR@10: 0.7361\n",
            "  NDCG@10: 0.4536\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 10/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1953\n",
            "  Batch 200/8989 - Loss: 0.1950\n",
            "  Batch 300/8989 - Loss: 0.1949\n",
            "  Batch 400/8989 - Loss: 0.1954\n",
            "  Batch 500/8989 - Loss: 0.1957\n",
            "  Batch 600/8989 - Loss: 0.1959\n",
            "  Batch 700/8989 - Loss: 0.1967\n",
            "  Batch 800/8989 - Loss: 0.1972\n",
            "  Batch 900/8989 - Loss: 0.1972\n",
            "  Batch 1000/8989 - Loss: 0.1975\n",
            "  Batch 1100/8989 - Loss: 0.1978\n",
            "  Batch 1200/8989 - Loss: 0.1985\n",
            "  Batch 1300/8989 - Loss: 0.1991\n",
            "  Batch 1400/8989 - Loss: 0.1994\n",
            "  Batch 1500/8989 - Loss: 0.1998\n",
            "  Batch 1600/8989 - Loss: 0.1999\n",
            "  Batch 1700/8989 - Loss: 0.1999\n",
            "  Batch 1800/8989 - Loss: 0.2003\n",
            "  Batch 1900/8989 - Loss: 0.2006\n",
            "  Batch 2000/8989 - Loss: 0.2006\n",
            "  Batch 2100/8989 - Loss: 0.2010\n",
            "  Batch 2200/8989 - Loss: 0.2012\n",
            "  Batch 2300/8989 - Loss: 0.2014\n",
            "  Batch 2400/8989 - Loss: 0.2015\n",
            "  Batch 2500/8989 - Loss: 0.2017\n",
            "  Batch 2600/8989 - Loss: 0.2018\n",
            "  Batch 2700/8989 - Loss: 0.2020\n",
            "  Batch 2800/8989 - Loss: 0.2022\n",
            "  Batch 2900/8989 - Loss: 0.2024\n",
            "  Batch 3000/8989 - Loss: 0.2024\n",
            "  Batch 3100/8989 - Loss: 0.2027\n",
            "  Batch 3200/8989 - Loss: 0.2028\n",
            "  Batch 3300/8989 - Loss: 0.2030\n",
            "  Batch 3400/8989 - Loss: 0.2034\n",
            "  Batch 3500/8989 - Loss: 0.2036\n",
            "  Batch 3600/8989 - Loss: 0.2038\n",
            "  Batch 3700/8989 - Loss: 0.2042\n",
            "  Batch 3800/8989 - Loss: 0.2044\n",
            "  Batch 3900/8989 - Loss: 0.2046\n",
            "  Batch 4000/8989 - Loss: 0.2047\n",
            "  Batch 4100/8989 - Loss: 0.2049\n",
            "  Batch 4200/8989 - Loss: 0.2051\n",
            "  Batch 4300/8989 - Loss: 0.2052\n",
            "  Batch 4400/8989 - Loss: 0.2052\n",
            "  Batch 4500/8989 - Loss: 0.2053\n",
            "  Batch 4600/8989 - Loss: 0.2055\n",
            "  Batch 4700/8989 - Loss: 0.2056\n",
            "  Batch 4800/8989 - Loss: 0.2057\n",
            "  Batch 4900/8989 - Loss: 0.2058\n",
            "  Batch 5000/8989 - Loss: 0.2060\n",
            "  Batch 5100/8989 - Loss: 0.2060\n",
            "  Batch 5200/8989 - Loss: 0.2061\n",
            "  Batch 5300/8989 - Loss: 0.2062\n",
            "  Batch 5400/8989 - Loss: 0.2063\n",
            "  Batch 5500/8989 - Loss: 0.2063\n",
            "  Batch 5600/8989 - Loss: 0.2064\n",
            "  Batch 5700/8989 - Loss: 0.2065\n",
            "  Batch 5800/8989 - Loss: 0.2065\n",
            "  Batch 5900/8989 - Loss: 0.2067\n",
            "  Batch 6000/8989 - Loss: 0.2068\n",
            "  Batch 6100/8989 - Loss: 0.2069\n",
            "  Batch 6200/8989 - Loss: 0.2070\n",
            "  Batch 6300/8989 - Loss: 0.2071\n",
            "  Batch 6400/8989 - Loss: 0.2072\n",
            "  Batch 6500/8989 - Loss: 0.2073\n",
            "  Batch 6600/8989 - Loss: 0.2073\n",
            "  Batch 6700/8989 - Loss: 0.2075\n",
            "  Batch 6800/8989 - Loss: 0.2076\n",
            "  Batch 6900/8989 - Loss: 0.2078\n",
            "  Batch 7000/8989 - Loss: 0.2080\n",
            "  Batch 7100/8989 - Loss: 0.2080\n",
            "  Batch 7200/8989 - Loss: 0.2082\n",
            "  Batch 7300/8989 - Loss: 0.2083\n",
            "  Batch 7400/8989 - Loss: 0.2084\n",
            "  Batch 7500/8989 - Loss: 0.2085\n",
            "  Batch 7600/8989 - Loss: 0.2087\n",
            "  Batch 7700/8989 - Loss: 0.2088\n",
            "  Batch 7800/8989 - Loss: 0.2089\n",
            "  Batch 7900/8989 - Loss: 0.2090\n",
            "  Batch 8000/8989 - Loss: 0.2092\n",
            "  Batch 8100/8989 - Loss: 0.2092\n",
            "  Batch 8200/8989 - Loss: 0.2094\n",
            "  Batch 8300/8989 - Loss: 0.2094\n",
            "  Batch 8400/8989 - Loss: 0.2095\n",
            "  Batch 8500/8989 - Loss: 0.2097\n",
            "  Batch 8600/8989 - Loss: 0.2097\n",
            "  Batch 8700/8989 - Loss: 0.2099\n",
            "  Batch 8800/8989 - Loss: 0.2100\n",
            "  Batch 8900/8989 - Loss: 0.2100\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:14\n",
            "  Loss: 0.2101\n",
            "  HR@10: 0.7366\n",
            "  NDCG@10: 0.4536\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 11/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1933\n",
            "  Batch 200/8989 - Loss: 0.1932\n",
            "  Batch 300/8989 - Loss: 0.1935\n",
            "  Batch 400/8989 - Loss: 0.1940\n",
            "  Batch 500/8989 - Loss: 0.1948\n",
            "  Batch 600/8989 - Loss: 0.1948\n",
            "  Batch 700/8989 - Loss: 0.1948\n",
            "  Batch 800/8989 - Loss: 0.1948\n",
            "  Batch 900/8989 - Loss: 0.1948\n",
            "  Batch 1000/8989 - Loss: 0.1950\n",
            "  Batch 1100/8989 - Loss: 0.1954\n",
            "  Batch 1200/8989 - Loss: 0.1960\n",
            "  Batch 1300/8989 - Loss: 0.1962\n",
            "  Batch 1400/8989 - Loss: 0.1967\n",
            "  Batch 1500/8989 - Loss: 0.1967\n",
            "  Batch 1600/8989 - Loss: 0.1969\n",
            "  Batch 1700/8989 - Loss: 0.1970\n",
            "  Batch 1800/8989 - Loss: 0.1974\n",
            "  Batch 1900/8989 - Loss: 0.1978\n",
            "  Batch 2000/8989 - Loss: 0.1977\n",
            "  Batch 2100/8989 - Loss: 0.1979\n",
            "  Batch 2200/8989 - Loss: 0.1980\n",
            "  Batch 2300/8989 - Loss: 0.1983\n",
            "  Batch 2400/8989 - Loss: 0.1985\n",
            "  Batch 2500/8989 - Loss: 0.1987\n",
            "  Batch 2600/8989 - Loss: 0.1989\n",
            "  Batch 2700/8989 - Loss: 0.1991\n",
            "  Batch 2800/8989 - Loss: 0.1994\n",
            "  Batch 2900/8989 - Loss: 0.1995\n",
            "  Batch 3000/8989 - Loss: 0.1997\n",
            "  Batch 3100/8989 - Loss: 0.2000\n",
            "  Batch 3200/8989 - Loss: 0.2000\n",
            "  Batch 3300/8989 - Loss: 0.2001\n",
            "  Batch 3400/8989 - Loss: 0.2003\n",
            "  Batch 3500/8989 - Loss: 0.2006\n",
            "  Batch 3600/8989 - Loss: 0.2008\n",
            "  Batch 3700/8989 - Loss: 0.2011\n",
            "  Batch 3800/8989 - Loss: 0.2012\n",
            "  Batch 3900/8989 - Loss: 0.2013\n",
            "  Batch 4000/8989 - Loss: 0.2015\n",
            "  Batch 4100/8989 - Loss: 0.2018\n",
            "  Batch 4200/8989 - Loss: 0.2019\n",
            "  Batch 4300/8989 - Loss: 0.2021\n",
            "  Batch 4400/8989 - Loss: 0.2024\n",
            "  Batch 4500/8989 - Loss: 0.2025\n",
            "  Batch 4600/8989 - Loss: 0.2027\n",
            "  Batch 4700/8989 - Loss: 0.2027\n",
            "  Batch 4800/8989 - Loss: 0.2029\n",
            "  Batch 4900/8989 - Loss: 0.2030\n",
            "  Batch 5000/8989 - Loss: 0.2030\n",
            "  Batch 5100/8989 - Loss: 0.2031\n",
            "  Batch 5200/8989 - Loss: 0.2033\n",
            "  Batch 5300/8989 - Loss: 0.2034\n",
            "  Batch 5400/8989 - Loss: 0.2035\n",
            "  Batch 5500/8989 - Loss: 0.2036\n",
            "  Batch 5600/8989 - Loss: 0.2037\n",
            "  Batch 5700/8989 - Loss: 0.2039\n",
            "  Batch 5800/8989 - Loss: 0.2040\n",
            "  Batch 5900/8989 - Loss: 0.2042\n",
            "  Batch 6000/8989 - Loss: 0.2043\n",
            "  Batch 6100/8989 - Loss: 0.2044\n",
            "  Batch 6200/8989 - Loss: 0.2045\n",
            "  Batch 6300/8989 - Loss: 0.2046\n",
            "  Batch 6400/8989 - Loss: 0.2048\n",
            "  Batch 6500/8989 - Loss: 0.2049\n",
            "  Batch 6600/8989 - Loss: 0.2050\n",
            "  Batch 6700/8989 - Loss: 0.2051\n",
            "  Batch 6800/8989 - Loss: 0.2052\n",
            "  Batch 6900/8989 - Loss: 0.2054\n",
            "  Batch 7000/8989 - Loss: 0.2055\n",
            "  Batch 7100/8989 - Loss: 0.2056\n",
            "  Batch 7200/8989 - Loss: 0.2057\n",
            "  Batch 7300/8989 - Loss: 0.2057\n",
            "  Batch 7400/8989 - Loss: 0.2058\n",
            "  Batch 7500/8989 - Loss: 0.2059\n",
            "  Batch 7600/8989 - Loss: 0.2061\n",
            "  Batch 7700/8989 - Loss: 0.2061\n",
            "  Batch 7800/8989 - Loss: 0.2062\n",
            "  Batch 7900/8989 - Loss: 0.2063\n",
            "  Batch 8000/8989 - Loss: 0.2064\n",
            "  Batch 8100/8989 - Loss: 0.2065\n",
            "  Batch 8200/8989 - Loss: 0.2067\n",
            "  Batch 8300/8989 - Loss: 0.2067\n",
            "  Batch 8400/8989 - Loss: 0.2067\n",
            "  Batch 8500/8989 - Loss: 0.2068\n",
            "  Batch 8600/8989 - Loss: 0.2069\n",
            "  Batch 8700/8989 - Loss: 0.2070\n",
            "  Batch 8800/8989 - Loss: 0.2070\n",
            "  Batch 8900/8989 - Loss: 0.2071\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:15\n",
            "  Loss: 0.2071\n",
            "  HR@10: 0.7383\n",
            "  NDCG@10: 0.4563\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 12/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1939\n",
            "  Batch 200/8989 - Loss: 0.1950\n",
            "  Batch 300/8989 - Loss: 0.1935\n",
            "  Batch 400/8989 - Loss: 0.1942\n",
            "  Batch 500/8989 - Loss: 0.1936\n",
            "  Batch 600/8989 - Loss: 0.1945\n",
            "  Batch 700/8989 - Loss: 0.1948\n",
            "  Batch 800/8989 - Loss: 0.1950\n",
            "  Batch 900/8989 - Loss: 0.1951\n",
            "  Batch 1000/8989 - Loss: 0.1956\n",
            "  Batch 1100/8989 - Loss: 0.1956\n",
            "  Batch 1200/8989 - Loss: 0.1960\n",
            "  Batch 1300/8989 - Loss: 0.1961\n",
            "  Batch 1400/8989 - Loss: 0.1960\n",
            "  Batch 1500/8989 - Loss: 0.1963\n",
            "  Batch 1600/8989 - Loss: 0.1967\n",
            "  Batch 1700/8989 - Loss: 0.1967\n",
            "  Batch 1800/8989 - Loss: 0.1969\n",
            "  Batch 1900/8989 - Loss: 0.1968\n",
            "  Batch 2000/8989 - Loss: 0.1967\n",
            "  Batch 2100/8989 - Loss: 0.1969\n",
            "  Batch 2200/8989 - Loss: 0.1970\n",
            "  Batch 2300/8989 - Loss: 0.1971\n",
            "  Batch 2400/8989 - Loss: 0.1973\n",
            "  Batch 2500/8989 - Loss: 0.1974\n",
            "  Batch 2600/8989 - Loss: 0.1974\n",
            "  Batch 2700/8989 - Loss: 0.1976\n",
            "  Batch 2800/8989 - Loss: 0.1978\n",
            "  Batch 2900/8989 - Loss: 0.1978\n",
            "  Batch 3000/8989 - Loss: 0.1981\n",
            "  Batch 3100/8989 - Loss: 0.1982\n",
            "  Batch 3200/8989 - Loss: 0.1984\n",
            "  Batch 3300/8989 - Loss: 0.1984\n",
            "  Batch 3400/8989 - Loss: 0.1986\n",
            "  Batch 3500/8989 - Loss: 0.1988\n",
            "  Batch 3600/8989 - Loss: 0.1988\n",
            "  Batch 3700/8989 - Loss: 0.1989\n",
            "  Batch 3800/8989 - Loss: 0.1991\n",
            "  Batch 3900/8989 - Loss: 0.1992\n",
            "  Batch 4000/8989 - Loss: 0.1994\n",
            "  Batch 4100/8989 - Loss: 0.1995\n",
            "  Batch 4200/8989 - Loss: 0.1997\n",
            "  Batch 4300/8989 - Loss: 0.1999\n",
            "  Batch 4400/8989 - Loss: 0.2000\n",
            "  Batch 4500/8989 - Loss: 0.2001\n",
            "  Batch 4600/8989 - Loss: 0.2003\n",
            "  Batch 4700/8989 - Loss: 0.2004\n",
            "  Batch 4800/8989 - Loss: 0.2005\n",
            "  Batch 4900/8989 - Loss: 0.2006\n",
            "  Batch 5000/8989 - Loss: 0.2007\n",
            "  Batch 5100/8989 - Loss: 0.2007\n",
            "  Batch 5200/8989 - Loss: 0.2009\n",
            "  Batch 5300/8989 - Loss: 0.2010\n",
            "  Batch 5400/8989 - Loss: 0.2011\n",
            "  Batch 5500/8989 - Loss: 0.2013\n",
            "  Batch 5600/8989 - Loss: 0.2014\n",
            "  Batch 5700/8989 - Loss: 0.2016\n",
            "  Batch 5800/8989 - Loss: 0.2018\n",
            "  Batch 5900/8989 - Loss: 0.2019\n",
            "  Batch 6000/8989 - Loss: 0.2020\n",
            "  Batch 6100/8989 - Loss: 0.2021\n",
            "  Batch 6200/8989 - Loss: 0.2022\n",
            "  Batch 6300/8989 - Loss: 0.2023\n",
            "  Batch 6400/8989 - Loss: 0.2025\n",
            "  Batch 6500/8989 - Loss: 0.2025\n",
            "  Batch 6600/8989 - Loss: 0.2027\n",
            "  Batch 6700/8989 - Loss: 0.2029\n",
            "  Batch 6800/8989 - Loss: 0.2030\n",
            "  Batch 6900/8989 - Loss: 0.2031\n",
            "  Batch 7000/8989 - Loss: 0.2032\n",
            "  Batch 7100/8989 - Loss: 0.2032\n",
            "  Batch 7200/8989 - Loss: 0.2033\n",
            "  Batch 7300/8989 - Loss: 0.2034\n",
            "  Batch 7400/8989 - Loss: 0.2035\n",
            "  Batch 7500/8989 - Loss: 0.2036\n",
            "  Batch 7600/8989 - Loss: 0.2038\n",
            "  Batch 7700/8989 - Loss: 0.2039\n",
            "  Batch 7800/8989 - Loss: 0.2040\n",
            "  Batch 7900/8989 - Loss: 0.2040\n",
            "  Batch 8000/8989 - Loss: 0.2042\n",
            "  Batch 8100/8989 - Loss: 0.2042\n",
            "  Batch 8200/8989 - Loss: 0.2043\n",
            "  Batch 8300/8989 - Loss: 0.2044\n",
            "  Batch 8400/8989 - Loss: 0.2044\n",
            "  Batch 8500/8989 - Loss: 0.2045\n",
            "  Batch 8600/8989 - Loss: 0.2046\n",
            "  Batch 8700/8989 - Loss: 0.2047\n",
            "  Batch 8800/8989 - Loss: 0.2047\n",
            "  Batch 8900/8989 - Loss: 0.2048\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:11\n",
            "  Loss: 0.2048\n",
            "  HR@10: 0.7361\n",
            "  NDCG@10: 0.4525\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 13/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1908\n",
            "  Batch 200/8989 - Loss: 0.1919\n",
            "  Batch 300/8989 - Loss: 0.1910\n",
            "  Batch 400/8989 - Loss: 0.1906\n",
            "  Batch 500/8989 - Loss: 0.1906\n",
            "  Batch 600/8989 - Loss: 0.1908\n",
            "  Batch 700/8989 - Loss: 0.1906\n",
            "  Batch 800/8989 - Loss: 0.1907\n",
            "  Batch 900/8989 - Loss: 0.1916\n",
            "  Batch 1000/8989 - Loss: 0.1921\n",
            "  Batch 1100/8989 - Loss: 0.1923\n",
            "  Batch 1200/8989 - Loss: 0.1922\n",
            "  Batch 1300/8989 - Loss: 0.1924\n",
            "  Batch 1400/8989 - Loss: 0.1927\n",
            "  Batch 1500/8989 - Loss: 0.1928\n",
            "  Batch 1600/8989 - Loss: 0.1930\n",
            "  Batch 1700/8989 - Loss: 0.1934\n",
            "  Batch 1800/8989 - Loss: 0.1936\n",
            "  Batch 1900/8989 - Loss: 0.1938\n",
            "  Batch 2000/8989 - Loss: 0.1940\n",
            "  Batch 2100/8989 - Loss: 0.1944\n",
            "  Batch 2200/8989 - Loss: 0.1944\n",
            "  Batch 2300/8989 - Loss: 0.1945\n",
            "  Batch 2400/8989 - Loss: 0.1947\n",
            "  Batch 2500/8989 - Loss: 0.1950\n",
            "  Batch 2600/8989 - Loss: 0.1950\n",
            "  Batch 2700/8989 - Loss: 0.1953\n",
            "  Batch 2800/8989 - Loss: 0.1955\n",
            "  Batch 2900/8989 - Loss: 0.1955\n",
            "  Batch 3000/8989 - Loss: 0.1956\n",
            "  Batch 3100/8989 - Loss: 0.1958\n",
            "  Batch 3200/8989 - Loss: 0.1958\n",
            "  Batch 3300/8989 - Loss: 0.1959\n",
            "  Batch 3400/8989 - Loss: 0.1962\n",
            "  Batch 3500/8989 - Loss: 0.1963\n",
            "  Batch 3600/8989 - Loss: 0.1964\n",
            "  Batch 3700/8989 - Loss: 0.1966\n",
            "  Batch 3800/8989 - Loss: 0.1968\n",
            "  Batch 3900/8989 - Loss: 0.1969\n",
            "  Batch 4000/8989 - Loss: 0.1970\n",
            "  Batch 4100/8989 - Loss: 0.1973\n",
            "  Batch 4200/8989 - Loss: 0.1975\n",
            "  Batch 4300/8989 - Loss: 0.1976\n",
            "  Batch 4400/8989 - Loss: 0.1977\n",
            "  Batch 4500/8989 - Loss: 0.1978\n",
            "  Batch 4600/8989 - Loss: 0.1979\n",
            "  Batch 4700/8989 - Loss: 0.1979\n",
            "  Batch 4800/8989 - Loss: 0.1981\n",
            "  Batch 4900/8989 - Loss: 0.1982\n",
            "  Batch 5000/8989 - Loss: 0.1982\n",
            "  Batch 5100/8989 - Loss: 0.1983\n",
            "  Batch 5200/8989 - Loss: 0.1984\n",
            "  Batch 5300/8989 - Loss: 0.1985\n",
            "  Batch 5400/8989 - Loss: 0.1985\n",
            "  Batch 5500/8989 - Loss: 0.1986\n",
            "  Batch 5600/8989 - Loss: 0.1987\n",
            "  Batch 5700/8989 - Loss: 0.1988\n",
            "  Batch 5800/8989 - Loss: 0.1990\n",
            "  Batch 5900/8989 - Loss: 0.1992\n",
            "  Batch 6000/8989 - Loss: 0.1994\n",
            "  Batch 6100/8989 - Loss: 0.1995\n",
            "  Batch 6200/8989 - Loss: 0.1997\n",
            "  Batch 6300/8989 - Loss: 0.1999\n",
            "  Batch 6400/8989 - Loss: 0.2001\n",
            "  Batch 6500/8989 - Loss: 0.2002\n",
            "  Batch 6600/8989 - Loss: 0.2003\n",
            "  Batch 6700/8989 - Loss: 0.2004\n",
            "  Batch 6800/8989 - Loss: 0.2005\n",
            "  Batch 6900/8989 - Loss: 0.2007\n",
            "  Batch 7000/8989 - Loss: 0.2008\n",
            "  Batch 7100/8989 - Loss: 0.2009\n",
            "  Batch 7200/8989 - Loss: 0.2009\n",
            "  Batch 7300/8989 - Loss: 0.2011\n",
            "  Batch 7400/8989 - Loss: 0.2012\n",
            "  Batch 7500/8989 - Loss: 0.2013\n",
            "  Batch 7600/8989 - Loss: 0.2014\n",
            "  Batch 7700/8989 - Loss: 0.2014\n",
            "  Batch 7800/8989 - Loss: 0.2015\n",
            "  Batch 7900/8989 - Loss: 0.2016\n",
            "  Batch 8000/8989 - Loss: 0.2016\n",
            "  Batch 8100/8989 - Loss: 0.2017\n",
            "  Batch 8200/8989 - Loss: 0.2018\n",
            "  Batch 8300/8989 - Loss: 0.2019\n",
            "  Batch 8400/8989 - Loss: 0.2020\n",
            "  Batch 8500/8989 - Loss: 0.2021\n",
            "  Batch 8600/8989 - Loss: 0.2022\n",
            "  Batch 8700/8989 - Loss: 0.2023\n",
            "  Batch 8800/8989 - Loss: 0.2024\n",
            "  Batch 8900/8989 - Loss: 0.2024\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:18\n",
            "  Loss: 0.2025\n",
            "  HR@10: 0.7386\n",
            "  NDCG@10: 0.4553\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 14/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1868\n",
            "  Batch 200/8989 - Loss: 0.1895\n",
            "  Batch 300/8989 - Loss: 0.1902\n",
            "  Batch 400/8989 - Loss: 0.1892\n",
            "  Batch 500/8989 - Loss: 0.1894\n",
            "  Batch 600/8989 - Loss: 0.1891\n",
            "  Batch 700/8989 - Loss: 0.1899\n",
            "  Batch 800/8989 - Loss: 0.1902\n",
            "  Batch 900/8989 - Loss: 0.1904\n",
            "  Batch 1000/8989 - Loss: 0.1909\n",
            "  Batch 1100/8989 - Loss: 0.1911\n",
            "  Batch 1200/8989 - Loss: 0.1909\n",
            "  Batch 1300/8989 - Loss: 0.1910\n",
            "  Batch 1400/8989 - Loss: 0.1911\n",
            "  Batch 1500/8989 - Loss: 0.1912\n",
            "  Batch 1600/8989 - Loss: 0.1913\n",
            "  Batch 1700/8989 - Loss: 0.1917\n",
            "  Batch 1800/8989 - Loss: 0.1920\n",
            "  Batch 1900/8989 - Loss: 0.1920\n",
            "  Batch 2000/8989 - Loss: 0.1921\n",
            "  Batch 2100/8989 - Loss: 0.1923\n",
            "  Batch 2200/8989 - Loss: 0.1923\n",
            "  Batch 2300/8989 - Loss: 0.1926\n",
            "  Batch 2400/8989 - Loss: 0.1928\n",
            "  Batch 2500/8989 - Loss: 0.1928\n",
            "  Batch 2600/8989 - Loss: 0.1931\n",
            "  Batch 2700/8989 - Loss: 0.1931\n",
            "  Batch 2800/8989 - Loss: 0.1935\n",
            "  Batch 2900/8989 - Loss: 0.1937\n",
            "  Batch 3000/8989 - Loss: 0.1939\n",
            "  Batch 3100/8989 - Loss: 0.1940\n",
            "  Batch 3200/8989 - Loss: 0.1940\n",
            "  Batch 3300/8989 - Loss: 0.1940\n",
            "  Batch 3400/8989 - Loss: 0.1940\n",
            "  Batch 3500/8989 - Loss: 0.1942\n",
            "  Batch 3600/8989 - Loss: 0.1944\n",
            "  Batch 3700/8989 - Loss: 0.1946\n",
            "  Batch 3800/8989 - Loss: 0.1948\n",
            "  Batch 3900/8989 - Loss: 0.1949\n",
            "  Batch 4000/8989 - Loss: 0.1952\n",
            "  Batch 4100/8989 - Loss: 0.1954\n",
            "  Batch 4200/8989 - Loss: 0.1956\n",
            "  Batch 4300/8989 - Loss: 0.1957\n",
            "  Batch 4400/8989 - Loss: 0.1959\n",
            "  Batch 4500/8989 - Loss: 0.1960\n",
            "  Batch 4600/8989 - Loss: 0.1961\n",
            "  Batch 4700/8989 - Loss: 0.1962\n",
            "  Batch 4800/8989 - Loss: 0.1963\n",
            "  Batch 4900/8989 - Loss: 0.1966\n",
            "  Batch 5000/8989 - Loss: 0.1967\n",
            "  Batch 5100/8989 - Loss: 0.1968\n",
            "  Batch 5200/8989 - Loss: 0.1969\n",
            "  Batch 5300/8989 - Loss: 0.1969\n",
            "  Batch 5400/8989 - Loss: 0.1971\n",
            "  Batch 5500/8989 - Loss: 0.1972\n",
            "  Batch 5600/8989 - Loss: 0.1973\n",
            "  Batch 5700/8989 - Loss: 0.1974\n",
            "  Batch 5800/8989 - Loss: 0.1976\n",
            "  Batch 5900/8989 - Loss: 0.1976\n",
            "  Batch 6000/8989 - Loss: 0.1977\n",
            "  Batch 6100/8989 - Loss: 0.1978\n",
            "  Batch 6200/8989 - Loss: 0.1980\n",
            "  Batch 6300/8989 - Loss: 0.1981\n",
            "  Batch 6400/8989 - Loss: 0.1983\n",
            "  Batch 6500/8989 - Loss: 0.1984\n",
            "  Batch 6600/8989 - Loss: 0.1985\n",
            "  Batch 6700/8989 - Loss: 0.1987\n",
            "  Batch 6800/8989 - Loss: 0.1987\n",
            "  Batch 6900/8989 - Loss: 0.1989\n",
            "  Batch 7000/8989 - Loss: 0.1990\n",
            "  Batch 7100/8989 - Loss: 0.1991\n",
            "  Batch 7200/8989 - Loss: 0.1992\n",
            "  Batch 7300/8989 - Loss: 0.1993\n",
            "  Batch 7400/8989 - Loss: 0.1993\n",
            "  Batch 7500/8989 - Loss: 0.1994\n",
            "  Batch 7600/8989 - Loss: 0.1995\n",
            "  Batch 7700/8989 - Loss: 0.1996\n",
            "  Batch 7800/8989 - Loss: 0.1996\n",
            "  Batch 7900/8989 - Loss: 0.1997\n",
            "  Batch 8000/8989 - Loss: 0.1998\n",
            "  Batch 8100/8989 - Loss: 0.1999\n",
            "  Batch 8200/8989 - Loss: 0.2000\n",
            "  Batch 8300/8989 - Loss: 0.2001\n",
            "  Batch 8400/8989 - Loss: 0.2002\n",
            "  Batch 8500/8989 - Loss: 0.2003\n",
            "  Batch 8600/8989 - Loss: 0.2005\n",
            "  Batch 8700/8989 - Loss: 0.2005\n",
            "  Batch 8800/8989 - Loss: 0.2006\n",
            "  Batch 8900/8989 - Loss: 0.2006\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:13\n",
            "  Loss: 0.2007\n",
            "  HR@10: 0.7368\n",
            "  NDCG@10: 0.4536\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 15/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1863\n",
            "  Batch 200/8989 - Loss: 0.1846\n",
            "  Batch 300/8989 - Loss: 0.1855\n",
            "  Batch 400/8989 - Loss: 0.1864\n",
            "  Batch 500/8989 - Loss: 0.1875\n",
            "  Batch 600/8989 - Loss: 0.1874\n",
            "  Batch 700/8989 - Loss: 0.1876\n",
            "  Batch 800/8989 - Loss: 0.1883\n",
            "  Batch 900/8989 - Loss: 0.1882\n",
            "  Batch 1000/8989 - Loss: 0.1886\n",
            "  Batch 1100/8989 - Loss: 0.1889\n",
            "  Batch 1200/8989 - Loss: 0.1891\n",
            "  Batch 1300/8989 - Loss: 0.1893\n",
            "  Batch 1400/8989 - Loss: 0.1894\n",
            "  Batch 1500/8989 - Loss: 0.1895\n",
            "  Batch 1600/8989 - Loss: 0.1895\n",
            "  Batch 1700/8989 - Loss: 0.1896\n",
            "  Batch 1800/8989 - Loss: 0.1897\n",
            "  Batch 1900/8989 - Loss: 0.1902\n",
            "  Batch 2000/8989 - Loss: 0.1903\n",
            "  Batch 2100/8989 - Loss: 0.1907\n",
            "  Batch 2200/8989 - Loss: 0.1910\n",
            "  Batch 2300/8989 - Loss: 0.1912\n",
            "  Batch 2400/8989 - Loss: 0.1914\n",
            "  Batch 2500/8989 - Loss: 0.1916\n",
            "  Batch 2600/8989 - Loss: 0.1918\n",
            "  Batch 2700/8989 - Loss: 0.1918\n",
            "  Batch 2800/8989 - Loss: 0.1921\n",
            "  Batch 2900/8989 - Loss: 0.1923\n",
            "  Batch 3000/8989 - Loss: 0.1924\n",
            "  Batch 3100/8989 - Loss: 0.1926\n",
            "  Batch 3200/8989 - Loss: 0.1927\n",
            "  Batch 3300/8989 - Loss: 0.1928\n",
            "  Batch 3400/8989 - Loss: 0.1929\n",
            "  Batch 3500/8989 - Loss: 0.1931\n",
            "  Batch 3600/8989 - Loss: 0.1932\n",
            "  Batch 3700/8989 - Loss: 0.1933\n",
            "  Batch 3800/8989 - Loss: 0.1934\n",
            "  Batch 3900/8989 - Loss: 0.1934\n",
            "  Batch 4000/8989 - Loss: 0.1934\n",
            "  Batch 4100/8989 - Loss: 0.1935\n",
            "  Batch 4200/8989 - Loss: 0.1936\n",
            "  Batch 4300/8989 - Loss: 0.1937\n",
            "  Batch 4400/8989 - Loss: 0.1939\n",
            "  Batch 4500/8989 - Loss: 0.1941\n",
            "  Batch 4600/8989 - Loss: 0.1942\n",
            "  Batch 4700/8989 - Loss: 0.1943\n",
            "  Batch 4800/8989 - Loss: 0.1945\n",
            "  Batch 4900/8989 - Loss: 0.1945\n",
            "  Batch 5000/8989 - Loss: 0.1947\n",
            "  Batch 5100/8989 - Loss: 0.1948\n",
            "  Batch 5200/8989 - Loss: 0.1950\n",
            "  Batch 5300/8989 - Loss: 0.1951\n",
            "  Batch 5400/8989 - Loss: 0.1953\n",
            "  Batch 5500/8989 - Loss: 0.1953\n",
            "  Batch 5600/8989 - Loss: 0.1954\n",
            "  Batch 5700/8989 - Loss: 0.1955\n",
            "  Batch 5800/8989 - Loss: 0.1956\n",
            "  Batch 5900/8989 - Loss: 0.1957\n",
            "  Batch 6000/8989 - Loss: 0.1957\n",
            "  Batch 6100/8989 - Loss: 0.1958\n",
            "  Batch 6200/8989 - Loss: 0.1959\n",
            "  Batch 6300/8989 - Loss: 0.1961\n",
            "  Batch 6400/8989 - Loss: 0.1962\n",
            "  Batch 6500/8989 - Loss: 0.1963\n",
            "  Batch 6600/8989 - Loss: 0.1964\n",
            "  Batch 6700/8989 - Loss: 0.1965\n",
            "  Batch 6800/8989 - Loss: 0.1966\n",
            "  Batch 6900/8989 - Loss: 0.1968\n",
            "  Batch 7000/8989 - Loss: 0.1968\n",
            "  Batch 7100/8989 - Loss: 0.1969\n",
            "  Batch 7200/8989 - Loss: 0.1970\n",
            "  Batch 7300/8989 - Loss: 0.1970\n",
            "  Batch 7400/8989 - Loss: 0.1971\n",
            "  Batch 7500/8989 - Loss: 0.1972\n",
            "  Batch 7600/8989 - Loss: 0.1973\n",
            "  Batch 7700/8989 - Loss: 0.1974\n",
            "  Batch 7800/8989 - Loss: 0.1975\n",
            "  Batch 7900/8989 - Loss: 0.1976\n",
            "  Batch 8000/8989 - Loss: 0.1977\n",
            "  Batch 8100/8989 - Loss: 0.1979\n",
            "  Batch 8200/8989 - Loss: 0.1979\n",
            "  Batch 8300/8989 - Loss: 0.1980\n",
            "  Batch 8400/8989 - Loss: 0.1981\n",
            "  Batch 8500/8989 - Loss: 0.1982\n",
            "  Batch 8600/8989 - Loss: 0.1983\n",
            "  Batch 8700/8989 - Loss: 0.1984\n",
            "  Batch 8800/8989 - Loss: 0.1985\n",
            "  Batch 8900/8989 - Loss: 0.1986\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:11\n",
            "  Loss: 0.1987\n",
            "  HR@10: 0.7361\n",
            "  NDCG@10: 0.4536\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 16/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1883\n",
            "  Batch 200/8989 - Loss: 0.1880\n",
            "  Batch 300/8989 - Loss: 0.1864\n",
            "  Batch 400/8989 - Loss: 0.1871\n",
            "  Batch 500/8989 - Loss: 0.1868\n",
            "  Batch 600/8989 - Loss: 0.1874\n",
            "  Batch 700/8989 - Loss: 0.1874\n",
            "  Batch 800/8989 - Loss: 0.1868\n",
            "  Batch 900/8989 - Loss: 0.1868\n",
            "  Batch 1000/8989 - Loss: 0.1871\n",
            "  Batch 1100/8989 - Loss: 0.1875\n",
            "  Batch 1200/8989 - Loss: 0.1877\n",
            "  Batch 1300/8989 - Loss: 0.1880\n",
            "  Batch 1400/8989 - Loss: 0.1881\n",
            "  Batch 1500/8989 - Loss: 0.1882\n",
            "  Batch 1600/8989 - Loss: 0.1883\n",
            "  Batch 1700/8989 - Loss: 0.1884\n",
            "  Batch 1800/8989 - Loss: 0.1884\n",
            "  Batch 1900/8989 - Loss: 0.1886\n",
            "  Batch 2000/8989 - Loss: 0.1887\n",
            "  Batch 2100/8989 - Loss: 0.1889\n",
            "  Batch 2200/8989 - Loss: 0.1892\n",
            "  Batch 2300/8989 - Loss: 0.1893\n",
            "  Batch 2400/8989 - Loss: 0.1896\n",
            "  Batch 2500/8989 - Loss: 0.1896\n",
            "  Batch 2600/8989 - Loss: 0.1898\n",
            "  Batch 2700/8989 - Loss: 0.1899\n",
            "  Batch 2800/8989 - Loss: 0.1900\n",
            "  Batch 2900/8989 - Loss: 0.1904\n",
            "  Batch 3000/8989 - Loss: 0.1905\n",
            "  Batch 3100/8989 - Loss: 0.1905\n",
            "  Batch 3200/8989 - Loss: 0.1906\n",
            "  Batch 3300/8989 - Loss: 0.1907\n",
            "  Batch 3400/8989 - Loss: 0.1908\n",
            "  Batch 3500/8989 - Loss: 0.1908\n",
            "  Batch 3600/8989 - Loss: 0.1910\n",
            "  Batch 3700/8989 - Loss: 0.1912\n",
            "  Batch 3800/8989 - Loss: 0.1914\n",
            "  Batch 3900/8989 - Loss: 0.1914\n",
            "  Batch 4000/8989 - Loss: 0.1915\n",
            "  Batch 4100/8989 - Loss: 0.1917\n",
            "  Batch 4200/8989 - Loss: 0.1917\n",
            "  Batch 4300/8989 - Loss: 0.1919\n",
            "  Batch 4400/8989 - Loss: 0.1921\n",
            "  Batch 4500/8989 - Loss: 0.1923\n",
            "  Batch 4600/8989 - Loss: 0.1924\n",
            "  Batch 4700/8989 - Loss: 0.1924\n",
            "  Batch 4800/8989 - Loss: 0.1925\n",
            "  Batch 4900/8989 - Loss: 0.1927\n",
            "  Batch 5000/8989 - Loss: 0.1928\n",
            "  Batch 5100/8989 - Loss: 0.1929\n",
            "  Batch 5200/8989 - Loss: 0.1931\n",
            "  Batch 5300/8989 - Loss: 0.1932\n",
            "  Batch 5400/8989 - Loss: 0.1934\n",
            "  Batch 5500/8989 - Loss: 0.1934\n",
            "  Batch 5600/8989 - Loss: 0.1936\n",
            "  Batch 5700/8989 - Loss: 0.1937\n",
            "  Batch 5800/8989 - Loss: 0.1937\n",
            "  Batch 5900/8989 - Loss: 0.1938\n",
            "  Batch 6000/8989 - Loss: 0.1939\n",
            "  Batch 6100/8989 - Loss: 0.1940\n",
            "  Batch 6200/8989 - Loss: 0.1941\n",
            "  Batch 6300/8989 - Loss: 0.1942\n",
            "  Batch 6400/8989 - Loss: 0.1943\n",
            "  Batch 6500/8989 - Loss: 0.1944\n",
            "  Batch 6600/8989 - Loss: 0.1944\n",
            "  Batch 6700/8989 - Loss: 0.1946\n",
            "  Batch 6800/8989 - Loss: 0.1947\n",
            "  Batch 6900/8989 - Loss: 0.1949\n",
            "  Batch 7000/8989 - Loss: 0.1949\n",
            "  Batch 7100/8989 - Loss: 0.1949\n",
            "  Batch 7200/8989 - Loss: 0.1951\n",
            "  Batch 7300/8989 - Loss: 0.1952\n",
            "  Batch 7400/8989 - Loss: 0.1952\n",
            "  Batch 7500/8989 - Loss: 0.1954\n",
            "  Batch 7600/8989 - Loss: 0.1954\n",
            "  Batch 7700/8989 - Loss: 0.1955\n",
            "  Batch 7800/8989 - Loss: 0.1956\n",
            "  Batch 7900/8989 - Loss: 0.1957\n",
            "  Batch 8000/8989 - Loss: 0.1959\n",
            "  Batch 8100/8989 - Loss: 0.1960\n",
            "  Batch 8200/8989 - Loss: 0.1961\n",
            "  Batch 8300/8989 - Loss: 0.1962\n",
            "  Batch 8400/8989 - Loss: 0.1963\n",
            "  Batch 8500/8989 - Loss: 0.1964\n",
            "  Batch 8600/8989 - Loss: 0.1965\n",
            "  Batch 8700/8989 - Loss: 0.1965\n",
            "  Batch 8800/8989 - Loss: 0.1966\n",
            "  Batch 8900/8989 - Loss: 0.1966\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:12\n",
            "  Loss: 0.1966\n",
            "  HR@10: 0.7367\n",
            "  NDCG@10: 0.4538\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 17/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1867\n",
            "  Batch 200/8989 - Loss: 0.1855\n",
            "  Batch 300/8989 - Loss: 0.1838\n",
            "  Batch 400/8989 - Loss: 0.1850\n",
            "  Batch 500/8989 - Loss: 0.1852\n",
            "  Batch 600/8989 - Loss: 0.1851\n",
            "  Batch 700/8989 - Loss: 0.1857\n",
            "  Batch 800/8989 - Loss: 0.1858\n",
            "  Batch 900/8989 - Loss: 0.1862\n",
            "  Batch 1000/8989 - Loss: 0.1864\n",
            "  Batch 1100/8989 - Loss: 0.1864\n",
            "  Batch 1200/8989 - Loss: 0.1864\n",
            "  Batch 1300/8989 - Loss: 0.1863\n",
            "  Batch 1400/8989 - Loss: 0.1863\n",
            "  Batch 1500/8989 - Loss: 0.1868\n",
            "  Batch 1600/8989 - Loss: 0.1868\n",
            "  Batch 1700/8989 - Loss: 0.1869\n",
            "  Batch 1800/8989 - Loss: 0.1872\n",
            "  Batch 1900/8989 - Loss: 0.1871\n",
            "  Batch 2000/8989 - Loss: 0.1869\n",
            "  Batch 2100/8989 - Loss: 0.1871\n",
            "  Batch 2200/8989 - Loss: 0.1872\n",
            "  Batch 2300/8989 - Loss: 0.1875\n",
            "  Batch 2400/8989 - Loss: 0.1875\n",
            "  Batch 2500/8989 - Loss: 0.1877\n",
            "  Batch 2600/8989 - Loss: 0.1880\n",
            "  Batch 2700/8989 - Loss: 0.1881\n",
            "  Batch 2800/8989 - Loss: 0.1880\n",
            "  Batch 2900/8989 - Loss: 0.1882\n",
            "  Batch 3000/8989 - Loss: 0.1884\n",
            "  Batch 3100/8989 - Loss: 0.1884\n",
            "  Batch 3200/8989 - Loss: 0.1884\n",
            "  Batch 3300/8989 - Loss: 0.1884\n",
            "  Batch 3400/8989 - Loss: 0.1886\n",
            "  Batch 3500/8989 - Loss: 0.1886\n",
            "  Batch 3600/8989 - Loss: 0.1889\n",
            "  Batch 3700/8989 - Loss: 0.1890\n",
            "  Batch 3800/8989 - Loss: 0.1892\n",
            "  Batch 3900/8989 - Loss: 0.1893\n",
            "  Batch 4000/8989 - Loss: 0.1894\n",
            "  Batch 4100/8989 - Loss: 0.1895\n",
            "  Batch 4200/8989 - Loss: 0.1896\n",
            "  Batch 4300/8989 - Loss: 0.1898\n",
            "  Batch 4400/8989 - Loss: 0.1900\n",
            "  Batch 4500/8989 - Loss: 0.1902\n",
            "  Batch 4600/8989 - Loss: 0.1904\n",
            "  Batch 4700/8989 - Loss: 0.1906\n",
            "  Batch 4800/8989 - Loss: 0.1907\n",
            "  Batch 4900/8989 - Loss: 0.1909\n",
            "  Batch 5000/8989 - Loss: 0.1910\n",
            "  Batch 5100/8989 - Loss: 0.1912\n",
            "  Batch 5200/8989 - Loss: 0.1913\n",
            "  Batch 5300/8989 - Loss: 0.1914\n",
            "  Batch 5400/8989 - Loss: 0.1915\n",
            "  Batch 5500/8989 - Loss: 0.1917\n",
            "  Batch 5600/8989 - Loss: 0.1917\n",
            "  Batch 5700/8989 - Loss: 0.1918\n",
            "  Batch 5800/8989 - Loss: 0.1918\n",
            "  Batch 5900/8989 - Loss: 0.1920\n",
            "  Batch 6000/8989 - Loss: 0.1920\n",
            "  Batch 6100/8989 - Loss: 0.1921\n",
            "  Batch 6200/8989 - Loss: 0.1923\n",
            "  Batch 6300/8989 - Loss: 0.1924\n",
            "  Batch 6400/8989 - Loss: 0.1926\n",
            "  Batch 6500/8989 - Loss: 0.1927\n",
            "  Batch 6600/8989 - Loss: 0.1928\n",
            "  Batch 6700/8989 - Loss: 0.1929\n",
            "  Batch 6800/8989 - Loss: 0.1930\n",
            "  Batch 6900/8989 - Loss: 0.1931\n",
            "  Batch 7000/8989 - Loss: 0.1932\n",
            "  Batch 7100/8989 - Loss: 0.1934\n",
            "  Batch 7200/8989 - Loss: 0.1935\n",
            "  Batch 7300/8989 - Loss: 0.1937\n",
            "  Batch 7400/8989 - Loss: 0.1938\n",
            "  Batch 7500/8989 - Loss: 0.1939\n",
            "  Batch 7600/8989 - Loss: 0.1940\n",
            "  Batch 7700/8989 - Loss: 0.1941\n",
            "  Batch 7800/8989 - Loss: 0.1942\n",
            "  Batch 7900/8989 - Loss: 0.1943\n",
            "  Batch 8000/8989 - Loss: 0.1944\n",
            "  Batch 8100/8989 - Loss: 0.1945\n",
            "  Batch 8200/8989 - Loss: 0.1945\n",
            "  Batch 8300/8989 - Loss: 0.1946\n",
            "  Batch 8400/8989 - Loss: 0.1947\n",
            "  Batch 8500/8989 - Loss: 0.1948\n",
            "  Batch 8600/8989 - Loss: 0.1948\n",
            "  Batch 8700/8989 - Loss: 0.1949\n",
            "  Batch 8800/8989 - Loss: 0.1950\n",
            "  Batch 8900/8989 - Loss: 0.1951\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:13\n",
            "  Loss: 0.1952\n",
            "  HR@10: 0.7358\n",
            "  NDCG@10: 0.4535\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 18/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1811\n",
            "  Batch 200/8989 - Loss: 0.1808\n",
            "  Batch 300/8989 - Loss: 0.1832\n",
            "  Batch 400/8989 - Loss: 0.1838\n",
            "  Batch 500/8989 - Loss: 0.1842\n",
            "  Batch 600/8989 - Loss: 0.1844\n",
            "  Batch 700/8989 - Loss: 0.1843\n",
            "  Batch 800/8989 - Loss: 0.1845\n",
            "  Batch 900/8989 - Loss: 0.1840\n",
            "  Batch 1000/8989 - Loss: 0.1846\n",
            "  Batch 1100/8989 - Loss: 0.1849\n",
            "  Batch 1200/8989 - Loss: 0.1851\n",
            "  Batch 1300/8989 - Loss: 0.1854\n",
            "  Batch 1400/8989 - Loss: 0.1854\n",
            "  Batch 1500/8989 - Loss: 0.1859\n",
            "  Batch 1600/8989 - Loss: 0.1862\n",
            "  Batch 1700/8989 - Loss: 0.1862\n",
            "  Batch 1800/8989 - Loss: 0.1862\n",
            "  Batch 1900/8989 - Loss: 0.1864\n",
            "  Batch 2000/8989 - Loss: 0.1866\n",
            "  Batch 2100/8989 - Loss: 0.1866\n",
            "  Batch 2200/8989 - Loss: 0.1866\n",
            "  Batch 2300/8989 - Loss: 0.1867\n",
            "  Batch 2400/8989 - Loss: 0.1869\n",
            "  Batch 2500/8989 - Loss: 0.1871\n",
            "  Batch 2600/8989 - Loss: 0.1873\n",
            "  Batch 2700/8989 - Loss: 0.1875\n",
            "  Batch 2800/8989 - Loss: 0.1876\n",
            "  Batch 2900/8989 - Loss: 0.1878\n",
            "  Batch 3000/8989 - Loss: 0.1879\n",
            "  Batch 3100/8989 - Loss: 0.1880\n",
            "  Batch 3200/8989 - Loss: 0.1882\n",
            "  Batch 3300/8989 - Loss: 0.1883\n",
            "  Batch 3400/8989 - Loss: 0.1883\n",
            "  Batch 3500/8989 - Loss: 0.1885\n",
            "  Batch 3600/8989 - Loss: 0.1886\n",
            "  Batch 3700/8989 - Loss: 0.1887\n",
            "  Batch 3800/8989 - Loss: 0.1889\n",
            "  Batch 3900/8989 - Loss: 0.1889\n",
            "  Batch 4000/8989 - Loss: 0.1890\n",
            "  Batch 4100/8989 - Loss: 0.1890\n",
            "  Batch 4200/8989 - Loss: 0.1891\n",
            "  Batch 4300/8989 - Loss: 0.1891\n",
            "  Batch 4400/8989 - Loss: 0.1892\n",
            "  Batch 4500/8989 - Loss: 0.1893\n",
            "  Batch 4600/8989 - Loss: 0.1894\n",
            "  Batch 4700/8989 - Loss: 0.1895\n",
            "  Batch 4800/8989 - Loss: 0.1896\n",
            "  Batch 4900/8989 - Loss: 0.1897\n",
            "  Batch 5000/8989 - Loss: 0.1899\n",
            "  Batch 5100/8989 - Loss: 0.1900\n",
            "  Batch 5200/8989 - Loss: 0.1901\n",
            "  Batch 5300/8989 - Loss: 0.1902\n",
            "  Batch 5400/8989 - Loss: 0.1903\n",
            "  Batch 5500/8989 - Loss: 0.1904\n",
            "  Batch 5600/8989 - Loss: 0.1905\n",
            "  Batch 5700/8989 - Loss: 0.1905\n",
            "  Batch 5800/8989 - Loss: 0.1906\n",
            "  Batch 5900/8989 - Loss: 0.1907\n",
            "  Batch 6000/8989 - Loss: 0.1908\n",
            "  Batch 6100/8989 - Loss: 0.1909\n",
            "  Batch 6200/8989 - Loss: 0.1910\n",
            "  Batch 6300/8989 - Loss: 0.1911\n",
            "  Batch 6400/8989 - Loss: 0.1912\n",
            "  Batch 6500/8989 - Loss: 0.1914\n",
            "  Batch 6600/8989 - Loss: 0.1915\n",
            "  Batch 6700/8989 - Loss: 0.1915\n",
            "  Batch 6800/8989 - Loss: 0.1917\n",
            "  Batch 6900/8989 - Loss: 0.1918\n",
            "  Batch 7000/8989 - Loss: 0.1919\n",
            "  Batch 7100/8989 - Loss: 0.1920\n",
            "  Batch 7200/8989 - Loss: 0.1920\n",
            "  Batch 7300/8989 - Loss: 0.1920\n",
            "  Batch 7400/8989 - Loss: 0.1921\n",
            "  Batch 7500/8989 - Loss: 0.1922\n",
            "  Batch 7600/8989 - Loss: 0.1922\n",
            "  Batch 7700/8989 - Loss: 0.1923\n",
            "  Batch 7800/8989 - Loss: 0.1923\n",
            "  Batch 7900/8989 - Loss: 0.1925\n",
            "  Batch 8000/8989 - Loss: 0.1925\n",
            "  Batch 8100/8989 - Loss: 0.1927\n",
            "  Batch 8200/8989 - Loss: 0.1928\n",
            "  Batch 8300/8989 - Loss: 0.1929\n",
            "  Batch 8400/8989 - Loss: 0.1929\n",
            "  Batch 8500/8989 - Loss: 0.1930\n",
            "  Batch 8600/8989 - Loss: 0.1931\n",
            "  Batch 8700/8989 - Loss: 0.1933\n",
            "  Batch 8800/8989 - Loss: 0.1934\n",
            "  Batch 8900/8989 - Loss: 0.1935\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:15\n",
            "  Loss: 0.1937\n",
            "  HR@10: 0.7357\n",
            "  NDCG@10: 0.4525\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 19/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1802\n",
            "  Batch 200/8989 - Loss: 0.1795\n",
            "  Batch 300/8989 - Loss: 0.1800\n",
            "  Batch 400/8989 - Loss: 0.1792\n",
            "  Batch 500/8989 - Loss: 0.1799\n",
            "  Batch 600/8989 - Loss: 0.1805\n",
            "  Batch 700/8989 - Loss: 0.1810\n",
            "  Batch 800/8989 - Loss: 0.1817\n",
            "  Batch 900/8989 - Loss: 0.1818\n",
            "  Batch 1000/8989 - Loss: 0.1818\n",
            "  Batch 1100/8989 - Loss: 0.1822\n",
            "  Batch 1200/8989 - Loss: 0.1822\n",
            "  Batch 1300/8989 - Loss: 0.1827\n",
            "  Batch 1400/8989 - Loss: 0.1830\n",
            "  Batch 1500/8989 - Loss: 0.1834\n",
            "  Batch 1600/8989 - Loss: 0.1838\n",
            "  Batch 1700/8989 - Loss: 0.1841\n",
            "  Batch 1800/8989 - Loss: 0.1842\n",
            "  Batch 1900/8989 - Loss: 0.1844\n",
            "  Batch 2000/8989 - Loss: 0.1845\n",
            "  Batch 2100/8989 - Loss: 0.1848\n",
            "  Batch 2200/8989 - Loss: 0.1851\n",
            "  Batch 2300/8989 - Loss: 0.1853\n",
            "  Batch 2400/8989 - Loss: 0.1854\n",
            "  Batch 2500/8989 - Loss: 0.1854\n",
            "  Batch 2600/8989 - Loss: 0.1855\n",
            "  Batch 2700/8989 - Loss: 0.1854\n",
            "  Batch 2800/8989 - Loss: 0.1855\n",
            "  Batch 2900/8989 - Loss: 0.1855\n",
            "  Batch 3000/8989 - Loss: 0.1857\n",
            "  Batch 3100/8989 - Loss: 0.1857\n",
            "  Batch 3200/8989 - Loss: 0.1859\n",
            "  Batch 3300/8989 - Loss: 0.1859\n",
            "  Batch 3400/8989 - Loss: 0.1860\n",
            "  Batch 3500/8989 - Loss: 0.1861\n",
            "  Batch 3600/8989 - Loss: 0.1862\n",
            "  Batch 3700/8989 - Loss: 0.1862\n",
            "  Batch 3800/8989 - Loss: 0.1865\n",
            "  Batch 3900/8989 - Loss: 0.1866\n",
            "  Batch 4000/8989 - Loss: 0.1867\n",
            "  Batch 4100/8989 - Loss: 0.1869\n",
            "  Batch 4200/8989 - Loss: 0.1869\n",
            "  Batch 4300/8989 - Loss: 0.1871\n",
            "  Batch 4400/8989 - Loss: 0.1872\n",
            "  Batch 4500/8989 - Loss: 0.1874\n",
            "  Batch 4600/8989 - Loss: 0.1876\n",
            "  Batch 4700/8989 - Loss: 0.1878\n",
            "  Batch 4800/8989 - Loss: 0.1879\n",
            "  Batch 4900/8989 - Loss: 0.1881\n",
            "  Batch 5000/8989 - Loss: 0.1881\n",
            "  Batch 5100/8989 - Loss: 0.1882\n",
            "  Batch 5200/8989 - Loss: 0.1883\n",
            "  Batch 5300/8989 - Loss: 0.1884\n",
            "  Batch 5400/8989 - Loss: 0.1885\n",
            "  Batch 5500/8989 - Loss: 0.1886\n",
            "  Batch 5600/8989 - Loss: 0.1888\n",
            "  Batch 5700/8989 - Loss: 0.1889\n",
            "  Batch 5800/8989 - Loss: 0.1890\n",
            "  Batch 5900/8989 - Loss: 0.1891\n",
            "  Batch 6000/8989 - Loss: 0.1891\n",
            "  Batch 6100/8989 - Loss: 0.1893\n",
            "  Batch 6200/8989 - Loss: 0.1893\n",
            "  Batch 6300/8989 - Loss: 0.1895\n",
            "  Batch 6400/8989 - Loss: 0.1896\n",
            "  Batch 6500/8989 - Loss: 0.1897\n",
            "  Batch 6600/8989 - Loss: 0.1898\n",
            "  Batch 6700/8989 - Loss: 0.1900\n",
            "  Batch 6800/8989 - Loss: 0.1900\n",
            "  Batch 6900/8989 - Loss: 0.1901\n",
            "  Batch 7000/8989 - Loss: 0.1901\n",
            "  Batch 7100/8989 - Loss: 0.1902\n",
            "  Batch 7200/8989 - Loss: 0.1903\n",
            "  Batch 7300/8989 - Loss: 0.1903\n",
            "  Batch 7400/8989 - Loss: 0.1905\n",
            "  Batch 7500/8989 - Loss: 0.1906\n",
            "  Batch 7600/8989 - Loss: 0.1907\n",
            "  Batch 7700/8989 - Loss: 0.1908\n",
            "  Batch 7800/8989 - Loss: 0.1909\n",
            "  Batch 7900/8989 - Loss: 0.1910\n",
            "  Batch 8000/8989 - Loss: 0.1910\n",
            "  Batch 8100/8989 - Loss: 0.1911\n",
            "  Batch 8200/8989 - Loss: 0.1912\n",
            "  Batch 8300/8989 - Loss: 0.1913\n",
            "  Batch 8400/8989 - Loss: 0.1914\n",
            "  Batch 8500/8989 - Loss: 0.1915\n",
            "  Batch 8600/8989 - Loss: 0.1916\n",
            "  Batch 8700/8989 - Loss: 0.1917\n",
            "  Batch 8800/8989 - Loss: 0.1918\n",
            "  Batch 8900/8989 - Loss: 0.1918\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:12\n",
            "  Loss: 0.1919\n",
            "  HR@10: 0.7354\n",
            "  NDCG@10: 0.4516\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Epoch 20/20\n",
            "----------------------------------------------------------------------\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "  Batch 100/8989 - Loss: 0.1803\n",
            "  Batch 200/8989 - Loss: 0.1779\n",
            "  Batch 300/8989 - Loss: 0.1779\n",
            "  Batch 400/8989 - Loss: 0.1795\n",
            "  Batch 500/8989 - Loss: 0.1790\n",
            "  Batch 600/8989 - Loss: 0.1800\n",
            "  Batch 700/8989 - Loss: 0.1801\n",
            "  Batch 800/8989 - Loss: 0.1804\n",
            "  Batch 900/8989 - Loss: 0.1810\n",
            "  Batch 1000/8989 - Loss: 0.1812\n",
            "  Batch 1100/8989 - Loss: 0.1816\n",
            "  Batch 1200/8989 - Loss: 0.1814\n",
            "  Batch 1300/8989 - Loss: 0.1815\n",
            "  Batch 1400/8989 - Loss: 0.1818\n",
            "  Batch 1500/8989 - Loss: 0.1821\n",
            "  Batch 1600/8989 - Loss: 0.1822\n",
            "  Batch 1700/8989 - Loss: 0.1824\n",
            "  Batch 1800/8989 - Loss: 0.1827\n",
            "  Batch 1900/8989 - Loss: 0.1828\n",
            "  Batch 2000/8989 - Loss: 0.1832\n",
            "  Batch 2100/8989 - Loss: 0.1832\n",
            "  Batch 2200/8989 - Loss: 0.1833\n",
            "  Batch 2300/8989 - Loss: 0.1833\n",
            "  Batch 2400/8989 - Loss: 0.1834\n",
            "  Batch 2500/8989 - Loss: 0.1836\n",
            "  Batch 2600/8989 - Loss: 0.1839\n",
            "  Batch 2700/8989 - Loss: 0.1842\n",
            "  Batch 2800/8989 - Loss: 0.1842\n",
            "  Batch 2900/8989 - Loss: 0.1843\n",
            "  Batch 3000/8989 - Loss: 0.1845\n",
            "  Batch 3100/8989 - Loss: 0.1845\n",
            "  Batch 3200/8989 - Loss: 0.1846\n",
            "  Batch 3300/8989 - Loss: 0.1848\n",
            "  Batch 3400/8989 - Loss: 0.1849\n",
            "  Batch 3500/8989 - Loss: 0.1851\n",
            "  Batch 3600/8989 - Loss: 0.1852\n",
            "  Batch 3700/8989 - Loss: 0.1854\n",
            "  Batch 3800/8989 - Loss: 0.1855\n",
            "  Batch 3900/8989 - Loss: 0.1856\n",
            "  Batch 4000/8989 - Loss: 0.1857\n",
            "  Batch 4100/8989 - Loss: 0.1859\n",
            "  Batch 4200/8989 - Loss: 0.1860\n",
            "  Batch 4300/8989 - Loss: 0.1860\n",
            "  Batch 4400/8989 - Loss: 0.1863\n",
            "  Batch 4500/8989 - Loss: 0.1864\n",
            "  Batch 4600/8989 - Loss: 0.1864\n",
            "  Batch 4700/8989 - Loss: 0.1866\n",
            "  Batch 4800/8989 - Loss: 0.1867\n",
            "  Batch 4900/8989 - Loss: 0.1869\n",
            "  Batch 5000/8989 - Loss: 0.1870\n",
            "  Batch 5100/8989 - Loss: 0.1871\n",
            "  Batch 5200/8989 - Loss: 0.1871\n",
            "  Batch 5300/8989 - Loss: 0.1873\n",
            "  Batch 5400/8989 - Loss: 0.1874\n",
            "  Batch 5500/8989 - Loss: 0.1875\n",
            "  Batch 5600/8989 - Loss: 0.1877\n",
            "  Batch 5700/8989 - Loss: 0.1878\n",
            "  Batch 5800/8989 - Loss: 0.1879\n",
            "  Batch 5900/8989 - Loss: 0.1881\n",
            "  Batch 6000/8989 - Loss: 0.1882\n",
            "  Batch 6100/8989 - Loss: 0.1884\n",
            "  Batch 6200/8989 - Loss: 0.1885\n",
            "  Batch 6300/8989 - Loss: 0.1886\n",
            "  Batch 6400/8989 - Loss: 0.1887\n",
            "  Batch 6500/8989 - Loss: 0.1889\n",
            "  Batch 6600/8989 - Loss: 0.1890\n",
            "  Batch 6700/8989 - Loss: 0.1891\n",
            "  Batch 6800/8989 - Loss: 0.1892\n",
            "  Batch 6900/8989 - Loss: 0.1893\n",
            "  Batch 7000/8989 - Loss: 0.1894\n",
            "  Batch 7100/8989 - Loss: 0.1896\n",
            "  Batch 7200/8989 - Loss: 0.1896\n",
            "  Batch 7300/8989 - Loss: 0.1897\n",
            "  Batch 7400/8989 - Loss: 0.1897\n",
            "  Batch 7500/8989 - Loss: 0.1898\n",
            "  Batch 7600/8989 - Loss: 0.1899\n",
            "  Batch 7700/8989 - Loss: 0.1899\n",
            "  Batch 7800/8989 - Loss: 0.1900\n",
            "  Batch 7900/8989 - Loss: 0.1900\n",
            "  Batch 8000/8989 - Loss: 0.1901\n",
            "  Batch 8100/8989 - Loss: 0.1901\n",
            "  Batch 8200/8989 - Loss: 0.1901\n",
            "  Batch 8300/8989 - Loss: 0.1902\n",
            "  Batch 8400/8989 - Loss: 0.1904\n",
            "  Batch 8500/8989 - Loss: 0.1905\n",
            "  Batch 8600/8989 - Loss: 0.1906\n",
            "  Batch 8700/8989 - Loss: 0.1907\n",
            "  Batch 8800/8989 - Loss: 0.1908\n",
            "  Batch 8900/8989 - Loss: 0.1908\n",
            "  Evaluating on test set...\n",
            "  Time: 00:01:11\n",
            "  Loss: 0.1908\n",
            "  HR@10: 0.7364\n",
            "  NDCG@10: 0.4531\n",
            "  (Best: HR@10: 0.7390 at epoch 4)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "======================================================================\n",
            "TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Best model at epoch 4:\n",
            "  HR@10: 0.7390\n",
            "  NDCG@10: 0.4572\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.2 TRAINING LOOP\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.2: Starting Training\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Training for {epochs} epochs...\")\n",
        "print(f\"Model: {model_name}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Track best performance\n",
        "best_hr = 0.0\n",
        "best_ndcg = 0.0\n",
        "best_epoch = 0\n",
        "training_history = {\n",
        "    'epoch': [],\n",
        "    'hr': [],\n",
        "    'ndcg': [],\n",
        "    'time': []\n",
        "}\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # ========================================================================\n",
        "    # TRAINING PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to training mode\n",
        "    # This enables dropout and other training-specific behaviors\n",
        "    ncf_model.train()\n",
        "    \n",
        "    # Start timer for this epoch\n",
        "    epoch_start_time = time.time()\n",
        "    \n",
        "    # Generate negative samples for this epoch\n",
        "    # Important: We generate fresh negatives each epoch for better learning\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "    print(\"-\" * 70)\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    # Track loss for this epoch\n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        # Move data to device (GPU or CPU)\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        # ================================================================\n",
        "        # FORWARD PASS\n",
        "        # ================================================================\n",
        "        # Clear gradients from previous iteration\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Get model predictions (raw scores/logits)\n",
        "        prediction = ncf_model(user, item)  # [batch_size]\n",
        "        \n",
        "        # ================================================================\n",
        "        # COMPUTE LOSS\n",
        "        # ================================================================\n",
        "        # Compare predictions with true labels (1 for positive, 0 for negative)\n",
        "        loss = loss_function(prediction, label)\n",
        "        \n",
        "        # ================================================================\n",
        "        # BACKWARD PASS\n",
        "        # ================================================================\n",
        "        # Compute gradients\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track loss\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Print progress every 100 batches\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Calculate average loss for this epoch\n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # ========================================================================\n",
        "    # EVALUATION PHASE\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Set model to evaluation mode\n",
        "    # This disables dropout and uses deterministic behavior\n",
        "    ncf_model.eval()\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"  Evaluating on test set...\")\n",
        "    HR, NDCG = evaluate_metrics(ncf_model, test_loader, top_k, device)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    # Store history\n",
        "    training_history['epoch'].append(epoch + 1)\n",
        "    training_history['hr'].append(HR)\n",
        "    training_history['ndcg'].append(NDCG)\n",
        "    training_history['time'].append(elapsed_time)\n",
        "    \n",
        "    # Print epoch results\n",
        "    print(f\"  Time: {time_str}\")\n",
        "    print(f\"  Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  HR@{top_k}: {HR:.4f}\")\n",
        "    print(f\"  NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    # ========================================================================\n",
        "    # SAVE BEST MODEL\n",
        "    # ========================================================================\n",
        "    \n",
        "    # Check if this is the best model so far\n",
        "    if HR > best_hr:\n",
        "        best_hr = HR\n",
        "        best_ndcg = NDCG\n",
        "        best_epoch = epoch + 1\n",
        "        \n",
        "        print(f\"  ✓ New best model! (HR@{top_k}: {HR:.4f})\")\n",
        "        \n",
        "        # Save model if enabled\n",
        "        if save_model:\n",
        "            if not os.path.exists(model_path):\n",
        "                os.makedirs(model_path)\n",
        "            \n",
        "            model_filename = os.path.join(model_path, f'{model_name}.pth')\n",
        "            torch.save(ncf_model, model_filename)\n",
        "            print(f\"  ✓ Model saved to {model_filename}\")\n",
        "    else:\n",
        "        print(f\"  (Best: HR@{top_k}: {best_hr:.4f} at epoch {best_epoch})\")\n",
        "    \n",
        "    print(\"-\" * 70)\n",
        "\n",
        "# ========================================================================\n",
        "# TRAINING COMPLETE\n",
        "# ========================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Best model at epoch {best_epoch}:\")\n",
        "print(f\"  HR@{top_k}: {best_hr:.4f}\")\n",
        "print(f\"  NDCG@{top_k}: {best_ndcg:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store the NeuMF-end model for later comparison\n",
        "ncf_model_neumf_end = ncf_model\n",
        "best_hr_neumf_end = best_hr\n",
        "best_ndcg_neumf_end = best_ndcg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Step 7.5: Train Models Separately (GMF, MLP, NeuMF-pre)\n",
        "\n",
        "In this step, we'll train each model architecture separately:\n",
        "1. **Train GMF model** - Generalized Matrix Factorization only\n",
        "2. **Train MLP model** - Multi-Layer Perceptron only  \n",
        "3. **Train NeuMF-pre** - Neural Matrix Factorization using pre-trained GMF and MLP weights\n",
        "\n",
        "This approach (NeuMF-pre) typically gives the best performance as it leverages pre-trained components.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "STEP 7.5: Training Models Separately\n",
            "======================================================================\n",
            "This will train:\n",
            "  1. GMF model (Generalized Matrix Factorization)\n",
            "  2. MLP model (Multi-Layer Perceptron)\n",
            "  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STEP 7.5.1: Training GMF Model\n",
            "======================================================================\n",
            "Training GMF for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:00:43 - Loss: 0.3576 - HR@10: 0.6605 - NDCG@10: 0.3890\n",
            "  ✓ Saved best GMF model (HR@10: 0.6605)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:00:43 - Loss: 0.2932 - HR@10: 0.7070 - NDCG@10: 0.4279\n",
            "  ✓ Saved best GMF model (HR@10: 0.7070)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:00:43 - Loss: 0.2742 - HR@10: 0.7301 - NDCG@10: 0.4476\n",
            "  ✓ Saved best GMF model (HR@10: 0.7301)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:00:43 - Loss: 0.2622 - HR@10: 0.7391 - NDCG@10: 0.4565\n",
            "  ✓ Saved best GMF model (HR@10: 0.7391)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:00:42 - Loss: 0.2534 - HR@10: 0.7451 - NDCG@10: 0.4606\n",
            "  ✓ Saved best GMF model (HR@10: 0.7451)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:00:43 - Loss: 0.2473 - HR@10: 0.7484 - NDCG@10: 0.4637\n",
            "  ✓ Saved best GMF model (HR@10: 0.7484)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:00:42 - Loss: 0.2417 - HR@10: 0.7500 - NDCG@10: 0.4661\n",
            "  ✓ Saved best GMF model (HR@10: 0.7500)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:00:44 - Loss: 0.2364 - HR@10: 0.7517 - NDCG@10: 0.4666\n",
            "  ✓ Saved best GMF model (HR@10: 0.7517)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:00:44 - Loss: 0.2317 - HR@10: 0.7511 - NDCG@10: 0.4666\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:00:44 - Loss: 0.2273 - HR@10: 0.7499 - NDCG@10: 0.4660\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:00:42 - Loss: 0.2236 - HR@10: 0.7496 - NDCG@10: 0.4655\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:00:42 - Loss: 0.2202 - HR@10: 0.7481 - NDCG@10: 0.4643\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:00:42 - Loss: 0.2176 - HR@10: 0.7482 - NDCG@10: 0.4642\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:00:42 - Loss: 0.2154 - HR@10: 0.7473 - NDCG@10: 0.4636\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:00:42 - Loss: 0.2136 - HR@10: 0.7467 - NDCG@10: 0.4629\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:42 - Loss: 0.2120 - HR@10: 0.7470 - NDCG@10: 0.4633\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:00:42 - Loss: 0.2102 - HR@10: 0.7471 - NDCG@10: 0.4629\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:41 - Loss: 0.2093 - HR@10: 0.7472 - NDCG@10: 0.4637\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:43 - Loss: 0.2080 - HR@10: 0.7470 - NDCG@10: 0.4629\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:41 - Loss: 0.2072 - HR@10: 0.7465 - NDCG@10: 0.4633\n",
            "\n",
            "✓ GMF Training Complete!\n",
            "  Best epoch: 8\n",
            "  Best HR@10: 0.7517\n",
            "  Best NDCG@10: 0.4666\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# STEP 7.5: TRAIN MODELS SEPARATELY (GMF, MLP, NeuMF-pre)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"STEP 7.5: Training Models Separately\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will train:\")\n",
        "print(\"  1. GMF model (Generalized Matrix Factorization)\")\n",
        "print(\"  2. MLP model (Multi-Layer Perceptron)\")\n",
        "print(\"  3. NeuMF-pre model (using pre-trained GMF and MLP weights)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Store trained models\n",
        "trained_models = {}\n",
        "\n",
        "# ============================================================================\n",
        "# 7.5.1 TRAIN GMF MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.1: Training GMF Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create GMF model\n",
        "gmf_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='GMF',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    gmf_model = gmf_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "gmf_optimizer = optim.Adam(gmf_model.parameters(), lr=learning_rate)\n",
        "gmf_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for GMF\n",
        "print(f\"Training GMF for {epochs} epochs...\")\n",
        "best_hr_gmf = 0.0\n",
        "best_ndcg_gmf = 0.0\n",
        "best_epoch_gmf = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    gmf_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        gmf_optimizer.zero_grad()\n",
        "        prediction = gmf_model(user, item)\n",
        "        loss = gmf_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        gmf_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    gmf_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(gmf_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_gmf:\n",
        "        best_hr_gmf = HR\n",
        "        best_ndcg_gmf = NDCG\n",
        "        best_epoch_gmf = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(gmf_model, GMF_model_path)\n",
        "            print(f\"  ✓ Saved best GMF model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ GMF Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_gmf}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_gmf:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_gmf:.4f}\")\n",
        "\n",
        "trained_models['GMF'] = {\n",
        "    'model': gmf_model,\n",
        "    'hr': best_hr_gmf,\n",
        "    'ndcg': best_ndcg_gmf,\n",
        "    'epoch': best_epoch_gmf\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.2: Training MLP Model\n",
            "======================================================================\n",
            "Training MLP for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:01:03 - Loss: 0.3503 - HR@10: 0.5682 - NDCG@10: 0.3265\n",
            "  ✓ Saved best MLP model (HR@10: 0.5682)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:01:06 - Loss: 0.3250 - HR@10: 0.6417 - NDCG@10: 0.3753\n",
            "  ✓ Saved best MLP model (HR@10: 0.6417)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:01:07 - Loss: 0.3077 - HR@10: 0.6613 - NDCG@10: 0.3891\n",
            "  ✓ Saved best MLP model (HR@10: 0.6613)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:01:03 - Loss: 0.2976 - HR@10: 0.6891 - NDCG@10: 0.4105\n",
            "  ✓ Saved best MLP model (HR@10: 0.6891)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:01:06 - Loss: 0.2862 - HR@10: 0.7079 - NDCG@10: 0.4266\n",
            "  ✓ Saved best MLP model (HR@10: 0.7079)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:01:05 - Loss: 0.2781 - HR@10: 0.7172 - NDCG@10: 0.4361\n",
            "  ✓ Saved best MLP model (HR@10: 0.7172)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:01:05 - Loss: 0.2731 - HR@10: 0.7242 - NDCG@10: 0.4419\n",
            "  ✓ Saved best MLP model (HR@10: 0.7242)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:01:04 - Loss: 0.2685 - HR@10: 0.7299 - NDCG@10: 0.4477\n",
            "  ✓ Saved best MLP model (HR@10: 0.7299)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:01:04 - Loss: 0.2644 - HR@10: 0.7344 - NDCG@10: 0.4519\n",
            "  ✓ Saved best MLP model (HR@10: 0.7344)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:01:05 - Loss: 0.2608 - HR@10: 0.7391 - NDCG@10: 0.4561\n",
            "  ✓ Saved best MLP model (HR@10: 0.7391)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:01:05 - Loss: 0.2582 - HR@10: 0.7412 - NDCG@10: 0.4575\n",
            "  ✓ Saved best MLP model (HR@10: 0.7412)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:01:04 - Loss: 0.2556 - HR@10: 0.7439 - NDCG@10: 0.4599\n",
            "  ✓ Saved best MLP model (HR@10: 0.7439)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:01:06 - Loss: 0.2531 - HR@10: 0.7459 - NDCG@10: 0.4624\n",
            "  ✓ Saved best MLP model (HR@10: 0.7459)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:01:05 - Loss: 0.2512 - HR@10: 0.7465 - NDCG@10: 0.4635\n",
            "  ✓ Saved best MLP model (HR@10: 0.7465)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:01:05 - Loss: 0.2488 - HR@10: 0.7493 - NDCG@10: 0.4657\n",
            "  ✓ Saved best MLP model (HR@10: 0.7493)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:01:06 - Loss: 0.2479 - HR@10: 0.7487 - NDCG@10: 0.4655\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:01:07 - Loss: 0.2451 - HR@10: 0.7511 - NDCG@10: 0.4664\n",
            "  ✓ Saved best MLP model (HR@10: 0.7511)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:01:05 - Loss: 0.2438 - HR@10: 0.7524 - NDCG@10: 0.4678\n",
            "  ✓ Saved best MLP model (HR@10: 0.7524)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:01:05 - Loss: 0.2421 - HR@10: 0.7539 - NDCG@10: 0.4696\n",
            "  ✓ Saved best MLP model (HR@10: 0.7539)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:01:03 - Loss: 0.2406 - HR@10: 0.7534 - NDCG@10: 0.4698\n",
            "\n",
            "✓ MLP Training Complete!\n",
            "  Best epoch: 19\n",
            "  Best HR@10: 0.7539\n",
            "  Best NDCG@10: 0.4696\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.2 TRAIN MLP MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.2: Training MLP Model\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create MLP model\n",
        "mlp_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='MLP',\n",
        "    GMF_model=None,\n",
        "    MLP_model=None\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    mlp_model = mlp_model.cuda()\n",
        "\n",
        "# Setup optimizer and loss\n",
        "mlp_optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
        "mlp_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for MLP\n",
        "print(f\"Training MLP for {epochs} epochs...\")\n",
        "best_hr_mlp = 0.0\n",
        "best_ndcg_mlp = 0.0\n",
        "best_epoch_mlp = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    mlp_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        mlp_optimizer.zero_grad()\n",
        "        prediction = mlp_model(user, item)\n",
        "        loss = mlp_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        mlp_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    mlp_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(mlp_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_mlp:\n",
        "        best_hr_mlp = HR\n",
        "        best_ndcg_mlp = NDCG\n",
        "        best_epoch_mlp = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(mlp_model, MLP_model_path)\n",
        "            print(f\"  ✓ Saved best MLP model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ MLP Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_mlp}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_mlp:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_mlp:.4f}\")\n",
        "\n",
        "trained_models['MLP'] = {\n",
        "    'model': mlp_model,\n",
        "    'hr': best_hr_mlp,\n",
        "    'ndcg': best_ndcg_mlp,\n",
        "    'epoch': best_epoch_mlp\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 7.5.3: Training NeuMF-pre Model\n",
            "======================================================================\n",
            "Creating NeuMF model with pre-trained GMF and MLP weights...\n",
            "Training NeuMF-pre for 20 epochs...\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 1/20 - Time: 00:01:00 - Loss: 0.2010 - HR@10: 0.7712 - NDCG@10: 0.4868\n",
            "  ✓ Saved best NeuMF-pre model (HR@10: 0.7712)\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 2/20 - Time: 00:01:01 - Loss: 0.2006 - HR@10: 0.7704 - NDCG@10: 0.4861\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 3/20 - Time: 00:01:01 - Loss: 0.1997 - HR@10: 0.7706 - NDCG@10: 0.4862\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 4/20 - Time: 00:01:00 - Loss: 0.1995 - HR@10: 0.7703 - NDCG@10: 0.4855\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 5/20 - Time: 00:01:01 - Loss: 0.1988 - HR@10: 0.7700 - NDCG@10: 0.4852\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 6/20 - Time: 00:01:03 - Loss: 0.1987 - HR@10: 0.7692 - NDCG@10: 0.4846\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 7/20 - Time: 00:01:00 - Loss: 0.1985 - HR@10: 0.7689 - NDCG@10: 0.4845\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 8/20 - Time: 00:01:00 - Loss: 0.1980 - HR@10: 0.7686 - NDCG@10: 0.4842\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 9/20 - Time: 00:01:01 - Loss: 0.1980 - HR@10: 0.7684 - NDCG@10: 0.4840\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 10/20 - Time: 00:01:00 - Loss: 0.1979 - HR@10: 0.7682 - NDCG@10: 0.4835\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 11/20 - Time: 00:01:01 - Loss: 0.1978 - HR@10: 0.7683 - NDCG@10: 0.4838\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 12/20 - Time: 00:01:01 - Loss: 0.1974 - HR@10: 0.7683 - NDCG@10: 0.4836\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 13/20 - Time: 00:01:01 - Loss: 0.1976 - HR@10: 0.7680 - NDCG@10: 0.4834\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 14/20 - Time: 00:01:00 - Loss: 0.1971 - HR@10: 0.7678 - NDCG@10: 0.4832\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 15/20 - Time: 00:01:03 - Loss: 0.1973 - HR@10: 0.7676 - NDCG@10: 0.4827\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 16/20 - Time: 00:00:59 - Loss: 0.1969 - HR@10: 0.7673 - NDCG@10: 0.4826\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 17/20 - Time: 00:01:00 - Loss: 0.1969 - HR@10: 0.7673 - NDCG@10: 0.4824\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 18/20 - Time: 00:00:59 - Loss: 0.1967 - HR@10: 0.7670 - NDCG@10: 0.4823\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 19/20 - Time: 00:00:59 - Loss: 0.1971 - HR@10: 0.7669 - NDCG@10: 0.4822\n",
            "✓ Generated 1840900 negative samples\n",
            "  - Total samples (positives + negatives): 2301125\n",
            "Epoch 20/20 - Time: 00:00:58 - Loss: 0.1968 - HR@10: 0.7670 - NDCG@10: 0.4823\n",
            "\n",
            "✓ NeuMF-pre Training Complete!\n",
            "  Best epoch: 1\n",
            "  Best HR@10: 0.7712\n",
            "  Best NDCG@10: 0.4868\n",
            "\n",
            "======================================================================\n",
            "MODEL COMPARISON SUMMARY\n",
            "======================================================================\n",
            "Model           HR@{top_k}   NDCG@{top_k} Best Epoch  \n",
            "----------------------------------------------------------------------\n",
            "GMF             0.7517       0.4666       8           \n",
            "MLP             0.7539       0.4696       19          \n",
            "NeuMF-end       0.7390       0.4572       4           \n",
            "NeuMF-pre       0.7712       0.4868       1           \n",
            "======================================================================\n",
            "\n",
            "🏆 Best Model: NeuMF-pre\n",
            "   HR@10: 0.7712\n",
            "   NDCG@10: 0.4868\n",
            "\n",
            "✓ Using NeuMF-pre model for recommendations\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# 7.5.3 TRAIN NeuMF-pre MODEL (Using Pre-trained GMF and MLP)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"STEP 7.5.3: Training NeuMF-pre Model\")\n",
        "print(\"=\" * 70)\n",
        "print(\"Creating NeuMF model with pre-trained GMF and MLP weights...\")\n",
        "\n",
        "# Create NeuMF-pre model using pre-trained GMF and MLP\n",
        "neumf_pre_model = NCF(\n",
        "    user_num=user_num,\n",
        "    item_num=item_num,\n",
        "    factor_num=factor_num,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout_rate,\n",
        "    model_name='NeuMF-pre',\n",
        "    GMF_model=gmf_model,\n",
        "    MLP_model=mlp_model\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    neumf_pre_model = neumf_pre_model.cuda()\n",
        "\n",
        "# Setup optimizer (SGD is typically used for NeuMF-pre)\n",
        "neumf_pre_optimizer = optim.SGD(neumf_pre_model.parameters(), lr=learning_rate)\n",
        "neumf_pre_loss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop for NeuMF-pre\n",
        "print(f\"Training NeuMF-pre for {epochs} epochs...\")\n",
        "best_hr_neumf_pre = 0.0\n",
        "best_ndcg_neumf_pre = 0.0\n",
        "best_epoch_neumf_pre = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    neumf_pre_model.train()\n",
        "    epoch_start_time = time.time()\n",
        "    train_dataset.ng_sample()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_idx, (user, item, label) in enumerate(train_loader):\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            user = user.cuda()\n",
        "            item = item.cuda()\n",
        "            label = label.float().cuda()\n",
        "        else:\n",
        "            user = user\n",
        "            item = item\n",
        "            label = label.float()\n",
        "        \n",
        "        neumf_pre_optimizer.zero_grad()\n",
        "        prediction = neumf_pre_model(user, item)\n",
        "        loss = neumf_pre_loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        neumf_pre_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
        "    \n",
        "    # Evaluate\n",
        "    neumf_pre_model.eval()\n",
        "    HR, NDCG = evaluate_metrics(neumf_pre_model, test_loader, top_k, device)\n",
        "    \n",
        "    elapsed_time = time.time() - epoch_start_time\n",
        "    time_str = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Time: {time_str} - Loss: {avg_loss:.4f} - HR@{top_k}: {HR:.4f} - NDCG@{top_k}: {NDCG:.4f}\")\n",
        "    \n",
        "    if HR > best_hr_neumf_pre:\n",
        "        best_hr_neumf_pre = HR\n",
        "        best_ndcg_neumf_pre = NDCG\n",
        "        best_epoch_neumf_pre = epoch + 1\n",
        "        if save_model:\n",
        "            torch.save(neumf_pre_model, NeuMF_model_path)\n",
        "            print(f\"  ✓ Saved best NeuMF-pre model (HR@{top_k}: {HR:.4f})\")\n",
        "\n",
        "print(f\"\\n✓ NeuMF-pre Training Complete!\")\n",
        "print(f\"  Best epoch: {best_epoch_neumf_pre}\")\n",
        "print(f\"  Best HR@{top_k}: {best_hr_neumf_pre:.4f}\")\n",
        "print(f\"  Best NDCG@{top_k}: {best_ndcg_neumf_pre:.4f}\")\n",
        "\n",
        "trained_models['NeuMF-pre'] = {\n",
        "    'model': neumf_pre_model,\n",
        "    'hr': best_hr_neumf_pre,\n",
        "    'ndcg': best_ndcg_neumf_pre,\n",
        "    'epoch': best_epoch_neumf_pre\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# COMPARISON OF ALL MODELS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"MODEL COMPARISON SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'HR@{top_k}':<12} {'NDCG@{top_k}':<12} {'Best Epoch':<12}\")\n",
        "print(\"-\" * 70)\n",
        "print(f\"{'GMF':<15} {best_hr_gmf:<12.4f} {best_ndcg_gmf:<12.4f} {best_epoch_gmf:<12}\")\n",
        "print(f\"{'MLP':<15} {best_hr_mlp:<12.4f} {best_ndcg_mlp:<12.4f} {best_epoch_mlp:<12}\")\n",
        "print(f\"{'NeuMF-end':<15} {best_hr_neumf_end:<12.4f} {best_ndcg_neumf_end:<12.4f} {best_epoch:<12}\")\n",
        "print(f\"{'NeuMF-pre':<15} {best_hr_neumf_pre:<12.4f} {best_ndcg_neumf_pre:<12.4f} {best_epoch_neumf_pre:<12}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find best model\n",
        "all_results = [\n",
        "    ('GMF', best_hr_gmf, best_ndcg_gmf),\n",
        "    ('MLP', best_hr_mlp, best_ndcg_mlp),\n",
        "    ('NeuMF-end', best_hr_neumf_end, best_ndcg_neumf_end),\n",
        "    ('NeuMF-pre', best_hr_neumf_pre, best_ndcg_neumf_pre)\n",
        "]\n",
        "best_model_name, best_hr_overall, best_ndcg_overall = max(all_results, key=lambda x: x[1])\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   HR@{top_k}: {best_hr_overall:.4f}\")\n",
        "print(f\"   NDCG@{top_k}: {best_ndcg_overall:.4f}\")\n",
        "\n",
        "# Set the best model as the main model for recommendations\n",
        "if best_model_name == 'GMF':\n",
        "    ncf_model = gmf_model\n",
        "elif best_model_name == 'MLP':\n",
        "    ncf_model = mlp_model\n",
        "elif best_model_name == 'NeuMF-pre':\n",
        "    ncf_model = neumf_pre_model\n",
        "else:\n",
        "    ncf_model = ncf_model_neumf_end\n",
        "\n",
        "print(f\"\\n✓ Using {best_model_name} model for recommendations\")\n",
        "print(\"=\" * 70)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
