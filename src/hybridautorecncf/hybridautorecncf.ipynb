{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5314f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abbas/Documents/Codes/thesis/recommender/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Walk up the directory tree until we find 'src'\n",
    "path = current_dir\n",
    "src_path = None\n",
    "\n",
    "while True:\n",
    "    if os.path.basename(path) == \"src\":\n",
    "        src_path = path\n",
    "        break\n",
    "    parent = os.path.dirname(path)\n",
    "    if parent == path:  # reached filesystem root\n",
    "        break\n",
    "    path = parent\n",
    "\n",
    "# Add src to sys.path if found\n",
    "if src_path and src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from helpers.data_downloader import download_ml1m_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9d6aea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRecEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Linear(input_dim, latent_dim)\n",
    "        self.decoder = nn.Linear(latent_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = F.relu(self.encoder(x))\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bbb0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, latent_dim, mlp_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # GMF\n",
    "        self.gmf = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        # MLP\n",
    "        mlp_modules = []\n",
    "        input_dim = latent_dim * 2\n",
    "        for h in mlp_layers:\n",
    "            mlp_modules.append(nn.Linear(input_dim, h))\n",
    "            mlp_modules.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        self.mlp = nn.Sequential(*mlp_modules)\n",
    "\n",
    "        # Final prediction\n",
    "        self.output = nn.Linear(latent_dim + mlp_layers[-1], 1)\n",
    "\n",
    "    def forward(self, user_z, item_z):\n",
    "        gmf_out = self.gmf(user_z * item_z)\n",
    "        mlp_out = self.mlp(torch.cat([user_z, item_z], dim=1))\n",
    "        concat = torch.cat([gmf_out, mlp_out], dim=1)\n",
    "        return self.output(concat).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd3a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAutoRecNCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, latent_dim, mlp_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.user_autorec = AutoRecEncoder(num_items, latent_dim)\n",
    "        self.item_autorec = AutoRecEncoder(num_users, latent_dim)\n",
    "\n",
    "        self.ncf = NCF(latent_dim, mlp_layers)\n",
    "\n",
    "    def forward(self, user_vecs, item_vecs, user_ids, item_ids):\n",
    "        # AutoRec forward\n",
    "        # user_vecs: (batch_size, num_items) - each row is a user's rating vector\n",
    "        # item_vecs: (batch_size, num_users) - each row is an item's rating vector\n",
    "        user_recon, user_z = self.user_autorec(user_vecs)\n",
    "        item_recon, item_z = self.item_autorec(item_vecs)\n",
    "\n",
    "        # Each element in the batch corresponds to a (user, item) pair\n",
    "        # user_z[i] is the latent for the user in pair i\n",
    "        # item_z[i] is the latent for the item in pair i\n",
    "        # So we can use them directly without indexing\n",
    "        pred = self.ncf(user_z, item_z)\n",
    "        return pred, user_recon, item_recon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e01886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(pred, target, mask):\n",
    "    diff = (pred - target) * mask\n",
    "    return torch.sum(diff ** 2) / torch.sum(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd8078d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interaction_loss(pred, rating):\n",
    "    return F.mse_loss(pred, rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e422215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(pred, rating,\n",
    "               user_recon, user_vecs, user_mask,\n",
    "               item_recon, item_vecs, item_mask,\n",
    "               alpha=1.0, beta=1.0):\n",
    "    rec_u = reconstruction_loss(user_recon, user_vecs, user_mask)\n",
    "    rec_i = reconstruction_loss(item_recon, item_vecs, item_mask)\n",
    "    inter = interaction_loss(pred, rating)\n",
    "    return alpha * (rec_u + rec_i) + beta * inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6f2efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        (user_ids, item_ids, ratings,\n",
    "         user_vecs, item_vecs,\n",
    "         user_mask, item_mask) = batch\n",
    "\n",
    "        user_ids = user_ids.to(device).squeeze()\n",
    "        item_ids = item_ids.to(device).squeeze()\n",
    "        ratings = ratings.to(device).squeeze()\n",
    "        user_vecs = user_vecs.to(device)\n",
    "        item_vecs = item_vecs.to(device)\n",
    "        user_mask = user_mask.to(device)\n",
    "        item_mask = item_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred, user_recon, item_recon = model(\n",
    "            user_vecs, item_vecs, user_ids, item_ids\n",
    "        )\n",
    "\n",
    "        loss = total_loss(\n",
    "            pred, ratings,\n",
    "            user_recon, user_vecs, user_mask,\n",
    "            item_recon, item_vecs, item_mask\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "\n",
    "    return total / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91e02aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n",
      "/Users/abbas/Documents/Codes/thesis/recommender/src/../data\n",
      "\n",
      "Loading ratings data...\n",
      "======================================================================\n",
      "Loading MovieLens 1M Dataset\n",
      "======================================================================\n",
      "Data path: /Users/abbas/Documents/Codes/thesis/recommender/src/../data/ml-1m/ratings.dat\n",
      "✓ Successfully loaded 1,000,209 ratings\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'data')\n",
    "data_path = os.path.join(data_dir,'ml-1m', 'ratings.dat')\n",
    "print(data_path)\n",
    "print(data_dir)\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    download_ml1m_dataset(data_dir=data_dir)\n",
    "\n",
    "def load_ml_1m_data(data_path = data_path) -> pd.DataFrame:  \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Loading MovieLens 1M Dataset\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Data path: {data_path}\")\n",
    "    return pd.read_csv(\n",
    "        data_path,\n",
    "        sep='::',\n",
    "        header=None,\n",
    "        names=['user_id', 'item_id', 'rating', 'timestamp'],\n",
    "        engine='python',  # Explicitly use python engine to avoid warning\n",
    "        dtype={\n",
    "            'user_id': np.int32,\n",
    "            'item_id': np.int32,\n",
    "            'rating': np.float32,\n",
    "            'timestamp': np.int32\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Load ratings data with proper engine to avoid warnings\n",
    "print(\"\\nLoading ratings data...\")\n",
    "\n",
    "ratings_df = load_ml_1m_data()\n",
    "\n",
    "print(f\"✓ Successfully loaded {len(ratings_df):,} ratings\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42293273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "✓ Preprocessing complete!\n",
      "  - Users: 6040, Items: 3706\n",
      "  - Train interactions: 800,167\n",
      "  - Test interactions: 200,042\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: remap IDs and create train/test split\n",
    "def preprocess_data(ratings_df, test_size=0.2, random_state=42):\n",
    "    \"\"\"Preprocess data: remap IDs to be contiguous and split train/test\"\"\"\n",
    "    # Create a copy\n",
    "    data = ratings_df.copy()\n",
    "    \n",
    "    # Remap user IDs to be contiguous (0-indexed)\n",
    "    unique_users = sorted(data['user_id'].unique())\n",
    "    user_map = {old_id: new_id for new_id, old_id in enumerate(unique_users)}\n",
    "    data['user_id'] = data['user_id'].map(user_map)\n",
    "    num_users = len(unique_users)\n",
    "    \n",
    "    # Remap item IDs to be contiguous (0-indexed)\n",
    "    unique_items = sorted(data['item_id'].unique())\n",
    "    item_map = {old_id: new_id for new_id, old_id in enumerate(unique_items)}\n",
    "    data['item_id'] = data['item_id'].map(item_map)\n",
    "    num_items = len(unique_items)\n",
    "    \n",
    "    # Split into train and test\n",
    "    train_df, test_df = train_test_split(\n",
    "        data[['user_id', 'item_id', 'rating']],\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Create rating matrices\n",
    "    train_mat = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "    test_mat = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "    \n",
    "    # Fill train matrix\n",
    "    for _, row in train_df.iterrows():\n",
    "        train_mat[int(row['user_id']), int(row['item_id'])] = float(row['rating'])\n",
    "    \n",
    "    # Fill test matrix\n",
    "    for _, row in test_df.iterrows():\n",
    "        test_mat[int(row['user_id']), int(row['item_id'])] = float(row['rating'])\n",
    "    \n",
    "    return train_mat, test_mat, train_df, test_df, num_users, num_items\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "train_mat, test_mat, train_df, test_df, num_users, num_items = preprocess_data(\n",
    "    ratings_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✓ Preprocessing complete!\")\n",
    "print(f\"  - Users: {num_users}, Items: {num_items}\")\n",
    "print(f\"  - Train interactions: {len(train_df):,}\")\n",
    "print(f\"  - Test interactions: {len(test_df):,}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a101f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Datasets created\n",
      "  - Train samples: 800,167\n",
      "  - Test samples: 200,042\n"
     ]
    }
   ],
   "source": [
    "# Dataset class for hybrid model\n",
    "class HybridDataset(data.Dataset):\n",
    "    def __init__(self, rating_matrix, interactions_df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rating_matrix: (num_users, num_items) rating matrix\n",
    "            interactions_df: DataFrame with columns ['user_id', 'item_id', 'rating']\n",
    "        \"\"\"\n",
    "        self.rating_matrix = rating_matrix\n",
    "        self.interactions_df = interactions_df.reset_index(drop=True)\n",
    "        \n",
    "        # Create masks (1 where rating exists, 0 otherwise)\n",
    "        self.user_mask = (rating_matrix > 0).astype(np.float32)\n",
    "        self.item_mask = (rating_matrix.T > 0).astype(np.float32)  # Transpose for items\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.interactions_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.interactions_df.iloc[idx]\n",
    "        user_id = int(row['user_id'])\n",
    "        item_id = int(row['item_id'])\n",
    "        rating = float(row['rating'])\n",
    "        \n",
    "        # Get user vector (ratings across all items)\n",
    "        user_vec = torch.FloatTensor(self.rating_matrix[user_id])\n",
    "        \n",
    "        # Get item vector (ratings across all users)\n",
    "        item_vec = torch.FloatTensor(self.rating_matrix[:, item_id])\n",
    "        \n",
    "        # Get masks\n",
    "        user_mask = torch.FloatTensor(self.user_mask[user_id])\n",
    "        item_mask = torch.FloatTensor(self.item_mask[item_id])\n",
    "        \n",
    "        return (\n",
    "            torch.LongTensor([user_id]),\n",
    "            torch.LongTensor([item_id]),\n",
    "            torch.FloatTensor([rating]),\n",
    "            user_vec,\n",
    "            item_vec,\n",
    "            user_mask,\n",
    "            item_mask\n",
    "        )\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = HybridDataset(train_mat, train_df)\n",
    "test_dataset = HybridDataset(test_mat, test_df)\n",
    "\n",
    "print(f\"✓ Datasets created\")\n",
    "print(f\"  - Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  - Test samples: {len(test_dataset):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28dfd8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loaders created (batch_size=256)\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders\n",
    "batch_size = 256\n",
    "train_loader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loaders created (batch_size={batch_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c163c65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✓ Model initialized\n",
      "  - Latent dim: 64\n",
      "  - MLP layers: [128, 64]\n",
      "  - Total parameters: 1,286,419\n",
      "model architecture: HybridAutoRecNCF(\n",
      "  (user_autorec): AutoRecEncoder(\n",
      "    (encoder): Linear(in_features=3706, out_features=64, bias=True)\n",
      "    (decoder): Linear(in_features=64, out_features=3706, bias=True)\n",
      "  )\n",
      "  (item_autorec): AutoRecEncoder(\n",
      "    (encoder): Linear(in_features=6040, out_features=64, bias=True)\n",
      "    (decoder): Linear(in_features=64, out_features=6040, bias=True)\n",
      "  )\n",
      "  (ncf): NCF(\n",
      "    (gmf): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (output): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "latent_dim = 64\n",
    "mlp_layers = [128, 64]\n",
    "\n",
    "model = HybridAutoRecNCF(\n",
    "    num_users, num_items, latent_dim, mlp_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=1e-3, weight_decay=1e-5\n",
    ")\n",
    "\n",
    "print(f\"✓ Model initialized\")\n",
    "print(f\"  - Latent dim: {latent_dim}\")\n",
    "print(f\"  - MLP layers: {mlp_layers}\")\n",
    "print(f\"  - Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(f\"model architecture: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6623e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for HitRate and NDCG\n",
    "def hit(gt_items, pred_items):\n",
    "    \"\"\"Calculate Hit Rate: 1 if any gt_item is in pred_items, else 0\"\"\"\n",
    "    if len(gt_items) == 0:\n",
    "        return 0.0\n",
    "    return 1.0 if any(item in pred_items for item in gt_items) else 0.0\n",
    "\n",
    "def ndcg(gt_items, pred_items):\n",
    "    \"\"\"Calculate NDCG for ranking quality\"\"\"\n",
    "    if len(gt_items) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate DCG\n",
    "    dcg = 0.0\n",
    "    for idx, item in enumerate(pred_items):\n",
    "        if item in gt_items:\n",
    "            dcg += 1.0 / np.log2(idx + 2)\n",
    "    \n",
    "    # Calculate IDCG\n",
    "    num_gt = len(gt_items)\n",
    "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(num_gt, len(pred_items))))\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "# Evaluation function for RMSE\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    accumulated_loss = 0.0\n",
    "    accumulated_rmse = 0.0\n",
    "    num_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            (user_ids, item_ids, ratings,\n",
    "             user_vecs, item_vecs,\n",
    "             user_mask, item_mask) = batch\n",
    "\n",
    "            user_ids = user_ids.to(device).squeeze()\n",
    "            item_ids = item_ids.to(device).squeeze()\n",
    "            ratings = ratings.to(device).squeeze()\n",
    "            user_vecs = user_vecs.to(device)\n",
    "            item_vecs = item_vecs.to(device)\n",
    "            user_mask = user_mask.to(device)\n",
    "            item_mask = item_mask.to(device)\n",
    "\n",
    "            pred, user_recon, item_recon = model(\n",
    "                user_vecs, item_vecs, user_ids, item_ids\n",
    "            )\n",
    "\n",
    "            loss = total_loss(\n",
    "                pred, ratings,\n",
    "                user_recon, user_vecs, user_mask,\n",
    "                item_recon, item_vecs, item_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate RMSE for predictions\n",
    "            rmse = torch.sqrt(F.mse_loss(pred, ratings))\n",
    "            \n",
    "            accumulated_loss += loss.item() * len(ratings)\n",
    "            accumulated_rmse += rmse.item() * len(ratings)\n",
    "            num_samples += len(ratings)\n",
    "    \n",
    "    return accumulated_loss / num_samples, accumulated_rmse / num_samples\n",
    "\n",
    "# Evaluation function for HitRate and NDCG\n",
    "def evaluate_ranking_metrics(model, train_mat, test_mat, num_users, num_items, top_k=10, device=None):\n",
    "    \"\"\"\n",
    "    Evaluate HitRate@K and NDCG@K metrics.\n",
    "    \n",
    "    For each user:\n",
    "    1. Get predictions for all items using training data\n",
    "    2. Mask out items seen in training\n",
    "    3. Get top-K items\n",
    "    4. Calculate HR and NDCG based on test items\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    train_mat_tensor = torch.FloatTensor(train_mat).to(device)\n",
    "    test_mat_np = test_mat\n",
    "    \n",
    "    # Get training mask\n",
    "    train_mask = (train_mat > 0)\n",
    "    \n",
    "    # Predict ratings for all user-item pairs\n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size_users = 256\n",
    "    predictions = np.zeros((num_users, num_items), dtype=np.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start_idx in range(0, num_users, batch_size_users):\n",
    "            end_idx = min(start_idx + batch_size_users, num_users)\n",
    "            batch_size = end_idx - start_idx\n",
    "            \n",
    "            # Get user vectors (ratings across all items from training)\n",
    "            user_vecs_batch = train_mat_tensor[start_idx:end_idx]  # (batch_size, num_items)\n",
    "            \n",
    "            # For each item, get its rating vector across all users\n",
    "            for item_id in range(num_items):\n",
    "                # Get item vector (ratings across all users from training)\n",
    "                # This is a column vector: (num_users,)\n",
    "                item_vec_full = train_mat_tensor[:, item_id]  # (num_users,)\n",
    "                # Expand to match batch size: each user in batch gets the same item vector\n",
    "                item_vecs_batch = item_vec_full.unsqueeze(0).expand(batch_size, -1)  # (batch_size, num_users)\n",
    "                \n",
    "                # Create dummy user_ids and item_ids (not used in forward pass after our fix)\n",
    "                user_ids_batch = torch.arange(start_idx, end_idx, device=device)\n",
    "                item_ids_batch = torch.full((batch_size,), item_id, device=device)\n",
    "                \n",
    "                # Get predictions\n",
    "                pred_batch, _, _ = model(\n",
    "                    user_vecs_batch, item_vecs_batch, user_ids_batch, item_ids_batch\n",
    "                )\n",
    "                \n",
    "                predictions[start_idx:end_idx, item_id] = pred_batch.cpu().numpy()\n",
    "    \n",
    "    # Mask out items seen in training\n",
    "    predictions = predictions * (~train_mask).astype(np.float32) - train_mask.astype(np.float32) * 1e10\n",
    "    \n",
    "    HR_list = []\n",
    "    NDCG_list = []\n",
    "    \n",
    "    # Calculate metrics for each user\n",
    "    for user_id in range(num_users):\n",
    "        # Get ground truth items for this user (items rated in test set)\n",
    "        test_items = set(np.where(test_mat_np[user_id] > 0)[0])\n",
    "        \n",
    "        # Skip if user has no test items\n",
    "        if len(test_items) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get top-K items for this user\n",
    "        user_predictions = predictions[user_id]\n",
    "        top_k_indices = np.argsort(user_predictions)[-top_k:][::-1]\n",
    "        top_k_items = top_k_indices.tolist()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        HR_list.append(hit(test_items, top_k_items))\n",
    "        NDCG_list.append(ndcg(test_items, top_k_items))\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    if len(HR_list) == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    mean_HR = np.mean(HR_list)\n",
    "    mean_NDCG = np.mean(NDCG_list)\n",
    "    \n",
    "    return mean_HR, mean_NDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177f995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting Training\n",
      "======================================================================\n",
      "Epochs: 20\n",
      "Batch size: 256\n",
      "Learning rate: 0.001\n",
      "Top-K for ranking metrics: 10\n",
      "======================================================================\n",
      "  Computing ranking metrics (HR@10, NDCG@10)...\n"
     ]
    }
   ],
   "source": [
    "# Training loop with metrics tracking\n",
    "num_epochs = 20\n",
    "best_test_rmse = float('inf')\n",
    "best_epoch = 0\n",
    "top_k = 10\n",
    "\n",
    "# Create model save directory\n",
    "model_dir = os.path.join(os.path.dirname(os.getcwd()), '..', 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'HybridAutoRecNCF.pth')\n",
    "\n",
    "# Track metrics for visualization\n",
    "training_history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'test_loss': [],\n",
    "    'test_rmse': [],\n",
    "    'hit_rate': [],\n",
    "    'ndcg': []\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Starting Training\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"Top-K for ranking metrics: {top_k}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Evaluation (RMSE)\n",
    "    test_loss, test_rmse = evaluate(model, test_loader, device)\n",
    "    \n",
    "    # Evaluation (HitRate and NDCG) - compute every epoch or every few epochs\n",
    "    # Note: This is computationally expensive, so we do it every epoch\n",
    "    # You can modify to compute less frequently if needed\n",
    "    print(f\"  Computing ranking metrics (HR@{top_k}, NDCG@{top_k})...\")\n",
    "    hit_rate, ndcg_score = evaluate_ranking_metrics(\n",
    "        model, train_mat, test_mat, num_users, num_items, top_k=top_k, device=device\n",
    "    )\n",
    "    \n",
    "    # Save best model based on RMSE\n",
    "    if test_rmse < best_test_rmse:\n",
    "        best_test_rmse = test_rmse\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    # Track metrics\n",
    "    training_history['epoch'].append(epoch + 1)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['test_loss'].append(test_loss)\n",
    "    training_history['test_rmse'].append(test_rmse)\n",
    "    training_history['hit_rate'].append(hit_rate)\n",
    "    training_history['ndcg'].append(ndcg_score)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  HR@{top_k}: {hit_rate:.4f} | NDCG@{top_k}: {ndcg_score:.4f}\")\n",
    "    if epoch == best_epoch:\n",
    "        print(f\"  ✓ Best model saved (RMSE: {best_test_rmse:.4f})\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best model at epoch {best_epoch+1} with RMSE: {best_test_rmse:.4f}\")\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893a2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of training metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "epochs = training_history['epoch']\n",
    "\n",
    "# Plot 1: RMSE\n",
    "axes[0].plot(epochs, training_history['test_rmse'], 'b-o', label='Test RMSE', linewidth=2, markersize=6)\n",
    "axes[0].axvline(x=best_epoch + 1, color='r', linestyle='--', alpha=0.7, label=f'Best Epoch ({best_epoch + 1})')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('RMSE', fontsize=12)\n",
    "axes[0].set_title('Test RMSE Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_xticks(epochs[::2] if len(epochs) > 10 else epochs)\n",
    "\n",
    "# Plot 2: Hit Rate\n",
    "axes[1].plot(epochs, training_history['hit_rate'], 'g-o', label=f'HR@{top_k}', linewidth=2, markersize=6)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Hit Rate', fontsize=12)\n",
    "axes[1].set_title(f'Hit Rate@{top_k} Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(epochs[::2] if len(epochs) > 10 else epochs)\n",
    "axes[1].set_ylim([0, max(1.0, max(training_history['hit_rate']) * 1.1)])\n",
    "\n",
    "# Plot 3: NDCG\n",
    "axes[2].plot(epochs, training_history['ndcg'], 'm-o', label=f'NDCG@{top_k}', linewidth=2, markersize=6)\n",
    "axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[2].set_ylabel('NDCG', fontsize=12)\n",
    "axes[2].set_title(f'NDCG@{top_k} Over Epochs', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].legend()\n",
    "axes[2].set_xticks(epochs[::2] if len(epochs) > 10 else epochs)\n",
    "axes[2].set_ylim([0, max(1.0, max(training_history['ndcg']) * 1.1)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Final Training Metrics\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best RMSE: {best_test_rmse:.4f} (Epoch {best_epoch + 1})\")\n",
    "print(f\"Final HR@{top_k}: {training_history['hit_rate'][-1]:.4f}\")\n",
    "print(f\"Final NDCG@{top_k}: {training_history['ndcg'][-1]:.4f}\")\n",
    "print(f\"Best HR@{top_k}: {max(training_history['hit_rate']):.4f} (Epoch {training_history['hit_rate'].index(max(training_history['hit_rate'])) + 1})\")\n",
    "print(f\"Best NDCG@{top_k}: {max(training_history['ndcg']):.4f} (Epoch {training_history['ndcg'].index(max(training_history['ndcg'])) + 1})\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846b782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
